#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ç½‘ç»œç¯å¢ƒæ˜¯å¦å¯ä»¥è®¿é—®Google News

åŠŸèƒ½ï¼š
1. æµ‹è¯•åŸºç¡€ç½‘ç»œè¿æ¥
2. æµ‹è¯•Googleæœç´¢å¯è¾¾æ€§
3. æµ‹è¯•Google News RSSå¯è¾¾æ€§
4. æ¨¡æ‹Ÿå®é™…æ–°é—»æŠ“å–
"""

import sys
import requests
from datetime import datetime, timedelta
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_basic_network():
    """æµ‹è¯•1: åŸºç¡€ç½‘ç»œè¿æ¥"""
    print("\n" + "="*80)
    print("æµ‹è¯•1: åŸºç¡€ç½‘ç»œè¿æ¥")
    print("="*80)
    
    test_urls = [
        ("ç™¾åº¦", "https://www.baidu.com"),
        ("Google", "https://www.google.com"),
    ]
    
    results = []
    for name, url in test_urls:
        try:
            response = requests.get(url, timeout=10)
            status = "âœ… å¯è®¿é—®" if response.status_code == 200 else f"âš ï¸ çŠ¶æ€ç : {response.status_code}"
            print(f"{name:15} {url:40} {status}")
            results.append({
                "name": name,
                "url": url,
                "accessible": response.status_code == 200,
                "status_code": response.status_code
            })
        except requests.exceptions.Timeout:
            print(f"{name:15} {url:40} âŒ è¶…æ—¶")
            results.append({"name": name, "url": url, "accessible": False, "error": "timeout"})
        except requests.exceptions.ConnectionError as e:
            print(f"{name:15} {url:40} âŒ è¿æ¥å¤±è´¥: {str(e)[:50]}")
            results.append({"name": name, "url": url, "accessible": False, "error": "connection_error"})
        except Exception as e:
            print(f"{name:15} {url:40} âŒ é”™è¯¯: {str(e)[:50]}")
            results.append({"name": name, "url": url, "accessible": False, "error": str(e)[:50]})
    
    return results

def test_google_news_rss():
    """æµ‹è¯•2: Google News RSSè®¢é˜…"""
    print("\n" + "="*80)
    print("æµ‹è¯•2: Google News RSSè®¿é—®")
    print("="*80)
    
    # Google News RSSæ ¼å¼
    keywords = "China policy"
    base_url = "https://news.google.com/rss/search"
    params = {
        'q': keywords,
        'hl': 'en-US',
        'gl': 'US',
        'ceid': 'US:en'
    }
    
    print(f"å…³é”®è¯: {keywords}")
    print(f"RSS URL: {base_url}")
    print(f"å‚æ•°: {params}")
    print()
    
    try:
        response = requests.get(base_url, params=params, timeout=15)
        print(f"çŠ¶æ€ç : {response.status_code}")
        print(f"å“åº”é•¿åº¦: {len(response.content)} bytes")
        
        if response.status_code == 200:
            content = response.text
            print(f"âœ… RSSè·å–æˆåŠŸ")
            print(f"å†…å®¹ç±»å‹: {response.headers.get('Content-Type', 'unknown')}")
            print(f"å‰500å­—ç¬¦:\n{content[:500]}")
            
            # å°è¯•è§£æRSS
            if '<?xml' in content:
                print("\nâœ… RSSæ ¼å¼æ­£ç¡®ï¼ˆåŒ…å«XMLå£°æ˜ï¼‰")
                
                # ç»Ÿè®¡æ¡ç›®æ•°é‡
                item_count = content.count('<item>')
                print(f"ğŸ“° æ–°é—»æ¡ç›®æ•°é‡: {item_count}")
                
                return {
                    "accessible": True,
                    "status_code": 200,
                    "item_count": item_count,
                    "content_length": len(content)
                }
            else:
                print("\nâš ï¸ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„XMLæ ¼å¼")
                return {"accessible": False, "error": "invalid_xml"}
        else:
            print(f"âŒ RSSè·å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return {"accessible": False, "status_code": response.status_code}
            
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶")
        return {"accessible": False, "error": "timeout"}
    except requests.exceptions.ConnectionError as e:
        print(f"âŒ è¿æ¥å¤±è´¥: {str(e)[:100]}")
        return {"accessible": False, "error": "connection_error"}
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)[:100]}")
        return {"accessible": False, "error": str(e)[:100]}

def test_news_parsing():
    """æµ‹è¯•3: æ–°é—»è§£æ"""
    print("\n" + "="*80)
    print("æµ‹è¯•3: æ¨¡æ‹Ÿæ–°é—»è§£æ")
    print("="*80)
    
    try:
        from gnews import GNews
        
        print("ä½¿ç”¨ GNews åº“...")
        gnews = GNews(
            language='en',
            country='US',
            period='7d',
            max_results=5
        )
        
        keywords = "China economy"
        print(f"æœç´¢å…³é”®è¯: {keywords}")
        
        news_list = gnews.get_news(keywords)
        
        print(f"\nâœ… è·å–åˆ° {len(news_list)} æ¡æ–°é—»")
        
        for i, news in enumerate(news_list[:3], 1):
            print(f"\n{i}. {news.get('title', 'N/A')}")
            print(f"   æ¥æº: {news.get('publisher', {}).get('title', 'N/A')}")
            print(f"   æ—¶é—´: {news.get('published date', 'N/A')}")
            print(f"   é“¾æ¥: {news.get('url', 'N/A')[:80]}...")
        
        return {
            "accessible": True,
            "library": "GNews",
            "news_count": len(news_list)
        }
        
    except ImportError:
        print("âš ï¸ GNewsåº“æœªå®‰è£…ï¼Œå°è¯•ç›´æ¥è§£æRSS...")
        
        try:
            import feedparser
            
            url = "https://news.google.com/rss/search?q=China+economy&hl=en-US&gl=US&ceid=US:en"
            print(f"RSS URL: {url}")
            
            feed = feedparser.parse(url)
            
            if feed.entries:
                print(f"\nâœ… è§£ææˆåŠŸï¼Œè·å–åˆ° {len(feed.entries)} æ¡æ–°é—»")
                
                for i, entry in enumerate(feed.entries[:3], 1):
                    print(f"\n{i}. {entry.get('title', 'N/A')}")
                    print(f"   æ—¶é—´: {entry.get('published', 'N/A')}")
                    print(f"   é“¾æ¥: {entry.get('link', 'N/A')[:80]}...")
                
                return {
                    "accessible": True,
                    "library": "feedparser",
                    "news_count": len(feed.entries)
                }
            else:
                print("âŒ æœªèƒ½è§£æåˆ°æ–°é—»æ¡ç›®")
                return {"accessible": False, "error": "no_entries"}
                
        except ImportError:
            print("âŒ feedparseråº“ä¹Ÿæœªå®‰è£…")
            return {"accessible": False, "error": "no_parser_library"}
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {str(e)}")
            return {"accessible": False, "error": str(e)[:100]}
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return {"accessible": False, "error": str(e)[:100]}

def save_test_results(results):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    output_dir = Path("./test_results")
    output_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSONæ ¼å¼
    json_file = output_dir / f"google_news_network_test_{timestamp}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            "test_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æµ‹è¯•ç»“æœå·²ä¿å­˜: {json_file}")
    
    # TXTæ ¼å¼
    txt_file = output_dir / f"google_news_network_test_{timestamp}.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("Google News ç½‘ç»œç¯å¢ƒæµ‹è¯•æŠ¥å‘Š\n")
        f.write("="*80 + "\n\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        for key, value in results.items():
            f.write(f"\n{key}:\n")
            f.write(json.dumps(value, ensure_ascii=False, indent=2))
            f.write("\n")
    
    print(f"âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {txt_file}")
    
    return json_file, txt_file

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "="*80)
    print("ğŸŒ Google News ç½‘ç»œç¯å¢ƒæµ‹è¯•")
    print("="*80)
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    results = {}
    
    # æµ‹è¯•1: åŸºç¡€ç½‘ç»œ
    results['basic_network'] = test_basic_network()
    
    # æµ‹è¯•2: Google News RSS
    results['google_news_rss'] = test_google_news_rss()
    
    # æµ‹è¯•3: æ–°é—»è§£æ
    results['news_parsing'] = test_news_parsing()
    
    # ä¿å­˜ç»“æœ
    json_file, txt_file = save_test_results(results)
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    google_accessible = any(
        r.get('accessible', False) and r.get('name') == 'Google' 
        for r in results.get('basic_network', [])
    )
    rss_accessible = results.get('google_news_rss', {}).get('accessible', False)
    parsing_works = results.get('news_parsing', {}).get('accessible', False)
    
    print(f"Googleè®¿é—®: {'âœ… å¯è¾¾' if google_accessible else 'âŒ ä¸å¯è¾¾'}")
    print(f"Google News RSS: {'âœ… å¯è¾¾' if rss_accessible else 'âŒ ä¸å¯è¾¾'}")
    print(f"æ–°é—»è§£æ: {'âœ… æ­£å¸¸' if parsing_works else 'âŒ å¤±è´¥'}")
    
    print(f"\nğŸ“ è¯¦ç»†ç»“æœ:")
    print(f"   - JSON: {json_file}")
    print(f"   - TXT:  {txt_file}")
    
    if google_accessible and rss_accessible and parsing_works:
        print("\nâœ… ç½‘ç»œç¯å¢ƒæ­£å¸¸ï¼Œå¯ä»¥ä½¿ç”¨Google News")
        return 0
    elif not google_accessible:
        print("\nâŒ æ— æ³•è®¿é—®Googleï¼Œå¯èƒ½éœ€è¦é…ç½®ä»£ç†æˆ–ä½¿ç”¨å…¶ä»–æ–°é—»æº")
        return 1
    elif not rss_accessible:
        print("\nâŒ æ— æ³•è®¿é—®Google News RSSï¼Œè¯·æ£€æŸ¥ç½‘ç»œé…ç½®")
        return 1
    else:
        print("\nâš ï¸ ç½‘ç»œå¯è¾¾ä½†æ–°é—»è§£æå¤±è´¥ï¼Œå¯èƒ½éœ€è¦å®‰è£…ä¾èµ–åº“")
        print("   å»ºè®®: pip install gnews feedparser")
        return 1

if __name__ == "__main__":
    exit(main())
