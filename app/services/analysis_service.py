"""
è‚¡ç¥¨åˆ†ææœåŠ¡
æ•´åˆäº†åŸ simple_analysis_service.py å’Œ analysis_service.py çš„åŠŸèƒ½
"""

import asyncio
import uuid
import json
import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Callable
from pathlib import Path
import sys
import concurrent.futures
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åˆå§‹åŒ–TradingAgentsæ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import init_logging
init_logging()

from tradingagents.graph.trading_graph import TradingAgentsGraph
from tradingagents.default_config import DEFAULT_CONFIG
from tradingagents.utils.runtime_paths import get_analysis_results_dir, resolve_path
from tradingagents.dataflows.data_source_manager import get_data_source_manager
from tradingagents.utils.stock_utils import StockUtils
from tradingagents.utils.dataflow_utils import get_trading_date_range

from app.models.analysis import (
    AnalysisParameters, AnalysisResult, AnalysisTask, AnalysisBatch,
    AnalysisStatus, BatchStatus, SingleAnalysisRequest, BatchAnalysisRequest
)
from app.models.user import PyObjectId
from app.models.notification import NotificationCreate
from bson import ObjectId
from app.core.database import get_mongo_db, get_redis_client
from app.core.redis_client import get_redis_service, RedisKeys
from app.services.queue_service import QueueService
from app.services.usage_statistics_service import UsageStatisticsService
from app.services.redis_progress_tracker import RedisProgressTracker, get_progress_by_id
from app.services.config_service import ConfigService
from app.services.config_provider import provider as config_provider
from app.services.memory_state_manager import get_memory_state_manager, TaskStatus
from app.services.progress_log_handler import register_analysis_tracker, unregister_analysis_tracker
from app.services.websocket_manager import get_websocket_manager
from app.core.config import settings
from app.services.queue import DEFAULT_USER_CONCURRENT_LIMIT, GLOBAL_CONCURRENT_LIMIT, VISIBILITY_TIMEOUT_SECONDS

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger("app.services.analysis_service")

# é…ç½®æœåŠ¡å®ä¾‹
config_service = ConfigService()

# è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è·å–ï¼ˆç”¨äºè¡¥å……æ˜¾ç¤ºåç§°ï¼‰
try:
    _data_source_manager = get_data_source_manager()
    def _get_stock_info_safe(stock_code: str):
        """è·å–è‚¡ç¥¨åŸºç¡€ä¿¡æ¯çš„å®‰å…¨å°è£…"""
        return _data_source_manager.get_stock_basic_info(stock_code)
except Exception:
    _get_stock_info_safe = None

# -----------------------------------------------------------------------------
# Helper Functions (from simple_analysis_service.py)
# -----------------------------------------------------------------------------

async def get_provider_by_model_name(model_name: str) -> str:
    """
    æ ¹æ®æ¨¡å‹åç§°ä»æ•°æ®åº“é…ç½®ä¸­æŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰
    """
    try:
        # ä»é…ç½®æœåŠ¡è·å–ç³»ç»Ÿé…ç½®
        system_config = await config_service.get_system_config()
        if not system_config or not system_config.llm_configs:
            logger.warning(f"âš ï¸ ç³»ç»Ÿé…ç½®ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤ä¾›åº”å•†æ˜ å°„")
            return _get_default_provider_by_model(model_name)

        # åœ¨LLMé…ç½®ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹
        for llm_config in system_config.llm_configs:
            if llm_config.model_name == model_name:
                provider = llm_config.provider.value if hasattr(llm_config.provider, 'value') else str(llm_config.provider)
                logger.info(f"âœ… ä»æ•°æ®åº“æ‰¾åˆ°æ¨¡å‹ {model_name} çš„ä¾›åº”å•†: {provider}")
                return provider

        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„
        logger.warning(f"âš ï¸ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ¨¡å‹ {model_name}ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
        return _get_default_provider_by_model(model_name)

    except Exception as e:
        logger.error(f"âŒ æŸ¥æ‰¾æ¨¡å‹ä¾›åº”å•†å¤±è´¥: {e}")
        return _get_default_provider_by_model(model_name)


def get_provider_by_model_name_sync(model_name: str) -> str:
    """
    æ ¹æ®æ¨¡å‹åç§°ä»æ•°æ®åº“é…ç½®ä¸­æŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
    """
    provider_info = get_provider_and_url_by_model_sync(model_name)
    return provider_info["provider"]


def get_provider_and_url_by_model_sync(model_name: str) -> dict:
    """
    æ ¹æ®æ¨¡å‹åç§°ä»æ•°æ®åº“é…ç½®ä¸­æŸ¥æ‰¾å¯¹åº”çš„ä¾›åº”å•†å’Œ API URLï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
    """
    try:
        # ä½¿ç”¨åŒæ­¥ MongoDB å®¢æˆ·ç«¯ç›´æ¥æŸ¥è¯¢
        from pymongo import MongoClient
        from app.core.config import settings
        
        client = MongoClient(settings.MONGO_URI)
        db = client[settings.MONGO_DB]

        try:
            # æŸ¥è¯¢æœ€æ–°çš„æ´»è·ƒé…ç½®
            configs_collection = db.system_configs
            doc = configs_collection.find_one({"is_active": True}, sort=[("version", -1)])

            if doc and "llm_configs" in doc:
                llm_configs = doc["llm_configs"]

                for config_dict in llm_configs:
                    if config_dict.get("model_name") == model_name:
                        provider = config_dict.get("provider")
                        api_base = config_dict.get("api_base")
                        model_api_key = config_dict.get("api_key")  # ğŸ”¥ è·å–æ¨¡å‹é…ç½®çš„ API Key

                        # ä» llm_providers é›†åˆä¸­æŸ¥æ‰¾å‚å®¶é…ç½®
                        providers_collection = db.llm_providers
                        provider_doc = providers_collection.find_one({"name": provider})

                        # ğŸ”¥ ç¡®å®š API Keyï¼ˆä¼˜å…ˆçº§ï¼šæ¨¡å‹é…ç½® > å‚å®¶é…ç½® > ç¯å¢ƒå˜é‡ï¼‰
                        api_key = None
                        if model_api_key and model_api_key.strip() and model_api_key != "your-api-key":
                            api_key = model_api_key
                            logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨æ¨¡å‹é…ç½®çš„ API Key")
                        elif provider_doc and provider_doc.get("api_key"):
                            provider_api_key = provider_doc["api_key"]
                            if provider_api_key and provider_api_key.strip() and provider_api_key != "your-api-key":
                                api_key = provider_api_key
                                logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨å‚å®¶é…ç½®çš„ API Key")

                        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ API Keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
                        if not api_key:
                            api_key = _get_env_api_key_for_provider(provider)
                            if api_key:
                                logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨ç¯å¢ƒå˜é‡çš„ API Key")
                            else:
                                logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] æœªæ‰¾åˆ° {provider} çš„ API Key")

                        # ç¡®å®š backend_url
                        backend_url = None
                        if api_base:
                            backend_url = api_base
                            logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] æ¨¡å‹ {model_name} ä½¿ç”¨è‡ªå®šä¹‰ API: {api_base}")
                        elif provider_doc and provider_doc.get("default_base_url"):
                            backend_url = provider_doc["default_base_url"]
                            logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] æ¨¡å‹ {model_name} ä½¿ç”¨å‚å®¶é»˜è®¤ API: {backend_url}")
                        else:
                            backend_url = _get_default_backend_url(provider)
                            logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] å‚å®¶ {provider} æ²¡æœ‰é…ç½® default_base_urlï¼Œä½¿ç”¨ç¡¬ç¼–ç é»˜è®¤å€¼")

                        return {
                            "provider": provider,
                            "backend_url": backend_url,
                            "api_key": api_key
                        }

            # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„
            logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ¨¡å‹ {model_name}ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
            provider = _get_default_provider_by_model(model_name)

            # å°è¯•ä»å‚å®¶é…ç½®ä¸­è·å– default_base_url å’Œ API Key
            try:
                providers_collection = db.llm_providers
                provider_doc = providers_collection.find_one({"name": provider})

                backend_url = _get_default_backend_url(provider)
                api_key = None

                if provider_doc:
                    if provider_doc.get("default_base_url"):
                        backend_url = provider_doc["default_base_url"]
                        logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨å‚å®¶ {provider} çš„ default_base_url: {backend_url}")

                    if provider_doc.get("api_key"):
                        provider_api_key = provider_doc["api_key"]
                        if provider_api_key and provider_api_key.strip() and provider_api_key != "your-api-key":
                            api_key = provider_api_key
                            logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨å‚å®¶ {provider} çš„ API Key")

                # å¦‚æœå‚å®¶é…ç½®ä¸­æ²¡æœ‰ API Keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
                if not api_key:
                    api_key = _get_env_api_key_for_provider(provider)
                    if api_key:
                        logger.info(f"âœ… [åŒæ­¥æŸ¥è¯¢] ä½¿ç”¨ç¯å¢ƒå˜é‡çš„ API Key")

                return {
                    "provider": provider,
                    "backend_url": backend_url,
                    "api_key": api_key
                }
            except Exception as e:
                logger.warning(f"âš ï¸ [åŒæ­¥æŸ¥è¯¢] æ— æ³•æŸ¥è¯¢å‚å®¶é…ç½®: {e}")

            # æœ€åå›é€€åˆ°ç¡¬ç¼–ç çš„é»˜è®¤ URL å’Œç¯å¢ƒå˜é‡ API Key
            return {
                "provider": provider,
                "backend_url": _get_default_backend_url(provider),
                "api_key": _get_env_api_key_for_provider(provider)
            }
        finally:
            client.close()

    except Exception as e:
        logger.error(f"âŒ [åŒæ­¥æŸ¥è¯¢] æŸ¥æ‰¾æ¨¡å‹ä¾›åº”å•†å¤±è´¥: {e}")
        provider = _get_default_provider_by_model(model_name)
        return {
            "provider": provider,
            "backend_url": _get_default_backend_url(provider),
            "api_key": _get_env_api_key_for_provider(provider)
        }


def _get_env_api_key_for_provider(provider: str) -> str:
    """ä»ç¯å¢ƒå˜é‡è·å–æŒ‡å®šä¾›åº”å•†çš„ API Key"""
    env_key_map = {
        "google": "GOOGLE_API_KEY",
        "dashscope": "DASHSCOPE_API_KEY",
        "openai": "OPENAI_API_KEY",
        "deepseek": "DEEPSEEK_API_KEY",
        "anthropic": "ANTHROPIC_API_KEY",
        "openrouter": "OPENROUTER_API_KEY",
        "siliconflow": "SILICONFLOW_API_KEY",
        "qianfan": "QIANFAN_API_KEY",
        "302ai": "AI302_API_KEY",
    }

    env_key_name = env_key_map.get(provider.lower())
    if env_key_name:
        api_key = os.getenv(env_key_name)
        if api_key and api_key.strip() and api_key != "your-api-key":
            return api_key

    return None


def _get_default_backend_url(provider: str) -> str:
    """æ ¹æ®ä¾›åº”å•†åç§°è¿”å›é»˜è®¤çš„ backend_url"""
    default_urls = {
        "google": "https://generativelanguage.googleapis.com/v1beta",
        "dashscope": "https://dashscope.aliyuncs.com/api/v1",
        "openai": "https://api.openai.com/v1",
        "deepseek": "https://api.deepseek.com",
        "anthropic": "https://api.anthropic.com",
        "openrouter": "https://openrouter.ai/api/v1",
        "qianfan": "https://qianfan.baidubce.com/v2",
        "302ai": "https://api.302.ai/v1",
    }

    url = default_urls.get(provider, "https://dashscope.aliyuncs.com/compatible-mode/v1")
    return url


def _get_default_provider_by_model(model_name: str) -> str:
    """æ ¹æ®æ¨¡å‹åç§°è¿”å›é»˜è®¤çš„ä¾›åº”å•†æ˜ å°„"""
    model_provider_map = {
        'qwen-turbo': 'dashscope',
        'qwen-plus': 'dashscope',
        'qwen-max': 'dashscope',
        'qwen-plus-latest': 'dashscope',
        'qwen-max-longcontext': 'dashscope',
        'gpt-3.5-turbo': 'openai',
        'gpt-4': 'openai',
        'gpt-4-turbo': 'openai',
        'gpt-4o': 'openai',
        'gpt-4o-mini': 'openai',
        'gemini-pro': 'google',
        'gemini-2.0-flash': 'google',
        'gemini-2.0-flash-thinking-exp': 'google',
        'deepseek-chat': 'deepseek',
        'deepseek-coder': 'deepseek',
        'glm-4': 'zhipu',
        'glm-3-turbo': 'zhipu',
        'chatglm3-6b': 'zhipu'
    }
    provider = model_provider_map.get(model_name, 'dashscope')
    return provider


def create_analysis_config(
    research_depth,
    selected_analysts: list,
    quick_model: str,
    deep_model: str,
    llm_provider: str,
    market_type: str = "Aè‚¡",
    quick_model_config: dict = None,
    deep_model_config: dict = None
) -> dict:
    """åˆ›å»ºåˆ†æé…ç½®"""
    
    numeric_to_chinese = {
        1: "å¿«é€Ÿ", 2: "åŸºç¡€", 3: "æ ‡å‡†", 4: "æ·±åº¦", 5: "å…¨é¢"
    }

    # æ ‡å‡†åŒ–ç ”ç©¶æ·±åº¦
    if isinstance(research_depth, (int, float)):
        research_depth = int(research_depth)
        if research_depth in numeric_to_chinese:
            research_depth = numeric_to_chinese[research_depth]
        else:
            research_depth = "æ ‡å‡†"
    elif isinstance(research_depth, str):
        if research_depth.isdigit():
            numeric_level = int(research_depth)
            if numeric_level in numeric_to_chinese:
                research_depth = numeric_to_chinese[numeric_level]
            else:
                research_depth = "æ ‡å‡†"
        elif research_depth not in ["å¿«é€Ÿ", "åŸºç¡€", "æ ‡å‡†", "æ·±åº¦", "å…¨é¢"]:
            research_depth = "æ ‡å‡†"
    else:
        research_depth = "æ ‡å‡†"

    config = DEFAULT_CONFIG.copy()
    config["llm_provider"] = llm_provider
    config["deep_think_llm"] = deep_model
    config["quick_think_llm"] = quick_model

    if research_depth == "å¿«é€Ÿ":
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 1
        config["memory_enabled"] = False
        config["online_tools"] = True
    elif research_depth == "åŸºç¡€":
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 1
        config["memory_enabled"] = True
        config["online_tools"] = True
    elif research_depth == "æ ‡å‡†":
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 2
        config["memory_enabled"] = True
        config["online_tools"] = True
    elif research_depth == "æ·±åº¦":
        config["max_debate_rounds"] = 2
        config["max_risk_discuss_rounds"] = 2
        config["memory_enabled"] = True
        config["online_tools"] = True
    elif research_depth == "å…¨é¢":
        config["max_debate_rounds"] = 3
        config["max_risk_discuss_rounds"] = 3
        config["memory_enabled"] = True
        config["online_tools"] = True
    else:
        config["max_debate_rounds"] = 1
        config["max_risk_discuss_rounds"] = 2
        config["memory_enabled"] = True
        config["online_tools"] = True

    try:
        quick_provider_info = get_provider_and_url_by_model_sync(quick_model)
        deep_provider_info = get_provider_and_url_by_model_sync(deep_model)

        config["backend_url"] = quick_provider_info["backend_url"]
        config["quick_api_key"] = quick_provider_info.get("api_key")
        config["deep_api_key"] = deep_provider_info.get("api_key")
    except Exception as e:
        logger.warning(f"âš ï¸  æ— æ³•ä»æ•°æ®åº“è·å– backend_url å’Œ API Key: {e}")
        config["backend_url"] = _get_default_backend_url(llm_provider)

    config["selected_analysts"] = selected_analysts
    config["debug"] = False
    config["research_depth"] = research_depth

    if quick_model_config:
        config["quick_model_config"] = quick_model_config
    if deep_model_config:
        config["deep_model_config"] = deep_model_config

    return config


# -----------------------------------------------------------------------------
# AnalysisService Class
# -----------------------------------------------------------------------------

class AnalysisService:
    """è‚¡ç¥¨åˆ†ææœåŠ¡ç±» - æ•´åˆç‰ˆ"""

    def __init__(self):
        # åˆå§‹åŒ–ç»„ä»¶
        self._trading_graph_cache = {}
        self.memory_manager = get_memory_state_manager()
        self._progress_trackers: Dict[str, RedisProgressTracker] = {}
        self._stock_name_cache: Dict[str, str] = {}

        # çº¿ç¨‹æ± 
        self._thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=3)
        logger.info(f"ğŸ”§ [æœåŠ¡åˆå§‹åŒ–] çº¿ç¨‹æ± æœ€å¤§å¹¶å‘æ•°: 3")

        # é˜Ÿåˆ—å’Œç»Ÿè®¡æœåŠ¡ (ä»åŸAnalysisServiceåˆå¹¶)
        try:
            redis_client = get_redis_client()
            self.queue_service = QueueService(redis_client)
            self.usage_service = UsageStatisticsService()
        except Exception as e:
            logger.warning(f"âš ï¸ é˜Ÿåˆ—æˆ–ç»Ÿè®¡æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")

        # è®¾ç½® WebSocket ç®¡ç†å™¨
        try:
            self.memory_manager.set_websocket_manager(get_websocket_manager())
        except ImportError:
            logger.warning("âš ï¸ WebSocket ç®¡ç†å™¨ä¸å¯ç”¨")

        logger.info(f"ğŸ”§ [æœåŠ¡åˆå§‹åŒ–] AnalysisService å®ä¾‹ID: {id(self)}")

    # -------------------------------------------------------------------------
    # Private Methods
    # -------------------------------------------------------------------------

    async def _update_progress_async(self, task_id: str, progress: int, message: str):
        """å¼‚æ­¥æ›´æ–°è¿›åº¦ï¼ˆå†…å­˜å’ŒMongoDBï¼‰"""
        try:
            await self.memory_manager.update_task_status(
                task_id=task_id,
                status=TaskStatus.RUNNING,
                progress=progress,
                message=message,
                current_step=message
            )
            db = get_mongo_db()
            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {
                    "$set": {
                        "progress": progress,
                        "current_step": message,
                        "message": message,
                        "updated_at": datetime.utcnow()
                    }
                }
            )
        except Exception as e:
            logger.warning(f"âš ï¸ [å¼‚æ­¥æ›´æ–°] å¤±è´¥: {e}")

    def _resolve_stock_name(self, code: Optional[str]) -> str:
        """è§£æè‚¡ç¥¨åç§°ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if not code:
            return ""
        if code in self._stock_name_cache:
            return self._stock_name_cache[code]
        name = None
        try:
            if _get_stock_info_safe:
                info = _get_stock_info_safe(code)
                if isinstance(info, dict):
                    name = info.get("name")
        except Exception:
            pass
        if not name:
            name = f"è‚¡ç¥¨{code}"
        self._stock_name_cache[code] = name
        return name

    def _enrich_stock_names(self, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¸ºä»»åŠ¡åˆ—è¡¨è¡¥é½è‚¡ç¥¨åç§°(å°±åœ°æ›´æ–°)"""
        try:
            for t in tasks:
                code = t.get("stock_code") or t.get("stock_symbol")
                name = t.get("stock_name")
                if not name and code:
                    t["stock_name"] = self._resolve_stock_name(code)
        except Exception as e:
            logger.warning(f"âš ï¸ è¡¥é½è‚¡ç¥¨åç§°æ—¶å‡ºç°å¼‚å¸¸: {e}")
        return tasks

    def _convert_user_id(self, user_id: str) -> PyObjectId:
        """å°†å­—ç¬¦ä¸²ç”¨æˆ·IDè½¬æ¢ä¸ºPyObjectId"""
        try:
            if user_id == "admin":
                return PyObjectId(ObjectId("507f1f77bcf86cd799439011"))
            return PyObjectId(ObjectId(user_id))
        except Exception:
            return PyObjectId(ObjectId())

    def _serialize_for_response(self, value: Any) -> Any:
        """é€’å½’è½¬æ¢ Mongo ç‰¹å®šç±»å‹ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(value, ObjectId):
            return str(value)
        if isinstance(value, list):
            return [self._serialize_for_response(v) for v in value]
        if isinstance(value, dict):
            return {k: self._serialize_for_response(v) for k, v in value.items()}
        return value

    def _get_trading_graph(self, config: Dict[str, Any]) -> TradingAgentsGraph:
        """è·å–æˆ–åˆ›å»ºTradingAgentså®ä¾‹ (æ¯æ¬¡åˆ›å»ºæ–°å®ä¾‹ä»¥ä¿è¯çº¿ç¨‹å®‰å…¨)"""
        return TradingAgentsGraph(
            selected_analysts=config.get("selected_analysts", ["market", "fundamentals"]),
            debug=config.get("debug", False),
            config=config
        )

    # -------------------------------------------------------------------------
    # Main Analysis Methods (Core Logic from simple_analysis_service.py)
    # -------------------------------------------------------------------------

    async def create_analysis_task(
        self,
        user_id: str,
        request: SingleAnalysisRequest
    ) -> Dict[str, Any]:
        """åˆ›å»ºåˆ†æä»»åŠ¡ï¼ˆç«‹å³è¿”å›ï¼Œä¸æ‰§è¡Œåˆ†æï¼‰"""
        try:
            task_id = str(uuid.uuid4())
            stock_code = request.get_symbol()
            if not stock_code:
                raise ValueError("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")

            logger.info(f"ğŸ“ åˆ›å»ºåˆ†æä»»åŠ¡: {task_id} - {stock_code}")

            # åœ¨å†…å­˜ä¸­åˆ›å»ºä»»åŠ¡çŠ¶æ€
            await self.memory_manager.create_task(
                task_id=task_id,
                user_id=user_id,
                stock_code=stock_code,
                parameters=request.parameters.model_dump() if request.parameters else {},
                stock_name=self._resolve_stock_name(stock_code),
            )

            # å†™å…¥MongoDB
            code = stock_code
            name = self._resolve_stock_name(code)
            try:
                db = get_mongo_db()
                await db.analysis_tasks.update_one(
                    {"task_id": task_id},
                    {"$setOnInsert": {
                        "task_id": task_id,
                        "user_id": user_id,
                        "stock_code": code,
                        "stock_symbol": code,
                        "stock_name": name,
                        "status": "pending",
                        "progress": 0,
                        "created_at": datetime.utcnow(),
                    }},
                    upsert=True
                )
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºä»»åŠ¡æ—¶å†™å…¥MongoDBå¤±è´¥: {e}")

            return {
                "task_id": task_id,
                "status": "pending",
                "message": "ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…æ‰§è¡Œ"
            }

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºåˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise

    async def execute_analysis_background(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest
    ):
        """åœ¨åå°æ‰§è¡Œåˆ†æä»»åŠ¡ (Core Logic)"""
        stock_code = request.get_symbol()
        progress_tracker = None
        try:
            logger.info(f"ğŸš€ å¼€å§‹åå°æ‰§è¡Œåˆ†æä»»åŠ¡: {task_id}")

            # éªŒè¯è‚¡ç¥¨ä»£ç 
            from tradingagents.utils.stock_validator import prepare_stock_data_async
            market_type = request.parameters.market_type if request.parameters else "Aè‚¡"
            analysis_date = request.parameters.analysis_date if request.parameters else None
            
            if analysis_date and isinstance(analysis_date, datetime):
                analysis_date = analysis_date.strftime('%Y-%m-%d')
            elif analysis_date and isinstance(analysis_date, str):
                try:
                    parsed_date = datetime.strptime(analysis_date, '%Y-%m-%d')
                    analysis_date = parsed_date.strftime('%Y-%m-%d')
                except ValueError:
                    analysis_date = datetime.now().strftime('%Y-%m-%d')

            validation_result = await prepare_stock_data_async(
                stock_code=stock_code,
                market_type=market_type,
                period_days=30,
                analysis_date=analysis_date
            )

            if not validation_result.is_valid:
                error_msg = f"âŒ è‚¡ç¥¨ä»£ç æ— æ•ˆ: {validation_result.error_message}"
                await self.memory_manager.update_task_status(
                    task_id=task_id, status=AnalysisStatus.FAILED, progress=0, error_message=error_msg
                )
                await self._update_task_status(task_id, AnalysisStatus.FAILED, 0, error_message=error_msg)
                return

            # åˆ›å»ºRedisè¿›åº¦è·Ÿè¸ªå™¨
            def create_progress_tracker():
                return RedisProgressTracker(
                    task_id=task_id,
                    analysts=request.parameters.selected_analysts or ["market", "fundamentals"],
                    research_depth=request.parameters.research_depth or "æ ‡å‡†",
                    llm_provider="dashscope"
                )

            progress_tracker = await asyncio.to_thread(create_progress_tracker)
            self._progress_trackers[task_id] = progress_tracker
            register_analysis_tracker(task_id, progress_tracker)

            # æ›´æ–°åˆå§‹çŠ¶æ€
            await asyncio.to_thread(progress_tracker.update_progress, {"progress_percentage": 10, "last_message": "ğŸš€ å¼€å§‹è‚¡ç¥¨åˆ†æ"})
            await self.memory_manager.update_task_status(
                task_id=task_id, status=TaskStatus.RUNNING, progress=10, message="åˆ†æå¼€å§‹...", current_step="initialization"
            )
            await self._update_task_status(task_id, AnalysisStatus.PROCESSING, 10)

            # æ‰§è¡Œå®é™…åˆ†æ
            result = await self._execute_analysis_sync(task_id, user_id, request, progress_tracker)

            # å®Œæˆ
            await asyncio.to_thread(progress_tracker.mark_completed)
            
            # ä¿å­˜ç»“æœ
            await self._save_analysis_results_complete(task_id, result)

            # æ›´æ–°å®ŒæˆçŠ¶æ€
            await self.memory_manager.update_task_status(
                task_id=task_id, status=TaskStatus.COMPLETED, progress=100, message="åˆ†æå®Œæˆ", current_step="completed", result_data=result
            )
            await self._update_task_status(task_id, AnalysisStatus.COMPLETED, 100)

            # å‘é€é€šçŸ¥
            try:
                from app.services.notifications_service import get_notifications_service
                svc = get_notifications_service()
                summary = str(result.get("summary", ""))[:120]
                await svc.create_and_publish(
                    payload=NotificationCreate(
                        user_id=str(user_id), type='analysis', title=f"{stock_code} åˆ†æå®Œæˆ",
                        content=summary, link=f"/stocks/{stock_code}", source='analysis'
                    )
                )
            except Exception:
                pass

        except Exception as e:
            logger.error(f"âŒ åå°åˆ†æä»»åŠ¡å¤±è´¥: {task_id} - {e}")
            if progress_tracker:
                progress_tracker.mark_failed(str(e))
            await self.memory_manager.update_task_status(
                task_id=task_id, status=TaskStatus.FAILED, progress=0, message="åˆ†æå¤±è´¥", error_message=str(e)
            )
            await self._update_task_status(task_id, AnalysisStatus.FAILED, 0, str(e))
        finally:
            if task_id in self._progress_trackers:
                del self._progress_trackers[task_id]
            unregister_analysis_tracker(task_id)

    # -------------------------------------------------------------------------
    # Compatibility Methods (for API Router)
    # -------------------------------------------------------------------------

    async def submit_single_analysis(self, user_id: str, request: SingleAnalysisRequest) -> Dict[str, Any]:
        """
        æäº¤å•è‚¡åˆ†æä»»åŠ¡ (å…¼å®¹æ—§ AnalysisService æ¥å£)
        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•ç°åœ¨åªæ˜¯ create_analysis_task çš„åˆ«åï¼Œ
        å®é™…æ‰§è¡Œéœ€è¦åœ¨è°ƒç”¨å¤„é€šè¿‡ BackgroundTasks æˆ–å…¶ä»–æ–¹å¼è§¦å‘ execute_analysis_background
        """
        return await self.create_analysis_task(user_id, request)

    async def submit_batch_analysis(self, user_id: str, request: BatchAnalysisRequest) -> Dict[str, Any]:
        """æäº¤æ‰¹é‡åˆ†æä»»åŠ¡ (ä¿ç•™åŸåŠŸèƒ½)"""
        try:
            batch_id = str(uuid.uuid4())
            converted_user_id = self._convert_user_id(user_id)
            
            # è¯»å–é…ç½®
            effective_settings = await config_provider.get_effective_system_settings()
            params = request.parameters or AnalysisParameters()
            
            if not getattr(params, 'quick_analysis_model', None):
                params.quick_analysis_model = effective_settings.get("quick_analysis_model", "qwen-turbo")
            if not getattr(params, 'deep_analysis_model', None):
                params.deep_analysis_model = effective_settings.get("deep_analysis_model", "qwen-max")

            stock_symbols = request.get_symbols()
            
            batch = AnalysisBatch(
                batch_id=batch_id,
                user_id=converted_user_id,
                title=request.title,
                description=request.description,
                total_tasks=len(stock_symbols),
                parameters=params,
                status=BatchStatus.PENDING
            )

            tasks = []
            for symbol in stock_symbols:
                task_id = str(uuid.uuid4())
                task = AnalysisTask(
                    task_id=task_id,
                    batch_id=batch_id,
                    user_id=converted_user_id,
                    symbol=symbol,
                    stock_code=symbol,
                    parameters=batch.parameters,
                    status=AnalysisStatus.PENDING
                )
                tasks.append(task)
            
            db = get_mongo_db()
            await db.analysis_batches.insert_one(batch.dict(by_alias=True))
            await db.analysis_tasks.insert_many([task.dict(by_alias=True) for task in tasks])
            
            for task in tasks:
                queue_params = task.parameters.dict() if task.parameters else {}
                queue_params.update({
                    "task_id": task.task_id,
                    "symbol": task.symbol,
                    "stock_code": task.symbol,
                    "user_id": str(task.user_id),
                    "batch_id": task.batch_id,
                    "created_at": task.created_at.isoformat() if task.created_at else None
                })
                await self.queue_service.enqueue_task(
                    user_id=str(converted_user_id),
                    symbol=task.symbol,
                    params=queue_params,
                    batch_id=task.batch_id
                )
            
            return {
                "batch_id": batch_id,
                "total_tasks": len(tasks),
                "status": BatchStatus.PENDING,
                "message": f"å·²æäº¤{len(tasks)}ä¸ªåˆ†æä»»åŠ¡åˆ°é˜Ÿåˆ—"
            }
        except Exception as e:
            logger.error(f"æäº¤æ‰¹é‡åˆ†æä»»åŠ¡å¤±è´¥: {e}")
            raise

    async def cancel_task(self, task_id: str) -> bool:
        """å–æ¶ˆä»»åŠ¡"""
        try:
            await self._update_task_status(task_id, AnalysisStatus.CANCELLED, 0)
            await self.queue_service.remove_task(task_id)
            return True
        except Exception as e:
            logger.error(f"å–æ¶ˆä»»åŠ¡å¤±è´¥: {task_id} - {e}")
            return False

    # -------------------------------------------------------------------------
    # Internal Execution Logic (from simple_analysis_service.py)
    # -------------------------------------------------------------------------

    async def _execute_analysis_sync(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest,
        progress_tracker: Optional[RedisProgressTracker] = None
    ) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œåˆ†æï¼ˆåœ¨å…±äº«çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(
            self._thread_pool,
            self._run_analysis_sync,
            task_id,
            user_id,
            request,
            progress_tracker
        )
        return result

    def _run_analysis_sync(
        self,
        task_id: str,
        user_id: str,
        request: SingleAnalysisRequest,
        progress_tracker: Optional[RedisProgressTracker] = None
    ) -> Dict[str, Any]:
        """åŒæ­¥æ‰§è¡Œåˆ†æçš„å…·ä½“å®ç°"""
        try:
            from tradingagents.utils.logging_init import init_logging, get_logger
            init_logging()
            
            # è¿›åº¦æ›´æ–°å›è°ƒ
            def update_progress_sync(progress: int, message: str, step: str):
                try:
                    if progress_tracker:
                        progress_tracker.update_progress({"progress_percentage": progress, "last_message": message})
                    
                    # 1. æ›´æ–°å†…å­˜çŠ¶æ€ï¼ˆåŒæ­¥ï¼‰
                    self.memory_manager.update_task_status_sync(
                        task_id=task_id, status=TaskStatus.RUNNING, progress=progress, message=message, current_step=step
                    )
                    
                    # 2. æ›´æ–°MongoDBï¼ˆåŒæ­¥ï¼‰
                    from pymongo import MongoClient
                    from app.core.config import settings
                    
                    client = MongoClient(settings.MONGO_URI)
                    try:
                        sync_db = client[settings.MONGO_DB]
                        sync_db.analysis_tasks.update_one(
                            {"task_id": task_id},
                            {"$set": {"progress": progress, "current_step": step, "message": message, "updated_at": datetime.utcnow()}}
                        )
                    finally:
                        client.close()
                except Exception as e:
                    logger.warning(f"âš ï¸ [Sync] æ›´æ–°è¿›åº¦å¤±è´¥: {e}")

            update_progress_sync(7, "âš™ï¸ é…ç½®åˆ†æå‚æ•°", "configuration")

            # æ¨¡å‹é€‰æ‹©é€»è¾‘
            from app.services.model_capability_service import get_model_capability_service
            capability_service = get_model_capability_service()
            research_depth = request.parameters.research_depth if request.parameters else "æ ‡å‡†"

            if (request.parameters and getattr(request.parameters, 'quick_analysis_model', None) 
                and getattr(request.parameters, 'deep_analysis_model', None)):
                quick_model = request.parameters.quick_analysis_model
                deep_model = request.parameters.deep_analysis_model
                # éªŒè¯é€»è¾‘çœç•¥ï¼Œç›´æ¥ä½¿ç”¨
            else:
                quick_model, deep_model = capability_service.recommend_models_for_depth(research_depth)

            quick_provider_info = get_provider_and_url_by_model_sync(quick_model)
            deep_provider_info = get_provider_and_url_by_model_sync(deep_model)
            quick_provider = quick_provider_info["provider"]
            
            # è·å–å¸‚åœºç±»å‹ - ä¼˜å…ˆä½¿ç”¨ StockUtils è‡ªåŠ¨è¯†åˆ«
            if request.parameters and request.parameters.market_type:
                market_type = request.parameters.market_type
            else:
                try:
                    # è‡ªåŠ¨è¯†åˆ«å¸‚åœºç±»å‹
                    market_info = StockUtils.get_market_info(request.get_symbol())
                    if market_info.get('is_china'):
                        market_type = "Aè‚¡"
                    elif market_info.get('is_hk'):
                        market_type = "æ¸¯è‚¡"
                    elif market_info.get('is_us'):
                        market_type = "ç¾è‚¡"
                    else:
                        market_type = "Aè‚¡"  # é»˜è®¤å…œåº•
                    logger.info(f"ğŸ“Š [è‡ªåŠ¨è¯†åˆ«] è‚¡ç¥¨ {request.get_symbol()} å¸‚åœºç±»å‹: {market_type}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ— æ³•è¯†åˆ«è‚¡ç¥¨å¸‚åœºç±»å‹: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'Aè‚¡'")
                    market_type = "Aè‚¡"
            
            config = create_analysis_config(
                research_depth=research_depth,
                selected_analysts=request.parameters.selected_analysts if request.parameters else ["market", "fundamentals"],
                quick_model=quick_model,
                deep_model=deep_model,
                llm_provider=quick_provider,
                market_type=market_type
            )
            
            # æ··åˆæ¨¡å¼é…ç½®
            config["quick_provider"] = quick_provider
            config["deep_provider"] = deep_provider_info["provider"]
            config["quick_backend_url"] = quick_provider_info["backend_url"]
            config["deep_backend_url"] = deep_provider_info["backend_url"]
            config["backend_url"] = quick_provider_info["backend_url"]

            update_progress_sync(9, "ğŸš€ åˆå§‹åŒ–AIåˆ†æå¼•æ“", "engine_initialization")
            trading_graph = self._get_trading_graph(config)
            
            start_time = datetime.now()
            analysis_date = datetime.now().strftime("%Y-%m-%d")
            if request.parameters and request.parameters.analysis_date:
                ad = request.parameters.analysis_date
                if isinstance(ad, datetime): analysis_date = ad.strftime("%Y-%m-%d")
                elif isinstance(ad, str): analysis_date = ad

            # ğŸ”§ æ™ºèƒ½æ—¥æœŸèŒƒå›´å¤„ç†ï¼šè·å–æœ€è¿‘10å¤©çš„æ•°æ®ï¼Œè‡ªåŠ¨å¤„ç†å‘¨æœ«/èŠ‚å‡æ—¥
            data_start_date, data_end_date = get_trading_date_range(analysis_date, lookback_days=10)
            logger.info(f"ğŸ“… åˆ†æç›®æ ‡æ—¥æœŸ: {analysis_date}, æ•°æ®èŒƒå›´: {data_start_date} è‡³ {data_end_date}")

            update_progress_sync(10, "ğŸ¤– å¼€å§‹å¤šæ™ºèƒ½ä½“åä½œåˆ†æ", "agent_analysis")

            # è¿›åº¦å›è°ƒ
            node_progress_map = {
                "ğŸ“Š å¸‚åœºåˆ†æå¸ˆ": 27.5, "ğŸ’¼ åŸºæœ¬é¢åˆ†æå¸ˆ": 45, "ğŸ“° æ–°é—»åˆ†æå¸ˆ": 27.5, "ğŸ’¬ ç¤¾äº¤åª’ä½“åˆ†æå¸ˆ": 27.5,
                "ğŸ‚ çœ‹æ¶¨ç ”ç©¶å‘˜": 51.25, "ğŸ» çœ‹è·Œç ”ç©¶å‘˜": 57.5, "ğŸ‘” ç ”ç©¶ç»ç†": 70,
                "ğŸ’¼ äº¤æ˜“å‘˜å†³ç­–": 78,
                "ğŸ”¥ æ¿€è¿›é£é™©è¯„ä¼°": 81.75, "ğŸ›¡ï¸ ä¿å®ˆé£é™©è¯„ä¼°": 85.5, "âš–ï¸ ä¸­æ€§é£é™©è¯„ä¼°": 89.25, "ğŸ¯ é£é™©ç»ç†": 93,
                "ğŸ“Š ç”ŸæˆæŠ¥å‘Š": 97,
            }

            def graph_progress_callback(message: str):
                try:
                    if not progress_tracker: return
                    progress_pct = node_progress_map.get(message)
                    if progress_pct is not None:
                        current_progress = progress_tracker.progress_data.get('progress_percentage', 0)
                        if int(progress_pct) > current_progress:
                            # ä¼˜å…ˆä½¿ç”¨åŒæ­¥æ›´æ–°
                            update_progress_sync(int(progress_pct), message, message)
                        else:
                            progress_tracker.update_progress({'last_message': message})
                    else:
                        progress_tracker.update_progress({'last_message': message})
                except Exception:
                    pass

            # æ‰§è¡Œåˆ†æ
            state, decision = trading_graph.propagate(
                request.stock_code,
                analysis_date,
                progress_callback=graph_progress_callback,
                task_id=task_id
            )

            update_progress_sync(90, "å¤„ç†åˆ†æç»“æœ...", "result_processing")
            execution_time = (datetime.now() - start_time).total_seconds()

            # æå– reports ä» state
            reports = {}
            if isinstance(state, dict):
                report_keys = [
                    "market_report", "sentiment_report", "news_report", "fundamentals_report",
                    "bull_researcher", "bear_researcher", "research_team_decision",
                    "trader_investment_plan",
                    "risky_analyst", "safe_analyst", "neutral_analyst", "risk_management_decision"
                ]
                for key in report_keys:
                    if key in state and state[key]:
                        content = state[key]
                        # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²æˆ–å¯åºåˆ—åŒ–çš„
                        if isinstance(content, str):
                            reports[key] = content
                        elif hasattr(content, "content") and isinstance(content.content, str):
                            # å¤„ç† LangChain Message å¯¹è±¡
                            reports[key] = content.content
                        else:
                            try:
                                reports[key] = str(content)
                            except:
                                pass

            # æ„å»ºç»“æœ (ç®€åŒ–ç‰ˆï¼Œå®Œæ•´ç‰ˆåœ¨ _save_analysis_result_web_style ä¸­é‡æ„)
            # è¿™é‡Œç›´æ¥è¿”å›å­—å…¸
            result = {
                "stock_code": request.stock_code,
                "stock_symbol": request.stock_code,
                "analysis_date": analysis_date,
                "summary": str(decision.get("summary", ""))[:200] if isinstance(decision, dict) else "",
                "recommendation": str(decision.get("recommendation", "")) if isinstance(decision, dict) else "",
                "confidence_score": decision.get("confidence_score", 0.0) if isinstance(decision, dict) else 0.0,
                "risk_level": decision.get("risk_level", "ä¸­ç­‰") if isinstance(decision, dict) else "ä¸­ç­‰",
                "detailed_analysis": decision,
                "execution_time": execution_time,
                "state": state,
                "reports": reports,  # ğŸ”¥ æ·»åŠ æå–çš„æŠ¥å‘Š
                "decision": decision,
                "model_info": decision.get('model_info', 'Unknown') if isinstance(decision, dict) else 'Unknown',
                "analysts": request.parameters.selected_analysts if request.parameters else [],
                "research_depth": request.parameters.research_depth if request.parameters else "å¿«é€Ÿ",
            }
            return result

        except Exception as e:
            logger.error(f"âŒ åˆ†ææ‰§è¡Œå¤±è´¥: {task_id} - {e}")
            raise

    # -------------------------------------------------------------------------
    # Status & Saving Methods
    # -------------------------------------------------------------------------

    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€ (åŒ…å«è¯¦ç»†è¿›åº¦)"""
        global_memory_manager = get_memory_state_manager()
        result = await global_memory_manager.get_task_dict(task_id)
        if result:
            redis_progress = get_progress_by_id(task_id)
            if redis_progress:
                result.update({
                    'progress': redis_progress.get('progress_percentage', result.get('progress', 0)),
                    'message': redis_progress.get('last_message', result.get('message', '')),
                    'steps': redis_progress.get('steps', [])
                })
        return result

    async def list_all_tasks(self, status: Optional[str] = None, limit: int = 20, offset: int = 0) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨ (æ•°æ®åº“ + å†…å­˜çŠ¶æ€åˆå¹¶)"""
        # å…¼å®¹æ€§å¤„ç†ï¼šprocessing -> running
        if status == "processing":
            status = "running"
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {}
        if status:
            query["status"] = status
        
        try:
            db = get_mongo_db()
            cursor = db.analysis_tasks.find(query).sort("created_at", -1).skip(offset).limit(limit)
            db_tasks = await cursor.to_list(length=limit)
            
            results = []
            for task in db_tasks:
                if "_id" in task:
                    task["_id"] = str(task["_id"])
                
                task_id = task.get("task_id")
                if task_id:
                    memory_task = await self.memory_manager.get_task_dict(task_id)
                    if memory_task:
                        task["status"] = memory_task.get("status", task.get("status"))
                        task["progress"] = memory_task.get("progress", task.get("progress"))
                        task["message"] = memory_task.get("message", task.get("message"))
                        task["current_step"] = memory_task.get("current_step", task.get("current_step"))
                
                results.append(task)
            
            enriched = self._enrich_stock_names(results)
            return self._serialize_for_response(enriched)
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨å¤±è´¥ (DB): {e}")
            status_enum = None
            if status:
                try:
                    status_enum = TaskStatus(status)
                except ValueError:
                    logger.warning(f"âš ï¸ æ— æ•ˆçš„ä»»åŠ¡çŠ¶æ€è¿‡æ»¤: {status}")
            
            tasks = await self.memory_manager.list_all_tasks(status=status_enum, limit=limit, offset=offset)
            enriched = self._enrich_stock_names(tasks)
            return self._serialize_for_response(enriched)

    async def list_user_tasks(self, user_id: str, status: Optional[str] = None, limit: int = 20, offset: int = 0) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨ (æ•°æ®åº“ + å†…å­˜çŠ¶æ€åˆå¹¶)"""
        # å…¼å®¹æ€§å¤„ç†ï¼šprocessing -> running
        if status == "processing":
            status = "running"
            
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {"user_id": user_id}
        if status:
            query["status"] = status
            
        try:
            db = get_mongo_db()
            # æŒ‰åˆ›å»ºæ—¶é—´å€’åº
            cursor = db.analysis_tasks.find(query).sort("created_at", -1).skip(offset).limit(limit)
            db_tasks = await cursor.to_list(length=limit)
            
            # è½¬æ¢ä¸ºå‰ç«¯å‹å¥½çš„æ ¼å¼ï¼Œå¹¶åˆå¹¶å†…å­˜ä¸­çš„å®æ—¶çŠ¶æ€
            results = []
            for task in db_tasks:
                # è½¬æ¢ ObjectId ç­‰
                if "_id" in task:
                    task["_id"] = str(task["_id"])
                
                # å°è¯•ä»å†…å­˜è·å–æœ€æ–°çŠ¶æ€
                task_id = task.get("task_id")
                if task_id:
                    memory_task = await self.memory_manager.get_task_dict(task_id)
                    if memory_task:
                        # å†…å­˜ä¸­çš„çŠ¶æ€é€šå¸¸æ›´æ–°ï¼ˆå°¤å…¶æ˜¯è¿›åº¦å’Œæ¶ˆæ¯ï¼‰
                        # æˆ‘ä»¬ä¸»è¦å…³å¿ƒ status, progress, message, current_step
                        task["status"] = memory_task.get("status", task.get("status"))
                        task["progress"] = memory_task.get("progress", task.get("progress"))
                        task["message"] = memory_task.get("message", task.get("message"))
                        task["current_step"] = memory_task.get("current_step", task.get("current_step"))
                
                results.append(task)
            
            # å¦‚æœæ•°æ®åº“è¿”å›ä¸ºç©ºï¼Œå¯èƒ½æ˜¯å› ä¸ºæ‰€æœ‰æ•°æ®éƒ½åœ¨å†…å­˜ä¸­ï¼ˆæå°‘è§æƒ…å†µï¼Œä¾‹å¦‚DBå†™å…¥å¤±è´¥ä½†å†…å­˜æˆåŠŸï¼‰
            # æˆ–è€…å¦‚æœæ˜¯åˆšå¯åŠ¨ï¼ŒDB ä¸ºç©ºä¹Ÿæ˜¯æ­£å¸¸çš„ã€‚
            # è¿™é‡Œæˆ‘ä»¬åªè¿”å› DB çš„ç»“æœï¼Œå› ä¸º create_analysis_task ä¿è¯äº†å…ˆå†™ DBã€‚
            
            enriched = self._enrich_stock_names(results)
            return self._serialize_for_response(enriched)
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨å¤±è´¥ (DB): {e}")
            # é™çº§ï¼šå¦‚æœ DB å¤±è´¥ï¼Œå°è¯•è¿”å›å†…å­˜ä¸­çš„æ•°æ®
            status_enum = None
            if status:
                try:
                    status_enum = TaskStatus(status)
                except ValueError:
                    pass
                    
            tasks = await self.memory_manager.list_user_tasks(
                user_id=user_id, 
                status=status_enum, 
                limit=limit, 
                offset=offset
            )
            enriched = self._enrich_stock_names(tasks)
            return self._serialize_for_response(enriched)

    async def query_user_tasks(
        self,
        user_id: str,
        status: Optional[str] = None,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        symbol: Optional[str] = None,
        market_type: Optional[str] = None,
        page: int = 1,
        page_size: int = 20
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨ï¼ˆæ”¯æŒå¤æ‚ç­›é€‰ä¸åˆ†é¡µï¼‰"""
        # å…¼å®¹æ€§å¤„ç†
        if status == "processing":
            status = "running"

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query = {"user_id": user_id}
        
        if status:
            query["status"] = status
            
        if symbol:
            # åŒæ—¶åŒ¹é… symbol å’Œ stock_code
            query["$or"] = [
                {"symbol": symbol},
                {"stock_code": symbol},
                {"stock_symbol": symbol}
            ]
            
        if market_type:
            query["parameters.market_type"] = market_type
            
        # æ—¶é—´èŒƒå›´æŸ¥è¯¢
        date_query = {}
        if start_date:
            try:
                # å‡è®¾ä¼ å…¥çš„æ˜¯ YYYY-MM-DD
                s_date = datetime.strptime(start_date, "%Y-%m-%d")
                date_query["$gte"] = s_date
            except:
                pass
        if end_date:
            try:
                e_date = datetime.strptime(end_date, "%Y-%m-%d")
                # ç»“æŸæ—¥æœŸåŠ ä¸€å¤©ï¼ŒåŒ…å«å½“å¤©
                e_date = e_date.replace(hour=23, minute=59, second=59, microsecond=999999)
                date_query["$lte"] = e_date
            except:
                pass
                
        if date_query:
            query["created_at"] = date_query

        try:
            db = get_mongo_db()
            
            # è·å–æ€»æ•°
            total = await db.analysis_tasks.count_documents(query)
            
            # åˆ†é¡µæŸ¥è¯¢
            skip = (page - 1) * page_size
            cursor = db.analysis_tasks.find(query).sort("created_at", -1).skip(skip).limit(page_size)
            db_tasks = await cursor.to_list(length=page_size)
            
            # è½¬æ¢ä¸ºå‰ç«¯å‹å¥½çš„æ ¼å¼ï¼Œå¹¶åˆå¹¶å†…å­˜ä¸­çš„å®æ—¶çŠ¶æ€
            results = []
            for task in db_tasks:
                if "_id" in task:
                    task["_id"] = str(task["_id"])
                
                task_id = task.get("task_id")
                if task_id:
                    memory_task = await self.memory_manager.get_task_dict(task_id)
                    if memory_task:
                        task["status"] = memory_task.get("status", task.get("status"))
                        task["progress"] = memory_task.get("progress", task.get("progress"))
                        task["message"] = memory_task.get("message", task.get("message"))
                        task["current_step"] = memory_task.get("current_step", task.get("current_step"))
                
                results.append(task)
                
            enriched_tasks = self._enrich_stock_names(results)
            
            return self._serialize_for_response({
                "tasks": enriched_tasks,
                "total": total,
                "page": page,
                "page_size": page_size
            })
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢ç”¨æˆ·ä»»åŠ¡åˆ—è¡¨å¤±è´¥ (DB): {e}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨ list_user_tasks è·å–å¹¶æ‰‹åŠ¨è¿‡æ»¤ï¼ˆä¸å¤ªç²¾ç¡®ä½†å¯ç”¨ï¼‰
            all_tasks = await self.list_user_tasks(user_id, status, limit=1000) # è·å–æœ€è¿‘1000æ¡
            
            # æ‰‹åŠ¨è¿‡æ»¤
            filtered = []
            for t in all_tasks:
                if symbol:
                    s = t.get("symbol") or t.get("stock_code") or t.get("stock_symbol")
                    if s != symbol:
                        continue
                if market_type and t.get("parameters", {}).get("market_type") != market_type:
                    continue
                filtered.append(t)
                
            # æ‰‹åŠ¨åˆ†é¡µ
            start = (page - 1) * page_size
            paginated = filtered[start : start + page_size]
            
            return self._serialize_for_response({
                "tasks": paginated,
                "total": len(filtered),
                "page": page,
                "page_size": page_size
            })

    async def cleanup_zombie_tasks(self, max_running_hours: int = 2) -> Dict[str, Any]:
        """æ¸…ç†åƒµå°¸ä»»åŠ¡"""
        return await self.memory_manager.cleanup_zombie_tasks(max_running_hours)

    async def _update_task_status(self, task_id: str, status: AnalysisStatus, progress: int, error_message: str = None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€åˆ°MongoDB"""
        try:
            db = get_mongo_db()
            update_data = {"status": status, "progress": progress, "updated_at": datetime.utcnow()}
            if status == AnalysisStatus.PROCESSING and progress == 10:
                update_data["started_at"] = datetime.utcnow()
            elif status == AnalysisStatus.COMPLETED:
                update_data["completed_at"] = datetime.utcnow()
            elif status == AnalysisStatus.FAILED:
                update_data["last_error"] = error_message
                update_data["completed_at"] = datetime.utcnow()
            await db.analysis_tasks.update_one({"task_id": task_id}, {"$set": update_data})
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id} - {e}")

    async def _save_analysis_results_complete(self, task_id: str, result: Dict[str, Any]):
        """å®Œæ•´çš„åˆ†æç»“æœä¿å­˜"""
        try:
            stock_symbol = result.get('stock_symbol') or result.get('stock_code', 'UNKNOWN')
            # 1. ä¿å­˜åˆ°æœ¬åœ°
            await self._save_modular_reports_to_data_dir(result, stock_symbol)
            # 2. ä¿å­˜åˆ°æ•°æ®åº“ (Web Style)
            await self._save_analysis_result_web_style(task_id, result)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    async def _save_modular_reports_to_data_dir(self, result: Dict[str, Any], stock_symbol: str) -> Dict[str, str]:
        """ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šåˆ°dataç›®å½• - å®Œå…¨é‡‡ç”¨webç›®å½•çš„æ–‡ä»¶ç»“æ„"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„è·å–æ–¹å¼
            runtime_base = settings.RUNTIME_BASE_DIR
            results_dir = get_analysis_results_dir(runtime_base)
            
            analysis_date_raw = result.get('analysis_date', datetime.now())
            
            # ç¡®ä¿ analysis_date æ˜¯å­—ç¬¦ä¸²æ ¼å¼
            if isinstance(analysis_date_raw, datetime):
                analysis_date_str = analysis_date_raw.strftime('%Y-%m-%d')
            elif isinstance(analysis_date_raw, str):
                # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œæ£€æŸ¥æ ¼å¼
                try:
                    # å°è¯•è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
                    datetime.strptime(analysis_date_raw, '%Y-%m-%d')
                    analysis_date_str = analysis_date_raw
                except ValueError:
                    # å¦‚æœæ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                    analysis_date_str = datetime.now().strftime('%Y-%m-%d')
            else:
                # å…¶ä»–ç±»å‹ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                analysis_date_str = datetime.now().strftime('%Y-%m-%d')
            
            stock_dir = results_dir / stock_symbol / analysis_date_str
            reports_dir = stock_dir / "reports"
            reports_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºmessage_tool.logæ–‡ä»¶ - ä¸webç›®å½•ä¿æŒä¸€è‡´
            log_file = stock_dir / "message_tool.log"
            log_file.touch(exist_ok=True)
            
            state = result.get('state', {})
            saved_files = {}
            
            # å®šä¹‰æŠ¥å‘Šæ¨¡å—æ˜ å°„ - å®Œå…¨æŒ‰ç…§webç›®å½•çš„å®šä¹‰
            report_modules = {
                'market_report': {'filename': 'market_report.md', 'title': f'{stock_symbol} è‚¡ç¥¨æŠ€æœ¯åˆ†ææŠ¥å‘Š', 'state_key': 'market_report'},
                'sentiment_report': {'filename': 'sentiment_report.md', 'title': f'{stock_symbol} å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š', 'state_key': 'sentiment_report'},
                'news_report': {'filename': 'news_report.md', 'title': f'{stock_symbol} æ–°é—»äº‹ä»¶åˆ†ææŠ¥å‘Š', 'state_key': 'news_report'},
                'fundamentals_report': {'filename': 'fundamentals_report.md', 'title': f'{stock_symbol} åŸºæœ¬é¢åˆ†ææŠ¥å‘Š', 'state_key': 'fundamentals_report'},
                'investment_plan': {'filename': 'investment_plan.md', 'title': f'{stock_symbol} æŠ•èµ„å†³ç­–æŠ¥å‘Š', 'state_key': 'investment_plan'},
                'trader_investment_plan': {'filename': 'trader_investment_plan.md', 'title': f'{stock_symbol} äº¤æ˜“è®¡åˆ’æŠ¥å‘Š', 'state_key': 'trader_investment_plan'},
                'final_trade_decision': {'filename': 'final_trade_decision.md', 'title': f'{stock_symbol} æœ€ç»ˆæŠ•èµ„å†³ç­–', 'state_key': 'final_trade_decision'},
                'investment_debate_state': {'filename': 'research_team_decision.md', 'title': f'{stock_symbol} ç ”ç©¶å›¢é˜Ÿå†³ç­–æŠ¥å‘Š', 'state_key': 'investment_debate_state'},
                'risk_debate_state': {'filename': 'risk_management_decision.md', 'title': f'{stock_symbol} é£é™©ç®¡ç†å›¢é˜Ÿå†³ç­–æŠ¥å‘Š', 'state_key': 'risk_debate_state'}
            }
            
            # ä¿å­˜å„æ¨¡å—æŠ¥å‘Š - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            for module_key, module_info in report_modules.items():
                try:
                    state_key = module_info['state_key']
                    if state_key in state:
                        module_content = state[state_key]
                        if isinstance(module_content, str):
                            report_content = module_content
                        else:
                            report_content = str(module_content)
                        
                        file_path = reports_dir / module_info['filename']
                        with open(file_path, 'w', encoding='utf-8') as f:
                            f.write(report_content)
                        
                        saved_files[module_key] = str(file_path)
                        logger.info(f"âœ… ä¿å­˜æ¨¡å—æŠ¥å‘Š: {file_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ä¿å­˜æ¨¡å— {module_key} å¤±è´¥: {e}")
            
            # ä¿å­˜æœ€ç»ˆå†³ç­–æŠ¥å‘Š - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            decision = result.get('decision', {})
            if decision:
                decision_content = f"# {stock_symbol} æœ€ç»ˆæŠ•èµ„å†³ç­–\n\n"
                if isinstance(decision, dict):
                    decision_content += f"## æŠ•èµ„å»ºè®®\n\n"
                    decision_content += f"**è¡ŒåŠ¨**: {decision.get('action', 'N/A')}\n\n"
                    decision_content += f"**ç½®ä¿¡åº¦**: {decision.get('confidence', 0):.1%}\n\n"
                    decision_content += f"**é£é™©è¯„åˆ†**: {decision.get('risk_score', 0):.1%}\n\n"
                    decision_content += f"**ç›®æ ‡ä»·ä½**: {decision.get('target_price', 'N/A')}\n\n"
                    decision_content += f"## åˆ†ææ¨ç†\n\n{decision.get('reasoning', 'æš‚æ— åˆ†ææ¨ç†')}\n\n"
                else:
                    decision_content += f"{str(decision)}\n\n"
                
                decision_file = reports_dir / "final_trade_decision.md"
                with open(decision_file, 'w', encoding='utf-8') as f:
                    f.write(decision_content)
                saved_files['final_trade_decision'] = str(decision_file)
            
            # ä¿å­˜åˆ†æå…ƒæ•°æ®æ–‡ä»¶ - å®Œå…¨æŒ‰ç…§webç›®å½•çš„æ–¹å¼
            metadata = {
                'stock_symbol': stock_symbol,
                'analysis_date': analysis_date_str,
                'timestamp': datetime.now().isoformat(),
                'research_depth': result.get('research_depth', 1),
                'analysts': result.get('analysts', []),
                'status': 'completed',
                'reports_count': len(saved_files),
                'report_types': list(saved_files.keys())
            }
            
            metadata_file = reports_dir.parent / "analysis_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
            return saved_files
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æ¨¡å—æŠ¥å‘Šå¤±è´¥: {e}")
            return {}

    async def _save_analysis_result_web_style(self, task_id: str, result: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ (Web Style)"""
        try:
            db = get_mongo_db()
            stock_symbol = result.get('stock_symbol') or result.get('stock_code', 'UNKNOWN')
            timestamp = datetime.utcnow()
            analysis_id = result.get('analysis_id') or f"{stock_symbol}_{timestamp.strftime('%Y%m%d_%H%M%S')}"

            # å¤„ç† reportsï¼Œç¡®ä¿ä¸ºå­—ç¬¦ä¸²å†…å®¹ï¼Œé¿å…ç©ºå€¼
            raw_reports = result.get("reports") or {}
            cleaned_reports: Dict[str, str] = {}
            if isinstance(raw_reports, dict):
                for key, value in raw_reports.items():
                    if value is None:
                        continue
                    if isinstance(value, str):
                        content = value.strip()
                    else:
                        # å¯¹éå­—ç¬¦ä¸²å†…å®¹è¿›è¡Œ JSON åºåˆ—åŒ–ï¼Œä¿æŒå¯è¯»
                        content = json.dumps(value, ensure_ascii=False, indent=2)
                    if content:
                        cleaned_reports[key] = content

            # å…³é”®å­—æ®µå…œåº•
            analysis_date = result.get('analysis_date') or timestamp.strftime('%Y-%m-%d')
            summary = result.get("summary", "")
            recommendation = result.get("recommendation", "")
            risk_level = result.get("risk_level", "ä¸­ç­‰")
            confidence_score = result.get("confidence_score", 0.0)
            key_points = result.get("key_points") or []
            analysts = result.get("analysts") or result.get("selected_analysts") or []
            research_depth = result.get("research_depth") or result.get("parameters", {}).get("research_depth") or "å¿«é€Ÿ"
            model_info = result.get("model_info") or result.get("llm_model") or "Unknown"
            tokens_used = result.get("tokens_used") or result.get("token_usage", {}).get("total_tokens", 0)
            execution_time = result.get("execution_time", 0)

            document = {
                "analysis_id": analysis_id,
                "stock_symbol": stock_symbol,
                "stock_name": self._resolve_stock_name(stock_symbol),
                "analysis_date": analysis_date,
                "status": result.get("status", "completed"),
                "decision": result.get("decision", {}),
                "task_id": task_id,
                "created_at": timestamp,
                "updated_at": timestamp,
                "summary": summary,
                "recommendation": recommendation,
                "reports": cleaned_reports,
                "confidence_score": confidence_score,
                "risk_level": risk_level,
                "key_points": key_points,
                "analysts": analysts,
                "research_depth": research_depth,
                "model_info": model_info,
                "tokens_used": tokens_used,
                "execution_time": execution_time,
                "source": result.get("source", "analysis_service")
            }

            # å†™å…¥æŠ¥å‘Šé›†åˆ
            insert_result = await db.analysis_reports.insert_one(document)

            # æ›´æ–°ä»»åŠ¡é›†åˆä¸­çš„ç»“æœï¼Œæºå¸¦ report_id ä¾¿äºå‰ç«¯å…³è”
            document_for_task = {**document, "_id": insert_result.inserted_id}
            await db.analysis_tasks.update_one(
                {"task_id": task_id},
                {"$set": {"result": document_for_task}}
            )
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜DBç»“æœå¤±è´¥: {e}")


# å…¨å±€åˆ†ææœåŠ¡å®ä¾‹
analysis_service: Optional[AnalysisService] = None

def get_analysis_service() -> AnalysisService:
    """è·å–åˆ†ææœåŠ¡å®ä¾‹"""
    global analysis_service
    if analysis_service is None:
        analysis_service = AnalysisService()
    return analysis_service


