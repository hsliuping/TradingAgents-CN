"""
è¶…çŸ­è¡Œæƒ…åˆ†ææœåŠ¡
ä¸“é—¨ç”¨äºåˆ†æAè‚¡è‚¡ç¥¨çš„è¶…çŸ­æœŸè¡Œæƒ…ï¼Œé¢„æµ‹æ˜æ—¥æ¶¨åœã€ä¸Šæ¶¨ã€ä¸‹è·Œçš„æ¦‚ç‡
"""

import asyncio
import uuid
import logging
from datetime import datetime
from typing import Dict, Any, Optional
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# åˆå§‹åŒ–TradingAgentsæ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import init_logging
init_logging()

from tradingagents.graph.trading_graph import create_llm_by_provider
from tradingagents.agents.analysts.short_term_analyst import create_short_term_analyst
from tradingagents.agents.utils.agent_utils import Toolkit
from tradingagents.agents.utils.agent_states import AgentState
from tradingagents.default_config import DEFAULT_CONFIG
from app.services.config_service import ConfigService
from app.core.database import get_mongo_db
from bson import ObjectId

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger("app.services.short_term_analysis_service")

# é…ç½®æœåŠ¡å®ä¾‹
config_service = ConfigService()


class ShortTermAnalysisService:
    """è¶…çŸ­è¡Œæƒ…åˆ†ææœåŠ¡ç±»"""

    def __init__(self):
        self._toolkit_cache = None
        logger.info(f"ğŸ”§ [æœåŠ¡åˆå§‹åŒ–] ShortTermAnalysisService å®ä¾‹ID: {id(self)}")

    def _get_toolkit(self, config: Dict[str, Any]) -> Toolkit:
        """è·å–æˆ–åˆ›å»ºToolkitå®ä¾‹"""
        if self._toolkit_cache is None:
            self._toolkit_cache = Toolkit(config)
            logger.info(f"âœ… Toolkitå®ä¾‹åˆ›å»ºæˆåŠŸ")
        return self._toolkit_cache

    async def analyze_short_term(
        self,
        ticker: str,
        analysis_date: str,
        llm_provider: str = "dashscope",
        llm_model: str = "qwen-max",
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¶…çŸ­è¡Œæƒ…åˆ†æ
        
        Args:
            ticker: è‚¡ç¥¨ä»£ç ï¼ˆAè‚¡6ä½æ•°å­—ï¼‰
            analysis_date: åˆ†ææ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
            llm_provider: LLMæä¾›å•†
            llm_model: LLMæ¨¡å‹åç§°
            user_id: ç”¨æˆ·IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            DictåŒ…å«åˆ†æç»“æœ
        """
        logger.info(f"ğŸš€ [è¶…çŸ­è¡Œæƒ…åˆ†æ] å¼€å§‹åˆ†æ: ticker={ticker}, date={analysis_date}")
        
        try:
            # 1. è·å–LLMé…ç½®ï¼ˆä½¿ç”¨ä¸æ­£å¸¸åˆ†æç›¸åŒçš„é€»è¾‘ï¼‰
            # ä½¿ç”¨ get_provider_and_url_by_model_sync è·å–å®Œæ•´çš„é…ç½®ä¿¡æ¯ï¼ˆåŒ…æ‹¬API Keyã€backend_urlç­‰ï¼‰
            from app.services.simple_analysis_service import get_provider_and_url_by_model_sync
            
            provider_info = get_provider_and_url_by_model_sync(llm_model)
            
            provider = provider_info["provider"]
            backend_url = provider_info.get("backend_url") or ""
            api_key = provider_info.get("api_key")  # å·²ç»ç»è¿‡éªŒè¯çš„API Keyï¼ˆä¼˜å…ˆçº§ï¼šæ¨¡å‹é…ç½® > å‚å®¶é…ç½® > ç¯å¢ƒå˜é‡ï¼‰
            
            logger.info(f"ğŸ” [è¶…çŸ­è¡Œæƒ…åˆ†æ] æ¨¡å‹ {llm_model} å¯¹åº”çš„ä¾›åº”å•†: {provider}")
            logger.info(f"ğŸ” [è¶…çŸ­è¡Œæƒ…åˆ†æ] backend_url: {backend_url}")
            logger.info(f"ğŸ”‘ [è¶…çŸ­è¡Œæƒ…åˆ†æ] API Key: {'å·²é…ç½®' if api_key else 'æœªé…ç½®ï¼ˆå°†ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰'}")
            if api_key:
                logger.info(f"ğŸ”‘ [è¶…çŸ­è¡Œæƒ…åˆ†æ] API Key é•¿åº¦: {len(api_key)}, å‰10ä½: {api_key[:10]}...")
            else:
                # å¦‚æœ api_key ä¸º Noneï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
                import os
                env_key_map = {
                    "dashscope": "DASHSCOPE_API_KEY",
                    "openai": "OPENAI_API_KEY",
                    "deepseek": "DEEPSEEK_API_KEY",
                    "google": "GOOGLE_API_KEY",
                }
                env_key_name = env_key_map.get(provider.lower())
                if env_key_name:
                    env_api_key = os.getenv(env_key_name)
                    if env_api_key:
                        logger.info(f"ğŸ”‘ [è¶…çŸ­è¡Œæƒ…åˆ†æ] ä»ç¯å¢ƒå˜é‡ {env_key_name} è·å– API Key")
                        api_key = env_api_key
            
            # 2. åˆ›å»ºLLMå®ä¾‹
            llm = create_llm_by_provider(
                provider=provider,
                model=llm_model,
                backend_url=backend_url,
                temperature=0.7,
                max_tokens=4000,
                timeout=180,
                api_key=api_key
            )
            
            logger.info(f"âœ… [è¶…çŸ­è¡Œæƒ…åˆ†æ] LLMåˆ›å»ºæˆåŠŸ: {provider}/{llm_model}")
            
            # 3. åˆ›å»ºé…ç½®
            config = DEFAULT_CONFIG.copy()
            config.update({
                "llm_provider": provider,
                "quick_think_llm": llm_model,
                "deep_think_llm": llm_model,
                "research_depth": "æ ‡å‡†",
                "online_tools": True,
            })
            
            # 4. åˆ›å»ºToolkit
            toolkit = self._get_toolkit(config)
            
            # 5. åˆ›å»ºè¶…çŸ­è¡Œæƒ…åˆ†æå¸ˆ
            short_term_analyst = create_short_term_analyst(llm, toolkit)
            
            # 6. åˆ›å»ºåˆå§‹çŠ¶æ€
            initial_state = {
                "company_of_interest": ticker,
                "trade_date": analysis_date,
                "messages": [],
                "short_term_report": None
            }
            
            # 7. æ‰§è¡Œåˆ†æï¼ˆå¯èƒ½éœ€è¦å¤šè½®å·¥å…·è°ƒç”¨ï¼‰
            current_state = initial_state
            max_iterations = 10  # æœ€å¤š10è½®è¿­ä»£
            iteration = 0
            
            while iteration < max_iterations:
                iteration += 1
                logger.info(f"ğŸ”„ [è¶…çŸ­è¡Œæƒ…åˆ†æ] ç¬¬ {iteration} è½®è¿­ä»£")
                
                # è°ƒç”¨åˆ†æå¸ˆ
                result = short_term_analyst(current_state)
                
                # æ›´æ–°çŠ¶æ€ï¼šåˆå¹¶æ¶ˆæ¯åˆ—è¡¨ï¼ˆè€Œä¸æ˜¯ç›´æ¥è¦†ç›–ï¼‰
                if "messages" in result:
                    # å°†æ–°æ¶ˆæ¯æ·»åŠ åˆ°ç°æœ‰æ¶ˆæ¯åˆ—è¡¨
                    if "messages" not in current_state:
                        current_state["messages"] = []
                    current_state["messages"].extend(result["messages"])
                
                # æ›´æ–°å…¶ä»–å­—æ®µ
                if "short_term_report" in result:
                    current_state["short_term_report"] = result["short_term_report"]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                has_tool_calls = False
                if current_state.get("messages"):
                    last_message = current_state["messages"][-1]
                    if last_message and hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                        has_tool_calls = True
                        logger.info(f"ğŸ”§ [è¶…çŸ­è¡Œæƒ…åˆ†æ] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œéœ€è¦æ‰§è¡Œå·¥å…·")
                        
                        # æ‰§è¡Œå·¥å…·è°ƒç”¨
                        from langgraph.prebuilt import ToolNode
                        tools = [
                            toolkit.get_stock_market_data_unified,
                            toolkit.get_stock_fundamentals_unified,
                            toolkit.get_realtime_stock_news,
                            toolkit.get_short_term_board_data
                        ]
                        tool_node = ToolNode(tools)
                        
                        # æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆToolNodeä¼šè‡ªåŠ¨å¤„ç†æ¶ˆæ¯æ ¼å¼ï¼‰
                        tool_result = tool_node.invoke(current_state)
                        
                        # æ›´æ–°çŠ¶æ€ï¼šåˆå¹¶å·¥å…·æ‰§è¡Œç»“æœçš„æ¶ˆæ¯
                        if "messages" in tool_result:
                            if "messages" not in current_state:
                                current_state["messages"] = []
                            current_state["messages"].extend(tool_result["messages"])
                        
                        logger.info(f"âœ… [è¶…çŸ­è¡Œæƒ…åˆ†æ] å·¥å…·æ‰§è¡Œå®Œæˆ")
                
                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜åˆ†æå®Œæˆ
                if not has_tool_calls:
                    logger.info(f"âœ… [è¶…çŸ­è¡Œæƒ…åˆ†æ] åˆ†æå®Œæˆï¼Œå…± {iteration} è½®è¿­ä»£")
                    break
            
            # 8. æå–åˆ†æç»“æœ
            report = current_state.get("short_term_report", "")
            if not report and current_state.get("messages"):
                # ä»æœ€åä¸€æ¡æ¶ˆæ¯ä¸­æå–å†…å®¹
                last_message = current_state["messages"][-1]
                if hasattr(last_message, 'content'):
                    report = last_message.content
                else:
                    report = str(last_message)
            
            # 9. è§£ææ¦‚ç‡å€¼ï¼ˆä»æŠ¥å‘Šä¸­æå–ï¼‰
            probabilities = self._parse_probabilities(report)
            
            # 10. æ„å»ºè¿”å›ç»“æœ
            result = {
                "success": True,
                "ticker": ticker,
                "analysis_date": analysis_date,
                "report": report,
                "probabilities": probabilities,
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"âœ… [è¶…çŸ­è¡Œæƒ…åˆ†æ] åˆ†æå®Œæˆ: {ticker}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ [è¶…çŸ­è¡Œæƒ…åˆ†æ] åˆ†æå¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "ticker": ticker,
                "analysis_date": analysis_date,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    def _parse_probabilities(self, report: str) -> Dict[str, float]:
        """
        ä»åˆ†ææŠ¥å‘Šä¸­è§£ææ¦‚ç‡å€¼
        
        Returns:
            DictåŒ…å«ä¸‰ä¸ªæ¦‚ç‡å€¼
        """
        probabilities = {
            "limit_up": None,  # æ¶¨åœæ¦‚ç‡
            "up": None,        # ä¸Šæ¶¨æ¦‚ç‡
            "down": None       # ä¸‹è·Œæ¦‚ç‡
        }
        
        try:
            # å°è¯•ä»æŠ¥å‘Šä¸­æå–æ¦‚ç‡å€¼
            import re
            
            # åŒ¹é…æ¨¡å¼ï¼šæ˜æ—¥æ¶¨åœæ¦‚ç‡: XX%
            limit_up_match = re.search(r'æ˜æ—¥æ¶¨åœæ¦‚ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%', report)
            if limit_up_match:
                probabilities["limit_up"] = float(limit_up_match.group(1))
            
            # åŒ¹é…æ¨¡å¼ï¼šæ˜æ—¥ä¸Šæ¶¨æ¦‚ç‡: XX%
            up_match = re.search(r'æ˜æ—¥ä¸Šæ¶¨æ¦‚ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%', report)
            if up_match:
                probabilities["up"] = float(up_match.group(1))
            
            # åŒ¹é…æ¨¡å¼ï¼šæ˜æ—¥ä¸‹è·Œæ¦‚ç‡: XX%
            down_match = re.search(r'æ˜æ—¥ä¸‹è·Œæ¦‚ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%', report)
            if down_match:
                probabilities["down"] = float(down_match.group(1))
                
        except Exception as e:
            logger.warning(f"âš ï¸ [è¶…çŸ­è¡Œæƒ…åˆ†æ] è§£ææ¦‚ç‡å€¼å¤±è´¥: {e}")
        
        return probabilities


# å…¨å±€æœåŠ¡å®ä¾‹
_short_term_analysis_service = None


def get_short_term_analysis_service() -> ShortTermAnalysisService:
    """è·å–è¶…çŸ­è¡Œæƒ…åˆ†ææœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _short_term_analysis_service
    if _short_term_analysis_service is None:
        _short_term_analysis_service = ShortTermAnalysisService()
    return _short_term_analysis_service

