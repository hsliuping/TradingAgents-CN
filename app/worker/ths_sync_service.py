"""
åŒèŠ±é¡ºé¢˜æåŒæ­¥æœåŠ¡
è´Ÿè´£åŒæ­¥åŒèŠ±é¡ºé¢˜ææ•°æ®åˆ°MongoDB
- limit_cpt_list: æœ€å¼ºæ¿å—ç»Ÿè®¡
- ths_member: åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æˆåˆ†
"""
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import logging

from tradingagents.dataflows.providers.china.tushare import TushareProvider
from app.core.database import get_mongo_db
from app.core.config import settings
from app.core.rate_limiter import get_tushare_rate_limiter
from pymongo import UpdateOne
from pymongo.errors import BulkWriteError

logger = logging.getLogger(__name__)


class THSSyncService:
    """
    åŒèŠ±é¡ºé¢˜æåŒæ­¥æœåŠ¡
    è´Ÿè´£å°†åŒèŠ±é¡ºé¢˜ææ•°æ®åŒæ­¥åˆ°MongoDB
    """
    
    def __init__(self):
        self.provider = TushareProvider()
        self.db = get_mongo_db()
        self.settings = settings
        
        # é€Ÿç‡é™åˆ¶å™¨
        tushare_tier = getattr(settings, "TUSHARE_TIER", "standard")
        safety_margin = float(getattr(settings, "TUSHARE_RATE_LIMIT_SAFETY_MARGIN", "0.8"))
        self.rate_limiter = get_tushare_rate_limiter(tier=tushare_tier, safety_margin=safety_margin)
        
        # æ‰¹é‡å¤„ç†é…ç½®
        self.batch_size = 500
        self._indexes_ensured = {
            "ths_limit_cpt_list": False,
            "ths_member": False,
            "ths_hot": False
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–åŒæ­¥æœåŠ¡"""
        success = await self.provider.connect()
        if not success:
            raise RuntimeError("âŒ Tushareè¿æ¥å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨åŒèŠ±é¡ºé¢˜æåŒæ­¥æœåŠ¡")
        logger.info("âœ… åŒèŠ±é¡ºé¢˜æåŒæ­¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    # ==================== ç´¢å¼•ç®¡ç† ====================
    
    async def _ensure_indexes(self, collection_name: str):
        """ç¡®ä¿å¿…è¦çš„ç´¢å¼•å­˜åœ¨"""
        if self._indexes_ensured.get(collection_name, False):
            return
        
        try:
            collection = self.db[collection_name]
            existing_indexes = await collection.list_indexes().to_list(length=None)
            existing_index_names = [idx["name"] for idx in existing_indexes]
            
            if collection_name == "ths_limit_cpt_list":
                # å¤åˆå”¯ä¸€ç´¢å¼•ï¼štrade_date + ts_code
                if "trade_date_ts_code_unique" not in existing_index_names:
                    await collection.create_index(
                        [("trade_date", 1), ("ts_code", 1)],
                        unique=True,
                        name="trade_date_ts_code_unique"
                    )
                
                # äº¤æ˜“æ—¥æœŸç´¢å¼•ï¼ˆé™åºï¼‰
                if "trade_date_desc" not in existing_index_names:
                    await collection.create_index(
                        [("trade_date", -1)],
                        name="trade_date_desc"
                    )
                
                # æ¿å—ä»£ç ç´¢å¼•
                if "ts_code_index" not in existing_index_names:
                    await collection.create_index(
                        [("ts_code", 1)],
                        name="ts_code_index"
                    )
                
                # æ’åç´¢å¼•ï¼ˆç”¨äºæ’åºï¼‰
                if "rank_index" not in existing_index_names:
                    await collection.create_index(
                        [("rank", 1)],
                        name="rank_index"
                    )
                
                # æ¶¨åœå®¶æ•°ç´¢å¼•ï¼ˆé™åºï¼‰
                if "up_nums_desc" not in existing_index_names:
                    await collection.create_index(
                        [("up_nums", -1)],
                        name="up_nums_desc"
                    )
            
            elif collection_name == "ths_member":
                # å¤åˆå”¯ä¸€ç´¢å¼•ï¼šts_code + con_code
                if "ts_code_con_code_unique" not in existing_index_names:
                    await collection.create_index(
                        [("ts_code", 1), ("con_code", 1)],
                        unique=True,
                        name="ts_code_con_code_unique"
                    )
                
                # æ¿å—ä»£ç ç´¢å¼•
                if "ts_code_index" not in existing_index_names:
                    await collection.create_index(
                        [("ts_code", 1)],
                        name="ts_code_index"
                    )
                
                # è‚¡ç¥¨ä»£ç ç´¢å¼•
                if "con_code_index" not in existing_index_names:
                    await collection.create_index(
                        [("con_code", 1)],
                        name="con_code_index"
                    )
                
                # æ˜¯å¦æœ€æ–°ç´¢å¼•
                if "is_new_index" not in existing_index_names:
                    await collection.create_index(
                        [("is_new", 1)],
                        name="is_new_index"
                    )
            
            elif collection_name == "ths_hot":
                # å¤åˆå”¯ä¸€ç´¢å¼•ï¼štrade_date + market + ts_code + rank_time
                if "trade_date_market_ts_code_rank_time_unique" not in existing_index_names:
                    await collection.create_index(
                        [("trade_date", 1), ("market", 1), ("ts_code", 1), ("rank_time", 1)],
                        unique=True,
                        name="trade_date_market_ts_code_rank_time_unique"
                    )
                
                # äº¤æ˜“æ—¥æœŸç´¢å¼•ï¼ˆé™åºï¼‰
                if "trade_date_desc" not in existing_index_names:
                    await collection.create_index(
                        [("trade_date", -1)],
                        name="trade_date_desc"
                    )
                
                # çƒ­æ¦œç±»å‹ç´¢å¼•
                if "market_index" not in existing_index_names:
                    await collection.create_index(
                        [("market", 1)],
                        name="market_index"
                    )
                
                # æ•°æ®ç±»å‹ç´¢å¼•
                if "data_type_index" not in existing_index_names:
                    await collection.create_index(
                        [("data_type", 1)],
                        name="data_type_index"
                    )
                
                # æ’è¡Œç´¢å¼•ï¼ˆç”¨äºæ’åºï¼‰
                if "rank_index" not in existing_index_names:
                    await collection.create_index(
                        [("rank", 1)],
                        name="rank_index"
                    )
                
                # çƒ­åº¦å€¼ç´¢å¼•ï¼ˆé™åºï¼‰
                if "hot_desc" not in existing_index_names:
                    await collection.create_index(
                        [("hot", -1)],
                        name="hot_desc"
                    )
                
                # æ˜¯å¦æœ€æ–°ç´¢å¼•
                if "is_new_index" not in existing_index_names:
                    await collection.create_index(
                        [("is_new", 1)],
                        name="is_new_index"
                    )
            
            self._indexes_ensured[collection_name] = True
            logger.debug(f"âœ… {collection_name} ç´¢å¼•æ£€æŸ¥å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ åˆ›å»º {collection_name} ç´¢å¼•æ—¶å‡ºé”™: {e}")
    
    # ==================== æœ€å¼ºæ¿å—ç»Ÿè®¡åŒæ­¥ ====================
    
    async def sync_limit_cpt_list(self, trade_date: Optional[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥æœ€å¼ºæ¿å—ç»Ÿè®¡æ•°æ®
        
        Args:
            trade_date: äº¤æ˜“æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
        
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥åŒèŠ±é¡ºæœ€å¼ºæ¿å—ç»Ÿè®¡...")
        
        stats = {
            "total_processed": 0,
            "inserted": 0,
            "updated": 0,
            "errors": 0,
            "start_time": datetime.utcnow(),
            "errors_list": []
        }
        
        try:
            # ç¡®ä¿ç´¢å¼•å­˜åœ¨
            await self._ensure_indexes("ths_limit_cpt_list")
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            if not trade_date:
                trade_date = await self._get_latest_trade_date()
            
            logger.info(f"ğŸ“… åŒæ­¥æ—¥æœŸ: {trade_date}")
            
            # ç­‰å¾…é€Ÿç‡é™åˆ¶
            await self.rate_limiter.acquire()
            
            # è°ƒç”¨Tushare APIè·å–æœ€å¼ºæ¿å—ç»Ÿè®¡
            df = await asyncio.to_thread(
                self.provider.api.limit_cpt_list,
                trade_date=trade_date
            )
            
            if df is None or df.empty:
                logger.warning(f"âš ï¸ æ—¥æœŸ {trade_date} æ— æœ€å¼ºæ¿å—ç»Ÿè®¡æ•°æ®")
                stats["errors"] = 1
                stats["errors_list"].append(f"æ—¥æœŸ {trade_date} æ— æ•°æ®")
                return stats
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            records = df.to_dict('records')
            stats["total_processed"] = len(records)
            
            # æ‰¹é‡å†™å…¥MongoDB
            operations = []
            now_iso = datetime.utcnow().isoformat()
            
            for record in records:
                # éªŒè¯å¿…éœ€å­—æ®µ
                ts_code = str(record.get("ts_code", "")).strip()
                if not ts_code:
                    logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè®°å½•ï¼ˆts_codeä¸ºç©ºï¼‰: {record}")
                    continue
                
                # å­˜å‚¨æ‰€æœ‰å­—æ®µ
                doc = {
                    "trade_date": str(record.get("trade_date", trade_date)),
                    "ts_code": ts_code,  # æ¿å—ä»£ç 
                    "name": str(record.get("name", "")),  # æ¿å—åç§°
                    "days": record.get("days"),  # ä¸Šæ¦œå¤©æ•°
                    "up_stat": str(record.get("up_stat", "")),  # è¿æ¿é«˜åº¦
                    "cons_nums": record.get("cons_nums"),  # è¿æ¿å®¶æ•°
                    "up_nums": str(record.get("up_nums", "")),  # æ¶¨åœå®¶æ•°
                    "pct_chg": record.get("pct_chg"),  # æ¶¨è·Œå¹…%
                    "rank": str(record.get("rank", "")),  # æ¿å—çƒ­ç‚¹æ’å
                    "data_source": "tushare",
                    "updated_at": now_iso
                }
                
                # ä¿ç•™æ‰€æœ‰åŸå§‹å­—æ®µï¼ˆç”¨äºè°ƒè¯•å’Œæ‰©å±•ï¼‰
                for key, value in record.items():
                    if key not in doc and value is not None:
                        doc[key] = value
                
                # ä½¿ç”¨ trade_date + ts_code ä½œä¸ºå”¯ä¸€é”®
                operations.append(
                    UpdateOne(
                        {
                            "trade_date": doc["trade_date"],
                            "ts_code": doc["ts_code"]
                        },
                        {"$set": doc},
                        upsert=True
                    )
                )
            
            # æ‰¹é‡å†™å…¥
            if operations:
                try:
                    result = await self.db["ths_limit_cpt_list"].bulk_write(operations, ordered=False)
                    stats["inserted"] = result.upserted_count
                    stats["updated"] = result.modified_count
                    logger.info(
                        f"âœ… æœ€å¼ºæ¿å—ç»Ÿè®¡æ•°æ®åŒæ­¥å®Œæˆ: "
                        f"æ–°å¢ {result.upserted_count} æ¡, "
                        f"æ›´æ–° {result.modified_count} æ¡, "
                        f"æ€»è®¡ {stats['total_processed']} æ¡"
                    )
                except BulkWriteError as e:
                    # è®°å½•éƒ¨åˆ†æˆåŠŸçš„å†™å…¥
                    stats["inserted"] = e.details.get("nInserted", 0)
                    stats["updated"] = e.details.get("nModified", 0)
                    stats["errors"] = len(e.details.get("writeErrors", []))
                    logger.error(f"âŒ æ‰¹é‡å†™å…¥æœ€å¼ºæ¿å—ç»Ÿè®¡æ•°æ®æ—¶å‡ºç°é”™è¯¯: {e.details}")
                    stats["errors_list"].append(f"æ‰¹é‡å†™å…¥é”™è¯¯: {str(e)}")
            else:
                logger.warning("âš ï¸ æœªç”Ÿæˆä»»ä½•æœ€å¼ºæ¿å—ç»Ÿè®¡æ•°æ®")
            
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
            
        except Exception as e:
            logger.exception(f"âŒ åŒæ­¥æœ€å¼ºæ¿å—ç»Ÿè®¡æ•°æ®å¤±è´¥: {e}")
            stats["errors"] = 1
            stats["errors_list"].append(str(e))
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds() if stats.get("end_time") else 0
        
        return stats
    
    # ==================== åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æˆåˆ†åŒæ­¥ ====================
    
    async def sync_ths_member(self, ts_codes: Optional[List[str]] = None, trade_date: Optional[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æˆåˆ†æ•°æ®
        
        Args:
            ts_codes: æ¿å—ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä» limit_cpt_list è·å–
            trade_date: äº¤æ˜“æ—¥æœŸï¼ˆç”¨äºä» limit_cpt_list è·å–æ¿å—ä»£ç ï¼‰
        
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æˆåˆ†...")
        
        stats = {
            "total_processed": 0,
            "inserted": 0,
            "updated": 0,
            "errors": 0,
            "concepts_processed": 0,
            "concepts_failed": 0,
            "start_time": datetime.utcnow(),
            "errors_list": []
        }
        
        try:
            # ç¡®ä¿ç´¢å¼•å­˜åœ¨
            await self._ensure_indexes("ths_member")
            
            # å¦‚æœæ²¡æœ‰æä¾›æ¿å—ä»£ç åˆ—è¡¨ï¼Œä» limit_cpt_list è·å–
            if not ts_codes:
                if not trade_date:
                    trade_date = await self._get_latest_trade_date()
                
                logger.info(f"ğŸ“… ä» limit_cpt_list è·å–æ¿å—ä»£ç ï¼ˆæ—¥æœŸ: {trade_date}ï¼‰")
                
                # ä» limit_cpt_list è·å–æ¿å—ä»£ç åˆ—è¡¨
                cursor = self.db["ths_limit_cpt_list"].find(
                    {"trade_date": trade_date},
                    {"ts_code": 1}
                )
                limit_cpt_records = await cursor.to_list(length=None)
                
                if not limit_cpt_records:
                    # å°è¯•æŸ¥æ‰¾æœ€è¿‘æœ‰æ•°æ®çš„äº¤æ˜“æ—¥
                    logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ—¥æœŸ {trade_date} çš„æœ€å¼ºæ¿å—æ•°æ®ï¼Œå°è¯•æŸ¥æ‰¾æœ€è¿‘æœ‰æ•°æ®çš„äº¤æ˜“æ—¥...")
                    
                    # æŸ¥æ‰¾æœ€è¿‘30å¤©å†…æœ‰æ•°æ®çš„äº¤æ˜“æ—¥
                    recent_cursor = self.db["ths_limit_cpt_list"].find(
                        {},
                        {"trade_date": 1, "ts_code": 1}
                    ).sort("trade_date", -1).limit(1)
                    
                    recent_record = await recent_cursor.to_list(length=1)
                    
                    if recent_record:
                        latest_available_date = recent_record[0].get("trade_date")
                        logger.info(f"ğŸ“… æ‰¾åˆ°æœ€è¿‘æœ‰æ•°æ®çš„äº¤æ˜“æ—¥: {latest_available_date}ï¼Œä½¿ç”¨è¯¥æ—¥æœŸçš„æ¿å—ä»£ç ")
                        
                        # ä½¿ç”¨æœ€è¿‘æœ‰æ•°æ®çš„äº¤æ˜“æ—¥è·å–æ¿å—ä»£ç 
                        latest_cursor = self.db["ths_limit_cpt_list"].find(
                            {"trade_date": latest_available_date},
                            {"ts_code": 1}
                        )
                        limit_cpt_records = await latest_cursor.to_list(length=None)
                    else:
                        logger.error(f"âŒ æ•°æ®åº“ä¸­æ²¡æœ‰ä»»ä½• limit_cpt_list æ•°æ®ï¼Œè¯·å…ˆåŒæ­¥ limit_cpt_list")
                        stats["errors"] = 1
                        stats["errors_list"].append(f"æ•°æ®åº“ä¸­æ²¡æœ‰ä»»ä½• limit_cpt_list æ•°æ®")
                        return stats
                
                if not limit_cpt_records:
                    logger.error(f"âŒ æ— æ³•è·å–æ¿å—ä»£ç åˆ—è¡¨")
                    stats["errors"] = 1
                    stats["errors_list"].append(f"æ— æ³•è·å–æ¿å—ä»£ç åˆ—è¡¨")
                    return stats
                
                ts_codes = [record.get("ts_code", "") for record in limit_cpt_records if record.get("ts_code")]
                logger.info(f"ğŸ“Š æ‰¾åˆ° {len(ts_codes)} ä¸ªæ¿å—ä»£ç ï¼Œå¼€å§‹åŒæ­¥æˆåˆ†è‚¡...")
            
            # æ‰¹é‡å†™å…¥æ“ä½œ
            all_operations = []
            now_iso = datetime.utcnow().isoformat()
            
            # å¾ªç¯æ¯ä¸ªæ¿å—ä»£ç ï¼Œè°ƒç”¨ ths_member æ¥å£
            for ts_code in ts_codes:
                if not ts_code:
                    continue
                
                try:
                    # ç­‰å¾…é€Ÿç‡é™åˆ¶
                    await self.rate_limiter.acquire()
                    
                    # è°ƒç”¨Tushare APIè·å–è¯¥æ¿å—çš„æˆåˆ†è‚¡
                    df = await asyncio.to_thread(
                        self.provider.api.ths_member,
                        ts_code=ts_code
                    )
                    
                    if df is None or df.empty:
                        logger.debug(f"âš ï¸ æ¿å— {ts_code} æ— æˆåˆ†è‚¡æ•°æ®")
                        stats["concepts_failed"] += 1
                        continue
                    
                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    records = df.to_dict('records')
                    stats["total_processed"] += len(records)
                    
                    # å¤„ç†æ¯æ¡è®°å½•
                    for record in records:
                        # éªŒè¯å¿…éœ€å­—æ®µ
                        con_code = str(record.get("con_code", "")).strip()
                        if not con_code:
                            logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè®°å½•ï¼ˆcon_codeä¸ºç©ºï¼‰: {record}")
                            continue
                        
                        # å­˜å‚¨æ‰€æœ‰å­—æ®µ
                        doc = {
                            "ts_code": str(record.get("ts_code", ts_code)),  # æ¿å—ä»£ç 
                            "con_code": con_code,  # è‚¡ç¥¨ä»£ç 
                            "con_name": str(record.get("con_name", "")),  # è‚¡ç¥¨åç§°
                            "weight": record.get("weight"),  # æƒé‡
                            "in_date": str(record.get("in_date", "")) if record.get("in_date") else None,  # çº³å…¥æ—¥æœŸ
                            "out_date": str(record.get("out_date", "")) if record.get("out_date") else None,  # å‰”é™¤æ—¥æœŸ
                            "is_new": str(record.get("is_new", "")),  # æ˜¯å¦æœ€æ–°Yæ˜¯Nå¦
                            "data_source": "tushare",
                            "updated_at": now_iso
                        }
                        
                        # ä¿ç•™æ‰€æœ‰åŸå§‹å­—æ®µï¼ˆç”¨äºè°ƒè¯•å’Œæ‰©å±•ï¼‰
                        for key, value in record.items():
                            if key not in doc and value is not None:
                                doc[key] = value
                        
                        # ä½¿ç”¨ ts_code + con_code ä½œä¸ºå”¯ä¸€é”®
                        all_operations.append(
                            UpdateOne(
                                {
                                    "ts_code": doc["ts_code"],
                                    "con_code": doc["con_code"]
                                },
                                {"$set": doc},
                                upsert=True
                            )
                        )
                    
                    stats["concepts_processed"] += 1
                    logger.info(f"âœ… æ¿å— {ts_code} è·å–å®Œæˆ: {len(records)} æ¡æˆåˆ†è‚¡æ•°æ®")
                    
                except Exception as e:
                    logger.error(f"âŒ è·å–æ¿å— {ts_code} æˆåˆ†è‚¡å¤±è´¥: {e}")
                    stats["concepts_failed"] += 1
                    stats["errors_list"].append(f"æ¿å— {ts_code}: {str(e)}")
                    continue
            
            # æ‰¹é‡å†™å…¥MongoDBï¼ˆåˆ†æ‰¹å†™å…¥ï¼Œé¿å…å•æ¬¡æ“ä½œè¿‡å¤§ï¼‰
            if all_operations:
                total_ops = len(all_operations)
                for i in range(0, total_ops, self.batch_size):
                    batch_ops = all_operations[i:i + self.batch_size]
                    try:
                        result = await self.db["ths_member"].bulk_write(batch_ops, ordered=False)
                        stats["inserted"] += result.upserted_count
                        stats["updated"] += result.modified_count
                        logger.debug(f"ğŸ“ æ‰¹é‡å†™å…¥è¿›åº¦: {min(i + self.batch_size, total_ops)}/{total_ops}")
                    except BulkWriteError as e:
                        # è®°å½•éƒ¨åˆ†æˆåŠŸçš„å†™å…¥
                        stats["inserted"] += e.details.get("nInserted", 0)
                        stats["updated"] += e.details.get("nModified", 0)
                        stats["errors"] += len(e.details.get("writeErrors", []))
                        logger.error(f"âŒ æ‰¹é‡å†™å…¥æˆåˆ†è‚¡æ•°æ®æ—¶å‡ºç°é”™è¯¯: {e.details}")
                        stats["errors_list"].append(f"æ‰¹é‡å†™å…¥é”™è¯¯: {str(e)}")
                
                logger.info(
                    f"âœ… åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æˆåˆ†åŒæ­¥å®Œæˆ: "
                    f"å¤„ç† {stats['concepts_processed']} ä¸ªæ¿å—, "
                    f"å¤±è´¥ {stats['concepts_failed']} ä¸ªæ¿å—, "
                    f"æ–°å¢ {stats['inserted']} æ¡, "
                    f"æ›´æ–° {stats['updated']} æ¡, "
                    f"æ€»è®¡ {stats['total_processed']} æ¡æˆåˆ†è‚¡æ•°æ®"
                )
            else:
                logger.warning("âš ï¸ æœªç”Ÿæˆä»»ä½•æˆåˆ†è‚¡æ•°æ®")
            
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
            
        except Exception as e:
            logger.exception(f"âŒ åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æˆåˆ†å¤±è´¥: {e}")
            stats["errors"] = 1
            stats["errors_list"].append(str(e))
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds() if stats.get("end_time") else 0
        
        return stats
    
    # ==================== åŒèŠ±é¡ºçƒ­æ¦œåŒæ­¥ ====================
    
    def _is_valid_hot_record(self, record: Dict[str, Any]) -> bool:
        """
        éªŒè¯çƒ­æ¦œè®°å½•æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            record: çƒ­æ¦œè®°å½•å­—å…¸
        
        Returns:
            å¦‚æœè®°å½•æœ‰æ•ˆè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        # æ£€æŸ¥å…³é”®å­—æ®µ
        ts_code = str(record.get("ts_code", "")).strip()
        rank_time = str(record.get("rank_time", "")).strip()
        
        # ts_code å¿…é¡»å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œä¸”ä¸èƒ½æ˜¯ "{}"
        if not ts_code or ts_code == "{}" or ts_code.lower() == "none":
            return False
        
        # rank_time åº”è¯¥å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œä¸”ä¸èƒ½æ˜¯ "{}"
        if not rank_time or rank_time == "{}" or rank_time.lower() == "none":
            return False
        
        # æ£€æŸ¥å…¶ä»–å…³é”®å­—æ®µæ˜¯å¦éƒ½æ˜¯ç©ºå€¼æˆ–å ä½ç¬¦
        # å¦‚æœæ‰€æœ‰ä¸šåŠ¡å­—æ®µéƒ½æ˜¯ç©ºçš„ï¼Œè§†ä¸ºæ— æ•ˆ
        key_fields = ["ts_name", "rank", "hot"]
        all_empty = True
        for field in key_fields:
            value = record.get(field)
            if value is not None:
                value_str = str(value).strip()
                if value_str and value_str != "{}" and value_str.lower() != "none":
                    all_empty = False
                    break
        
        # å¦‚æœæ‰€æœ‰å…³é”®ä¸šåŠ¡å­—æ®µéƒ½æ˜¯ç©ºçš„ï¼Œè§†ä¸ºæ— æ•ˆ
        if all_empty:
            return False
        
        return True
    
    def _filter_valid_records(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        è¿‡æ»¤å‡ºæœ‰æ•ˆçš„çƒ­æ¦œè®°å½•
        
        Args:
            records: è®°å½•åˆ—è¡¨
        
        Returns:
            è¿‡æ»¤åçš„æœ‰æ•ˆè®°å½•åˆ—è¡¨
        """
        valid_records = []
        for record in records:
            if self._is_valid_hot_record(record):
                valid_records.append(record)
            else:
                logger.debug(f"âš ï¸ è·³è¿‡æ— æ•ˆè®°å½•: ts_code={record.get('ts_code')}, rank_time={record.get('rank_time')}")
        return valid_records
    
    async def sync_ths_hot(self, trade_date: Optional[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥åŒèŠ±é¡ºçƒ­æ¦œæ•°æ®
        
        Args:
            trade_date: äº¤æ˜“æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
        
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸ”„ å¼€å§‹åŒæ­¥åŒèŠ±é¡ºçƒ­æ¦œæ•°æ®...")
        
        stats = {
            "total_processed": 0,
            "inserted": 0,
            "updated": 0,
            "errors": 0,
            "markets_processed": 0,
            "markets_failed": 0,
            "concept_codes_found": [],  # è®°å½•æ‰¾åˆ°çš„æ¦‚å¿µæ¿å—ä»£ç 
            "start_time": datetime.utcnow(),
            "errors_list": []
        }
        
        try:
            # ç¡®ä¿ç´¢å¼•å­˜åœ¨
            await self._ensure_indexes("ths_hot")
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            if not trade_date:
                trade_date = await self._get_latest_trade_date()
            
            logger.info(f"ğŸ“… åŒæ­¥æ—¥æœŸ: {trade_date}")
            
            # å®šä¹‰éœ€è¦åŒæ­¥çš„çƒ­æ¦œç±»å‹
            markets = ['çƒ­è‚¡', 'æ¦‚å¿µæ¿å—']
            logger.info(f"ğŸ“Š å¼€å§‹å¾ªç¯è·å–çƒ­æ¦œæ•°æ®ï¼Œç±»å‹: {markets}")
            
            # æ‰¹é‡å†™å…¥æ“ä½œ
            all_operations = []
            now_iso = datetime.utcnow().isoformat()
            
            # å¾ªç¯æ¯ä¸ªçƒ­æ¦œç±»å‹
            for market in markets:
                try:
                    # ç­‰å¾…é€Ÿç‡é™åˆ¶
                    await self.rate_limiter.acquire()
                    
                    # è°ƒç”¨Tushare APIè·å–è¯¥ç±»å‹çš„çƒ­æ¦œæ•°æ®
                    # é»˜è®¤è·å–æœ€æ–°æ•°æ®ï¼ˆis_new='Y'ï¼‰
                    df = await asyncio.to_thread(
                        self.provider.api.ths_hot,
                        trade_date=trade_date,
                        market=market,
                        is_new='Y'
                    )
                    
                    is_new_value = 'Y'
                    records = []
                    
                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨å¹¶è¿‡æ»¤æ— æ•ˆè®°å½•
                    if df is not None and not df.empty:
                        records = df.to_dict('records')
                        records = self._filter_valid_records(records)
                    
                    # å¦‚æœæ²¡æœ‰è·å–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œå°è¯•ä½¿ç”¨ is_new='N' è·å–æ•°æ®
                    if not records:
                        logger.debug(f"âš ï¸ çƒ­æ¦œç±»å‹ {market} is_new='Y' æ— æœ‰æ•ˆæ•°æ®ï¼Œå°è¯•ä½¿ç”¨ is_new='N' è·å–ï¼ˆæ—¥æœŸ: {trade_date}ï¼‰")
                        await self.rate_limiter.acquire()
                        df = await asyncio.to_thread(
                            self.provider.api.ths_hot,
                            trade_date=trade_date,
                            market=market,
                            is_new='N'
                        )
                        is_new_value = 'N'
                        
                        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨å¹¶è¿‡æ»¤æ— æ•ˆè®°å½•
                        if df is not None and not df.empty:
                            records = df.to_dict('records')
                            records = self._filter_valid_records(records)
                    
                    if not records:
                        logger.debug(f"âš ï¸ çƒ­æ¦œç±»å‹ {market} æ— æœ‰æ•ˆæ•°æ®ï¼ˆæ—¥æœŸ: {trade_date}ï¼‰")
                        stats["markets_failed"] += 1
                        continue
                    
                    # å¦‚æœä½¿ç”¨ is_new='N' è·å–æ•°æ®ï¼Œéœ€è¦æ ¹æ® rank_time å–æœ€æ–°çš„æ•°æ®
                    if is_new_value == 'N':
                        # æŒ‰ ts_code åˆ†ç»„ï¼Œæ¯ç»„åªä¿ç•™ rank_time æœ€æ–°çš„è®°å½•
                        records_by_code = {}
                        for record in records:
                            # å†æ¬¡éªŒè¯è®°å½•æœ‰æ•ˆæ€§ï¼ˆåŒé‡ä¿é™©ï¼‰
                            if not self._is_valid_hot_record(record):
                                continue
                            
                            ts_code = str(record.get("ts_code", "")).strip()
                            rank_time = str(record.get("rank_time", ""))
                            
                            if ts_code not in records_by_code:
                                records_by_code[ts_code] = record
                            else:
                                # æ¯”è¾ƒ rank_timeï¼Œä¿ç•™æœ€æ–°çš„
                                existing_rank_time = str(records_by_code[ts_code].get("rank_time", ""))
                                if rank_time > existing_rank_time:
                                    records_by_code[ts_code] = record
                        
                        # åªä¿ç•™æ¯ä¸ª ts_code æœ€æ–°çš„è®°å½•
                        records = list(records_by_code.values())
                        logger.debug(f"ğŸ“Š çƒ­æ¦œç±»å‹ {market} ä½¿ç”¨ is_new='N'ï¼Œæ ¹æ® rank_time ç­›é€‰åä¿ç•™ {len(records)} æ¡æœ€æ–°æ•°æ®")
                    
                    stats["total_processed"] += len(records)
                    
                    # å¤„ç†æ¯æ¡è®°å½•
                    for record in records:
                        # å†æ¬¡éªŒè¯è®°å½•æœ‰æ•ˆæ€§ï¼ˆåŒé‡ä¿é™©ï¼‰
                        if not self._is_valid_hot_record(record):
                            logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆè®°å½•: {record}")
                            continue
                        
                        ts_code = str(record.get("ts_code", "")).strip()
                        
                        # è·å– is_new å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
                        record_is_new = str(record.get("is_new", is_new_value)).strip()
                        if not record_is_new:
                            record_is_new = is_new_value
                        
                        # å­˜å‚¨æ‰€æœ‰å­—æ®µ
                        doc = {
                            "trade_date": str(record.get("trade_date", trade_date)),
                            "market": market,  # çƒ­æ¦œç±»å‹
                            "data_type": str(record.get("data_type", "")),  # æ•°æ®ç±»å‹
                            "ts_code": ts_code,  # è‚¡ç¥¨/æ¿å—ä»£ç 
                            "ts_name": str(record.get("ts_name", "")),  # è‚¡ç¥¨/æ¿å—åç§°
                            "rank": record.get("rank"),  # æ’è¡Œ
                            "pct_change": record.get("pct_change"),  # æ¶¨è·Œå¹…%
                            "current_price": record.get("current_price"),  # å½“å‰ä»·æ ¼
                            "concept": str(record.get("concept", "")),  # æ ‡ç­¾
                            "rank_reason": str(record.get("rank_reason", "")),  # ä¸Šæ¦œè§£è¯»
                            "hot": record.get("hot"),  # çƒ­åº¦å€¼
                            "rank_time": str(record.get("rank_time", "")),  # æ’è¡Œæ¦œè·å–æ—¶é—´
                            "is_new": record_is_new,  # æ˜¯å¦æœ€æ–°ï¼ˆYæ˜¯Nå¦ï¼‰
                            "data_source": "tushare",
                            "updated_at": now_iso
                        }
                        
                        # å¦‚æœæ˜¯æ¦‚å¿µæ¿å—ï¼Œè®°å½•æ¿å—ä»£ç ç”¨äºåç»­åŒæ­¥ ths_member
                        if market == "æ¦‚å¿µæ¿å—" and ts_code:
                            if ts_code not in stats["concept_codes_found"]:
                                stats["concept_codes_found"].append(ts_code)
                        
                        # ä¿ç•™æ‰€æœ‰åŸå§‹å­—æ®µï¼ˆç”¨äºè°ƒè¯•å’Œæ‰©å±•ï¼‰
                        for key, value in record.items():
                            if key not in doc and value is not None:
                                doc[key] = value
                        
                        # ä½¿ç”¨ trade_date + market + ts_code + rank_time ä½œä¸ºå”¯ä¸€é”®
                        all_operations.append(
                            UpdateOne(
                                {
                                    "trade_date": doc["trade_date"],
                                    "market": doc["market"],
                                    "ts_code": doc["ts_code"],
                                    "rank_time": doc["rank_time"]
                                },
                                {"$set": doc},
                                upsert=True
                            )
                        )
                    
                    stats["markets_processed"] += 1
                    logger.info(f"âœ… çƒ­æ¦œç±»å‹ {market} è·å–å®Œæˆ: {len(records)} æ¡æ•°æ® (is_new={is_new_value})")
                    
                except Exception as e:
                    logger.error(f"âŒ è·å–çƒ­æ¦œç±»å‹ {market} å¤±è´¥: {e}")
                    stats["markets_failed"] += 1
                    stats["errors_list"].append(f"çƒ­æ¦œç±»å‹ {market}: {str(e)}")
                    continue
            
            # æ‰¹é‡å†™å…¥MongoDBï¼ˆåˆ†æ‰¹å†™å…¥ï¼Œé¿å…å•æ¬¡æ“ä½œè¿‡å¤§ï¼‰
            if all_operations:
                total_ops = len(all_operations)
                for i in range(0, total_ops, self.batch_size):
                    batch_ops = all_operations[i:i + self.batch_size]
                    try:
                        result = await self.db["ths_hot"].bulk_write(batch_ops, ordered=False)
                        stats["inserted"] += result.upserted_count
                        stats["updated"] += result.modified_count
                        logger.debug(f"ğŸ“ æ‰¹é‡å†™å…¥è¿›åº¦: {min(i + self.batch_size, total_ops)}/{total_ops}")
                    except BulkWriteError as e:
                        # è®°å½•éƒ¨åˆ†æˆåŠŸçš„å†™å…¥
                        stats["inserted"] += e.details.get("nInserted", 0)
                        stats["updated"] += e.details.get("nModified", 0)
                        stats["errors"] += len(e.details.get("writeErrors", []))
                        logger.error(f"âŒ æ‰¹é‡å†™å…¥çƒ­æ¦œæ•°æ®æ—¶å‡ºç°é”™è¯¯: {e.details}")
                        stats["errors_list"].append(f"æ‰¹é‡å†™å…¥é”™è¯¯: {str(e)}")
                
                logger.info(
                    f"âœ… åŒèŠ±é¡ºçƒ­æ¦œæ•°æ®åŒæ­¥å®Œæˆ: "
                    f"å¤„ç† {stats['markets_processed']} ä¸ªç±»å‹, "
                    f"å¤±è´¥ {stats['markets_failed']} ä¸ªç±»å‹, "
                    f"æ–°å¢ {stats['inserted']} æ¡, "
                    f"æ›´æ–° {stats['updated']} æ¡, "
                    f"æ€»è®¡ {stats['total_processed']} æ¡çƒ­æ¦œæ•°æ®"
                )
                
                # å¦‚æœæ‰¾åˆ°æ¦‚å¿µæ¿å—ä»£ç ï¼Œè®°å½•æ—¥å¿—
                if stats["concept_codes_found"]:
                    logger.info(f"ğŸ“Š æ‰¾åˆ° {len(stats['concept_codes_found'])} ä¸ªæ¦‚å¿µæ¿å—ä»£ç : {stats['concept_codes_found'][:10]}...")
            else:
                logger.warning("âš ï¸ æœªç”Ÿæˆä»»ä½•çƒ­æ¦œæ•°æ®")
            
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds()
            
        except Exception as e:
            logger.exception(f"âŒ åŒæ­¥åŒèŠ±é¡ºçƒ­æ¦œæ•°æ®å¤±è´¥: {e}")
            stats["errors"] = 1
            stats["errors_list"].append(str(e))
            stats["end_time"] = datetime.utcnow()
            stats["duration"] = (stats["end_time"] - stats["start_time"]).total_seconds() if stats.get("end_time") else 0
        
        return stats
    
    # ==================== ç»Ÿä¸€åŒæ­¥å…¥å£ ====================
    
    async def sync_all(self, trade_date: Optional[str] = None) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰åŒèŠ±é¡ºé¢˜ææ•°æ®
        
        Args:
            trade_date: äº¤æ˜“æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
        
        Returns:
            åŒæ­¥ç»“æœç»Ÿè®¡
        """
        logger.info("ğŸš€ å¼€å§‹åŒæ­¥æ‰€æœ‰åŒèŠ±é¡ºé¢˜ææ•°æ®...")
        start_time = datetime.utcnow()
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œè·å–æœ€æ–°äº¤æ˜“æ—¥
        if not trade_date:
            trade_date = await self._get_latest_trade_date()
        
        # 1. åŒæ­¥æœ€å¼ºæ¿å—ç»Ÿè®¡
        limit_cpt_stats = await self.sync_limit_cpt_list(trade_date=trade_date)
        
        # 2. åŒæ­¥åŒèŠ±é¡ºçƒ­æ¦œæ•°æ®
        ths_hot_stats = await self.sync_ths_hot(trade_date=trade_date)
        
        # 3. æ”¶é›†éœ€è¦åŒæ­¥æˆåˆ†è‚¡çš„æ¿å—ä»£ç 
        concept_codes_to_sync = []
        
        # ä» limit_cpt_list è·å–æ¿å—ä»£ç 
        if limit_cpt_stats.get("inserted", 0) > 0 or limit_cpt_stats.get("updated", 0) > 0 or limit_cpt_stats.get("total_processed", 0) > 0:
            logger.info(f"ğŸ“Š ä» limit_cpt_list è·å–æ¿å—ä»£ç ...")
            cursor = self.db["ths_limit_cpt_list"].find(
                {"trade_date": trade_date},
                {"ts_code": 1}
            )
            limit_cpt_records = await cursor.to_list(length=None)
            limit_cpt_codes = [r.get("ts_code", "") for r in limit_cpt_records if r.get("ts_code")]
            concept_codes_to_sync.extend(limit_cpt_codes)
            logger.info(f"ğŸ“Š ä» limit_cpt_list è·å–åˆ° {len(limit_cpt_codes)} ä¸ªæ¿å—ä»£ç ")
        
        # ä» ths_hot çš„æ¦‚å¿µæ¿å—æ•°æ®ä¸­è·å–æ¿å—ä»£ç 
        if ths_hot_stats.get("concept_codes_found"):
            concept_codes_from_hot = ths_hot_stats.get("concept_codes_found", [])
            concept_codes_to_sync.extend(concept_codes_from_hot)
            logger.info(f"ğŸ“Š ä» ths_hot è·å–åˆ° {len(concept_codes_from_hot)} ä¸ªæ¦‚å¿µæ¿å—ä»£ç ")
        
        # å»é‡
        concept_codes_to_sync = list(set(concept_codes_to_sync))
        
        # 4. åŒæ­¥åŒèŠ±é¡ºæ¦‚å¿µæ¿å—æˆåˆ†
        if concept_codes_to_sync:
            logger.info(f"ğŸ“Š å¼€å§‹åŒæ­¥æ¿å—æˆåˆ†ï¼Œå…± {len(concept_codes_to_sync)} ä¸ªæ¿å—ä»£ç ")
            ths_member_stats = await self.sync_ths_member(ts_codes=concept_codes_to_sync, trade_date=trade_date)
        elif limit_cpt_stats.get("total_processed", 0) > 0:
            # å³ä½¿æ²¡æœ‰æ–°å¢æˆ–æ›´æ–°ï¼Œå¦‚æœæœ‰å¤„ç†çš„æ•°æ®ï¼Œä¹Ÿå¯ä»¥åŒæ­¥æˆåˆ†è‚¡
            logger.info(f"ğŸ“Š limit_cpt_list å·²æœ‰æ•°æ®ï¼Œå¼€å§‹åŒæ­¥æ¿å—æˆåˆ†...")
            ths_member_stats = await self.sync_ths_member(trade_date=trade_date)
        else:
            # limit_cpt_list åŒæ­¥å¤±è´¥æˆ–æ²¡æœ‰æ•°æ®
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ¿å—ä»£ç ï¼Œè·³è¿‡æ¿å—æˆåˆ†åŒæ­¥")
            ths_member_stats = {
                "total_processed": 0,
                "inserted": 0,
                "updated": 0,
                "errors": 1,
                "concepts_processed": 0,
                "concepts_failed": 0,
                "start_time": datetime.utcnow(),
                "errors_list": ["æœªæ‰¾åˆ°æ¿å—ä»£ç ï¼Œæ— æ³•åŒæ­¥æˆåˆ†è‚¡"],
                "end_time": datetime.utcnow(),
                "duration": 0
            }
        
        end_time = datetime.utcnow()
        total_duration = (end_time - start_time).total_seconds()
        
        result = {
            "limit_cpt_list": limit_cpt_stats,
            "ths_hot": ths_hot_stats,
            "ths_member": ths_member_stats,
            "total_duration": total_duration
        }
        
        logger.info(
            f"âœ… æ‰€æœ‰åŒèŠ±é¡ºé¢˜ææ•°æ®åŒæ­¥å®Œæˆ: "
            f"æœ€å¼ºæ¿å—-æ–°å¢{limit_cpt_stats.get('inserted', 0)}æ¡, "
            f"çƒ­æ¦œ-æ–°å¢{ths_hot_stats.get('inserted', 0)}æ¡, "
            f"æ¿å—æˆåˆ†-æ–°å¢{ths_member_stats.get('inserted', 0)}æ¡, "
            f"æ€»è€—æ—¶: {total_duration:.2f}ç§’"
        )
        
        return result
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    async def _get_latest_trade_date(self) -> str:
        """
        è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼ˆis_open=1ï¼‰
        å¦‚æœä»Šå¤©ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œè¿”å›æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥
        """
        try:
            # ä½¿ç”¨Tushare APIè·å–äº¤æ˜“æ—¥å†
            # æŸ¥è¯¢æœ€è¿‘30å¤©çš„äº¤æ˜“æ—¥å†ï¼Œç­›é€‰å‡ºäº¤æ˜“æ—¥ï¼ˆis_open=1ï¼‰
            await self.rate_limiter.acquire()
            today_str = datetime.now().strftime('%Y%m%d')
            start_date_str = (datetime.now() - timedelta(days=30)).strftime('%Y%m%d')
            
            logger.debug(f"ğŸ” æŸ¥è¯¢äº¤æ˜“æ—¥å†: {start_date_str} åˆ° {today_str}")
            
            cal_df = await asyncio.to_thread(
                self.provider.api.trade_cal,
                exchange='SSE',
                start_date=start_date_str,
                end_date=today_str
            )
            
            if cal_df is not None and not cal_df.empty:
                logger.debug(f"ğŸ“Š è·å–åˆ° {len(cal_df)} æ¡äº¤æ˜“æ—¥å†æ•°æ®")
                
                # is_open å¯èƒ½æ˜¯å­—ç¬¦ä¸² '1' æˆ–æ•°å­— 1ï¼Œéœ€è¦å…¼å®¹å¤„ç†
                # å…ˆè½¬æ¢ä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒï¼Œæˆ–è€…è½¬æ¢ä¸ºæ•°å­—
                if 'is_open' in cal_df.columns:
                    # å°è¯•è½¬æ¢ä¸ºæ•°å­—ç±»å‹
                    cal_df['is_open'] = cal_df['is_open'].astype(str).str.strip()
                    # ç­›é€‰å‡ºäº¤æ˜“æ—¥ï¼ˆis_open='1'ï¼‰
                    trade_days = cal_df[cal_df['is_open'] == '1']
                else:
                    logger.warning("âš ï¸ äº¤æ˜“æ—¥å†æ•°æ®ä¸­ç¼ºå°‘ is_open å­—æ®µ")
                    trade_days = cal_df
                
                if not trade_days.empty:
                    # æŒ‰æ—¥æœŸæ’åºï¼Œç¡®ä¿è·å–æœ€æ–°çš„äº¤æ˜“æ—¥
                    trade_days = trade_days.sort_values('cal_date', ascending=True)
                    # è·å–æœ€æ–°çš„äº¤æ˜“æ—¥ï¼ˆæœ€åä¸€è¡Œï¼‰
                    latest_date = trade_days.iloc[-1]['cal_date']
                    latest_date_str = str(latest_date)
                    logger.info(f"ğŸ“… è·å–åˆ°æœ€æ–°äº¤æ˜“æ—¥: {latest_date_str} (å…± {len(trade_days)} ä¸ªäº¤æ˜“æ—¥)")
                    return latest_date_str
                else:
                    logger.warning(f"âš ï¸ æœ€è¿‘30å¤©æ— äº¤æ˜“æ—¥ï¼Œæ‰©å¤§æŸ¥è¯¢èŒƒå›´...")
                    # å¦‚æœæœ€è¿‘30å¤©éƒ½æ²¡æœ‰äº¤æ˜“æ—¥ï¼Œæ‰©å¤§æŸ¥è¯¢èŒƒå›´
                    await self.rate_limiter.acquire()
                    start_date_extended = (datetime.now() - timedelta(days=90)).strftime('%Y%m%d')
                    cal_df_extended = await asyncio.to_thread(
                        self.provider.api.trade_cal,
                        exchange='SSE',
                        start_date=start_date_extended,
                        end_date=today_str
                    )
                    
                    if cal_df_extended is not None and not cal_df_extended.empty:
                        if 'is_open' in cal_df_extended.columns:
                            cal_df_extended['is_open'] = cal_df_extended['is_open'].astype(str).str.strip()
                            trade_days_extended = cal_df_extended[cal_df_extended['is_open'] == '1']
                        else:
                            trade_days_extended = cal_df_extended
                        
                        if not trade_days_extended.empty:
                            trade_days_extended = trade_days_extended.sort_values('cal_date', ascending=True)
                            latest_date = trade_days_extended.iloc[-1]['cal_date']
                            latest_date_str = str(latest_date)
                            logger.info(f"ğŸ“… è·å–åˆ°æœ€æ–°äº¤æ˜“æ—¥ï¼ˆæ‰©å±•æŸ¥è¯¢ï¼‰: {latest_date_str} (å…± {len(trade_days_extended)} ä¸ªäº¤æ˜“æ—¥)")
                            return latest_date_str
            
            # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æ˜¨å¤©ä½œä¸ºé»˜è®¤å€¼
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
            logger.warning(f"âš ï¸ æ— æ³•è·å–æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {yesterday}")
            return yesterday
            
        except Exception as e:
            logger.exception(f"âš ï¸ è·å–æœ€æ–°äº¤æ˜“æ—¥å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            # å°è¯•ä½¿ç”¨æ˜¨å¤©ä½œä¸ºé»˜è®¤å€¼
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
            return yesterday


# å•ä¾‹æ¨¡å¼
_ths_sync_service: Optional[THSSyncService] = None


async def get_ths_sync_service() -> THSSyncService:
    """è·å–åŒèŠ±é¡ºé¢˜æåŒæ­¥æœåŠ¡å•ä¾‹"""
    global _ths_sync_service
    if _ths_sync_service is None:
        _ths_sync_service = THSSyncService()
        await _ths_sync_service.initialize()
    return _ths_sync_service

