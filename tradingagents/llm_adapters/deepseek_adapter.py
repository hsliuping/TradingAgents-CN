"""
DeepSeek LLMé€‚é…å™¨ï¼Œæ”¯æŒTokenä½¿ç”¨ç»Ÿè®¡
"""

import os
import time
from typing import Any, Dict, List, Optional, Union
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from langchain_core.prompt_values import ChatPromptValue
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import CallbackManagerForLLMRun

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import setup_llm_logging

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger, get_logger_manager
logger = get_logger('agents')
logger = setup_llm_logging()

# å¯¼å…¥tokenè·Ÿè¸ªå™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
    logger.info("âœ… Tokenè·Ÿè¸ªåŠŸèƒ½å·²å¯ç”¨")
except ImportError:
    TOKEN_TRACKING_ENABLED = False
    logger.warning("âš ï¸ Tokenè·Ÿè¸ªåŠŸèƒ½æœªå¯ç”¨")


class ChatDeepSeek(ChatOpenAI):
    """
    DeepSeekèŠå¤©æ¨¡å‹é€‚é…å™¨ï¼Œæ”¯æŒTokenä½¿ç”¨ç»Ÿè®¡
    
    ç»§æ‰¿è‡ªChatOpenAIï¼Œæ·»åŠ äº†Tokenä½¿ç”¨é‡ç»Ÿè®¡åŠŸèƒ½
    """
    
    def __init__(
        self,
        model: str = "deepseek-chat",
        api_key: Optional[str] = None,
        base_url: str = "https://api.deepseek.com",
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–DeepSeeké€‚é…å™¨
        
        Args:
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºdeepseek-chat
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡DEEPSEEK_API_KEYè·å–
            base_url: APIåŸºç¡€URL
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        # è·å–APIå¯†é’¥
        if api_key is None:
            # å¯¼å…¥ API Key éªŒè¯å·¥å…·
            try:
                from app.utils.api_key_utils import is_valid_api_key
            except ImportError:
                def is_valid_api_key(key):
                    if not key or len(key) <= 10:
                        return False
                    if key.startswith('your_') or key.startswith('your-'):
                        return False
                    if key.endswith('_here') or key.endswith('-here'):
                        return False
                    if '...' in key:
                        return False
                    return True

            # ä»ç¯å¢ƒå˜é‡è¯»å– API Key
            env_api_key = os.getenv("DEEPSEEK_API_KEY")

            # éªŒè¯ç¯å¢ƒå˜é‡ä¸­çš„ API Key æ˜¯å¦æœ‰æ•ˆï¼ˆæ’é™¤å ä½ç¬¦ï¼‰
            if env_api_key and is_valid_api_key(env_api_key):
                api_key = env_api_key
                logger.info("âœ… [DeepSeekåˆå§‹åŒ–] ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„æœ‰æ•ˆ API Key")
            elif env_api_key:
                logger.warning("âš ï¸ [DeepSeekåˆå§‹åŒ–] ç¯å¢ƒå˜é‡ä¸­çš„ API Key æ— æ•ˆï¼ˆå¯èƒ½æ˜¯å ä½ç¬¦ï¼‰ï¼Œå°†è¢«å¿½ç•¥")
                api_key = None
            else:
                api_key = None

            if not api_key:
                raise ValueError(
                    "DeepSeek APIå¯†é’¥æœªæ‰¾åˆ°ã€‚è¯·åœ¨ Web ç•Œé¢é…ç½® API Key "
                    "(è®¾ç½® -> å¤§æ¨¡å‹å‚å®¶) æˆ–è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ã€‚"
                )
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(
            model=model,
            openai_api_key=api_key,
            openai_api_base=base_url,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        
        self.model_name = model

    def _process_messages_for_deepseek(self, messages) -> List[BaseMessage]:
        """
        å¤„ç†æ¶ˆæ¯æ ¼å¼ä»¥æ»¡è¶³DeepSeek APIè¦æ±‚
        ä¸»è¦å¤„ç†å·¥å…·è°ƒç”¨æ—¶çš„reasoning_contentå­—æ®µ
        """
        logger.debug(f"ğŸ”§ [DeepSeek] å¼€å§‹å¤„ç†æ¶ˆæ¯ï¼Œè¾“å…¥ç±»å‹: {type(messages)}")

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤„ç†ä¸åŒçš„è¾“å…¥ç±»å‹
        if isinstance(messages, ChatPromptValue):
            # å¦‚æœæ˜¯ChatPromptValueï¼Œæå–æ¶ˆæ¯åˆ—è¡¨
            message_list = messages.messages
            logger.debug(f"ğŸ”§ [DeepSeek] ä»ChatPromptValueæå–æ¶ˆæ¯ï¼Œæ•°é‡: {len(message_list)}")
        elif isinstance(messages, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
            message_list = messages
            logger.debug(f"ğŸ”§ [DeepSeek] ç›´æ¥ä½¿ç”¨æ¶ˆæ¯åˆ—è¡¨ï¼Œæ•°é‡: {len(message_list)}")
        else:
            logger.warning(f"ğŸ”§ [DeepSeek] æœªçŸ¥æ¶ˆæ¯ç±»å‹: {type(messages)}ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨")
            message_list = [messages] if not isinstance(messages, list) else messages

        # ğŸ”¥ è°ƒè¯•ï¼šæ‰“å°åŸå§‹æ¶ˆæ¯ä¿¡æ¯
        logger.debug(f"ğŸ”§ [DeepSeek] ===== åŸå§‹æ¶ˆæ¯è°ƒè¯•ä¿¡æ¯ =====")
        for i, msg in enumerate(message_list):
            msg_type = type(msg).__name__
            has_additional = hasattr(msg, 'additional_kwargs') and msg.additional_kwargs
            reasoning_content = getattr(msg, 'additional_kwargs', {}).get('reasoning_content', 'None') if has_additional else 'None'
            logger.debug(f"ğŸ”§ [DeepSeek] æ¶ˆæ¯{i}: {msg_type}, has_additional_kwargs: {has_additional}, reasoning_content: {reasoning_content}")

        processed_messages = []

        for i, message in enumerate(message_list):
            logger.debug(f"ğŸ”§ [DeepSeek] å¤„ç†æ¶ˆæ¯ {i}: ç±»å‹={type(message).__name__}")

            # ğŸ”¥ ä¿®å¤ï¼šæ›´å®½æ¾çš„æ¶ˆæ¯ç±»å‹æ£€æŸ¥å’Œå¤„ç†
            if isinstance(message, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•ç¡®å®šæ¶ˆæ¯ç±»å‹å¹¶å¤„ç†
                role = message.get('role', '')
                content = message.get('content', '')
                logger.debug(f"ğŸ”§ [DeepSeek] å­—å…¸æ¶ˆæ¯ role={role}, contenté•¿åº¦={len(str(content))}")

                # å¯¹äºå­—å…¸æ¶ˆæ¯ï¼Œç›´æ¥æ·»åŠ reasoning_content
                if role == 'assistant':
                    message['reasoning_content'] = f"åŠ©æ‰‹å“åº”æ¨ç†ï¼šåŸºäºå½“å‰ä¸Šä¸‹æ–‡ç”Ÿæˆå“åº”ã€‚ç´¢å¼•ä½ç½®: {i}"
                processed_messages.append(message)
                logger.debug(f"ğŸ”§ [DeepSeek] å¤„ç†å­—å…¸æ¶ˆæ¯å®Œæˆï¼Œç´¢å¼•: {i}")

            elif isinstance(message, (AIMessage, HumanMessage, SystemMessage, ToolMessage)):
                if isinstance(message, AIMessage):
                    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                    if hasattr(message, 'tool_calls') and message.tool_calls:
                        # ğŸ”¥ ä¿®å¤ï¼šç›´æ¥ä¿®æ”¹åŸæ¶ˆæ¯çš„additional_kwargsï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°æ¶ˆæ¯
                        if not hasattr(message, 'additional_kwargs'):
                            message.additional_kwargs = {}
                        message.additional_kwargs["reasoning_content"] = f"å·¥å…·è°ƒç”¨å†³ç­–ï¼šåŸºäºå½“å‰åˆ†æéœ€è¦ï¼Œå†³å®šè°ƒç”¨å·¥å…·è·å–æ•°æ®ã€‚ç´¢å¼•ä½ç½®: {i}"
                        processed_messages.append(message)
                        logger.debug(f"ğŸ”§ [DeepSeek] ä¸ºAIMessageæ·»åŠ reasoning_contentå­—æ®µï¼Œç´¢å¼•: {i}")
                    else:
                        # æ²¡æœ‰å·¥å…·è°ƒç”¨çš„æƒ…å†µï¼Œç›´æ¥æ·»åŠ reasoning_contentåˆ°åŸæ¶ˆæ¯
                        if not hasattr(message, 'additional_kwargs'):
                            message.additional_kwargs = {}
                        if "reasoning_content" not in message.additional_kwargs:
                            message.additional_kwargs["reasoning_content"] = f"åˆ†ææ¨ç†ï¼šåŸºäºå·²æœ‰ä¿¡æ¯è¿›è¡Œåˆ†æå’Œåˆ¤æ–­ã€‚ç´¢å¼•ä½ç½®: {i}"
                        processed_messages.append(message)
                        logger.debug(f"ğŸ”§ [DeepSeek] ä¸ºAIMessageæ·»åŠ analysis reasoning_contentå­—æ®µï¼Œç´¢å¼•: {i}")
                elif isinstance(message, ToolMessage):
                    # ç›´æ¥ä¿®æ”¹åŸToolMessageçš„additional_kwargs
                    if not hasattr(message, 'additional_kwargs'):
                        message.additional_kwargs = {}
                    message.additional_kwargs["reasoning_content"] = f"å·¥å…·è¿”å›ç»“æœå¤„ç†ï¼šæ­£åœ¨å¤„ç†å·¥å…·è¿”å›çš„æ•°æ®ã€‚ç´¢å¼•ä½ç½®: {i}"
                    processed_messages.append(message)
                    logger.debug(f"ğŸ”§ [DeepSeek] ä¸ºToolMessageæ·»åŠ reasoning_contentå­—æ®µï¼Œç´¢å¼•: {i}")
                else:
                    # HumanMessageå’ŒSystemMessageä¿æŒåŸæ ·
                    processed_messages.append(message)
                    logger.debug(f"ğŸ”§ [DeepSeek] ä¿æŒ{type(message).__name__}åŸæ ·ï¼Œç´¢å¼•: {i}")
            else:
                # å…¶ä»–æœªçŸ¥ç±»å‹ï¼Œä¿æŒåŸæ ·ä½†è®°å½•è­¦å‘Š
                logger.warning(f"âš ï¸ [DeepSeek] æœªçŸ¥æ¶ˆæ¯ç±»å‹ä½†ä¿æŒåŸæ ·: {type(message)}")
                processed_messages.append(message)

        # ğŸ”¥ è°ƒè¯•ï¼šæ‰“å°å¤„ç†åæ¶ˆæ¯ä¿¡æ¯
        logger.debug(f"ğŸ”§ [DeepSeek] ===== å¤„ç†åæ¶ˆæ¯è°ƒè¯•ä¿¡æ¯ =====")
        for i, msg in enumerate(processed_messages):
            msg_type = type(msg).__name__
            has_additional = hasattr(msg, 'additional_kwargs') and msg.additional_kwargs
            reasoning_content = getattr(msg, 'additional_kwargs', {}).get('reasoning_content', 'None') if has_additional else 'None'
            logger.debug(f"ğŸ”§ [DeepSeek] å¤„ç†åæ¶ˆæ¯{i}: {msg_type}, has_additional_kwargs: {has_additional}, reasoning_content: {reasoning_content}")

        logger.debug(f"ğŸ”§ [DeepSeek] æ¶ˆæ¯å¤„ç†å®Œæˆï¼Œå¤„ç†åæ¶ˆæ¯æ•°é‡: {len(processed_messages)}")
        return processed_messages

    def _get_request_payload(self, messages: List[BaseMessage], **kwargs: Any) -> dict:
        """
        é‡å†™è¯·æ±‚payloadç”Ÿæˆï¼Œæ³¨å…¥reasoning_contentå­—æ®µ
        """
        # å…ˆè°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–åŸºç¡€payload
        payload = super()._get_request_payload(messages, **kwargs)

        logger.debug(f"ğŸ”§ [DeepSeek] ===== OpenAI Payload ä¿®æ”¹ =====")
        logger.debug(f"ğŸ”§ [DeepSeek] åŸå§‹æ¶ˆæ¯æ•°é‡: {len(messages)}")

        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç›´æ¥ä¿®æ”¹payloadä¸­çš„messagesæ ¼å¼
        if 'messages' in payload:
            for i, msg_data in enumerate(payload['messages']):
                # æ‰¾åˆ°å¯¹åº”çš„LangChainæ¶ˆæ¯
                if i < len(messages):
                    original_msg = messages[i]

                    # æ£€æŸ¥æ˜¯å¦æ˜¯AIMessageä¸”éœ€è¦reasoning_content
                    if (isinstance(original_msg, AIMessage) and
                        hasattr(original_msg, 'additional_kwargs') and
                        original_msg.additional_kwargs and
                        'reasoning_content' in original_msg.additional_kwargs):

                        # ç›´æ¥æ³¨å…¥reasoning_contentå­—æ®µåˆ°OpenAI API payload
                        msg_data['reasoning_content'] = original_msg.additional_kwargs['reasoning_content']
                        logger.debug(f"ğŸ”§ [DeepSeek] æ³¨å…¥reasoning_contentåˆ°æ¶ˆæ¯{i}: {msg_data['reasoning_content']}")

                    # æ£€æŸ¥æ˜¯å¦æ˜¯å­—å…¸æ¶ˆæ¯ä¸”å·²ç»æœ‰reasoning_content
                    elif isinstance(original_msg, dict) and 'reasoning_content' in original_msg:
                        msg_data['reasoning_content'] = original_msg['reasoning_content']
                        logger.debug(f"ğŸ”§ [DeepSeek] ä»å­—å…¸æ¶ˆæ¯æ³¨å…¥reasoning_contentåˆ°æ¶ˆæ¯{i}: {msg_data['reasoning_content']}")

                logger.debug(f"ğŸ”§ [DeepSeek] Payloadæ¶ˆæ¯{i}å­—æ®µ: {list(msg_data.keys())}")

        logger.debug(f"ğŸ”§ [DeepSeek] ===== Payload ä¿®æ”¹å®Œæˆ =====")
        return payload

    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å“åº”ï¼Œå¹¶è®°å½•tokenä½¿ç”¨é‡
        """

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # æå–å¹¶ç§»é™¤è‡ªå®šä¹‰å‚æ•°ï¼Œé¿å…ä¼ é€’ç»™çˆ¶ç±»
        session_id = kwargs.pop('session_id', None)
        analysis_type = kwargs.pop('analysis_type', None)

        # ğŸ”¥ DeepSeekä¿®å¤ï¼šå¤„ç†æ¶ˆæ¯æ ¼å¼ï¼Œæ·»åŠ reasoning_contentå­—æ®µ
        processed_messages = self._process_messages_for_deepseek(messages)

        try:
            logger.debug(f"ğŸ”§ [DeepSeek] ä½¿ç”¨å¤„ç†åçš„æ¶ˆæ¯ï¼Œæ•°é‡: {len(processed_messages)}")
            result = super()._generate(processed_messages, stop, run_manager, **kwargs)
            
            # æå–tokenä½¿ç”¨é‡
            input_tokens = 0
            output_tokens = 0
            
            # å°è¯•ä»å“åº”ä¸­æå–tokenä½¿ç”¨é‡
            if hasattr(result, 'llm_output') and result.llm_output:
                token_usage = result.llm_output.get('token_usage', {})
                if token_usage:
                    input_tokens = token_usage.get('prompt_tokens', 0)
                    output_tokens = token_usage.get('completion_tokens', 0)
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°tokenä½¿ç”¨é‡ï¼Œè¿›è¡Œä¼°ç®—
            if input_tokens == 0 and output_tokens == 0:
                input_tokens = self._estimate_input_tokens(messages)
                output_tokens = self._estimate_output_tokens(result)
                logger.debug(f"ğŸ” [DeepSeek] ä½¿ç”¨ä¼°ç®—token: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
            else:
                logger.info(f"ğŸ“Š [DeepSeek] å®é™…tokenä½¿ç”¨: è¾“å…¥={input_tokens}, è¾“å‡º={output_tokens}")
            
            # è®°å½•tokenä½¿ç”¨é‡
            if TOKEN_TRACKING_ENABLED and (input_tokens > 0 or output_tokens > 0):
                try:
                    # ä½¿ç”¨æå–çš„å‚æ•°æˆ–ç”Ÿæˆé»˜è®¤å€¼
                    if session_id is None:
                        session_id = f"deepseek_{hash(str(messages))%10000}"
                    if analysis_type is None:
                        analysis_type = 'stock_analysis'

                    # è®°å½•ä½¿ç”¨é‡
                    usage_record = token_tracker.track_usage(
                        provider="deepseek",
                        model_name=self.model_name,
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        session_id=session_id,
                        analysis_type=analysis_type
                    )

                    if usage_record:
                        if usage_record.cost == 0.0:
                            logger.warning(f"âš ï¸ [DeepSeek] æˆæœ¬è®¡ç®—ä¸º0ï¼Œå¯èƒ½é…ç½®æœ‰é—®é¢˜")
                        else:
                            logger.info(f"ğŸ’° [DeepSeek] æœ¬æ¬¡è°ƒç”¨æˆæœ¬: Â¥{usage_record.cost:.6f}")

                        # ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨çš„Tokenè®°å½•æ–¹æ³•
                        logger_manager = get_logger_manager()
                        logger_manager.log_token_usage(
                            logger, "deepseek", self.model_name,
                            input_tokens, output_tokens, usage_record.cost,
                            session_id
                        )
                    else:
                        logger.warning(f"âš ï¸ [DeepSeek] æœªåˆ›å»ºä½¿ç”¨è®°å½•")

                except Exception as track_error:
                    logger.error(f"âš ï¸ [DeepSeek] Tokenç»Ÿè®¡å¤±è´¥: {track_error}", exc_info=True)
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ [DeepSeek] è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            raise
    
    def _estimate_input_tokens(self, messages: List[BaseMessage]) -> int:
        """
        ä¼°ç®—è¾“å…¥tokenæ•°é‡
        
        Args:
            messages: è¾“å…¥æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            ä¼°ç®—çš„è¾“å…¥tokenæ•°é‡
        """
        total_chars = 0
        for message in messages:
            if hasattr(message, 'content'):
                total_chars += len(str(message.content))
        
        # ç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡çº¦1.5å­—ç¬¦/tokenï¼Œè‹±æ–‡çº¦4å­—ç¬¦/token
        # è¿™é‡Œä½¿ç”¨ä¿å®ˆä¼°ç®—ï¼š2å­—ç¬¦/token
        estimated_tokens = max(1, total_chars // 2)
        return estimated_tokens
    
    def _estimate_output_tokens(self, result: ChatResult) -> int:
        """
        ä¼°ç®—è¾“å‡ºtokenæ•°é‡
        
        Args:
            result: èŠå¤©ç»“æœ
            
        Returns:
            ä¼°ç®—çš„è¾“å‡ºtokenæ•°é‡
        """
        total_chars = 0
        for generation in result.generations:
            if hasattr(generation, 'message') and hasattr(generation.message, 'content'):
                total_chars += len(str(generation.message.content))
        
        # ç²—ç•¥ä¼°ç®—ï¼š2å­—ç¬¦/token
        estimated_tokens = max(1, total_chars // 2)
        return estimated_tokens
    
    def invoke(
        self,
        input: Union[str, List[BaseMessage]],
        config: Optional[Dict] = None,
        **kwargs: Any,
    ) -> AIMessage:
        """
        è°ƒç”¨æ¨¡å‹ç”Ÿæˆå“åº”
        
        Args:
            input: è¾“å…¥æ¶ˆæ¯
            config: é…ç½®å‚æ•°
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆåŒ…æ‹¬session_idå’Œanalysis_typeï¼‰
            
        Returns:
            AIæ¶ˆæ¯å“åº”
        """
        
        # å¤„ç†è¾“å…¥
        if isinstance(input, str):
            messages = [HumanMessage(content=input)]
        else:
            messages = input
        
        # è°ƒç”¨ç”Ÿæˆæ–¹æ³•
        result = self._generate(messages, **kwargs)
        
        # è¿”å›ç¬¬ä¸€ä¸ªç”Ÿæˆç»“æœçš„æ¶ˆæ¯
        if result.generations:
            return result.generations[0].message
        else:
            return AIMessage(content="")


def create_deepseek_llm(
    model: str = "deepseek-chat",
    temperature: float = 0.1,
    max_tokens: Optional[int] = None,
    **kwargs
) -> ChatDeepSeek:
    """
    åˆ›å»ºDeepSeek LLMå®ä¾‹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ChatDeepSeekå®ä¾‹
    """
    return ChatDeepSeek(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


# ä¸ºäº†å‘åå…¼å®¹ï¼Œæä¾›åˆ«å
DeepSeekLLM = ChatDeepSeek
