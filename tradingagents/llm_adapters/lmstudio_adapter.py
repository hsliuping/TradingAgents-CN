"""
LM Studioæœ¬åœ°AIæ¨¡å‹é€‚é…å™¨
ä¸ºLM Studioæœ¬åœ°è¿è¡Œçš„AIæ¨¡å‹æä¾›å®Œæ•´çš„é€‚é…å™¨æ”¯æŒ
"""

import os
import requests
import time
from typing import Any, Dict, List, Optional, Union
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatResult
from langchain_core.callbacks import CallbackManagerForLLMRun

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import setup_llm_logging
from tradingagents.utils.logging_manager import get_logger

# å¯¼å…¥OpenAIå…¼å®¹åŸºç±»
from .openai_compatible_base import OpenAICompatibleBase, TOKEN_TRACKING_ENABLED

# å¯¼å…¥tokenè·Ÿè¸ªå™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
except ImportError:
    TOKEN_TRACKING_ENABLED = False

logger = get_logger('agents')
logger = setup_llm_logging()


class ChatLMStudio(OpenAICompatibleBase):
    """
    LM Studioæœ¬åœ°æ¨¡å‹é€‚é…å™¨

    æ”¯æŒé€šè¿‡OpenAIå…¼å®¹æ¥å£ä¸æœ¬åœ°è¿è¡Œçš„LM Studioæ¨¡å‹è¿›è¡Œäº¤äº’
    """

    def __init__(
        self,
        model: str = "llama-3.1-8b-instruct",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        timeout: int = 30,
        **kwargs
    ):
        """
        åˆå§‹åŒ–LM Studioé€‚é…å™¨

        Args:
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º llama-3.1-8b-instruct
            api_key: APIå¯†é’¥ï¼ŒLM Studioé€šå¸¸ä½¿ç”¨ "lm-studio-local"
            base_url: LM StudioæœåŠ¡åœ°å€ï¼Œé»˜è®¤ä¸º http://localhost:1234/v1
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        """

        # è·å–é…ç½®å‚æ•°
        self.base_url = base_url or os.getenv("LM_STUDIO_BASE_URL", "http://localhost:1234/v1")
        self.api_key = api_key or os.getenv("LM_STUDIO_API_KEY", "lm-studio-local")
        self.timeout = timeout

        # éªŒè¯LM StudioæœåŠ¡å¯ç”¨æ€§
        self._validate_lmstudio_service()

        super().__init__(
            provider_name="lmstudio",
            model=model,
            api_key_env_var="LM_STUDIO_API_KEY",
            base_url=self.base_url,
            api_key=self.api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            **kwargs
        )

        logger.info(f"ğŸ  LM Studioé€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   æœåŠ¡åœ°å€: {self.base_url}")
        logger.info(f"   è¶…æ—¶è®¾ç½®: {timeout}ç§’")

    def _validate_lmstudio_service(self):
        """éªŒè¯LM StudioæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            # æµ‹è¯•è¿æ¥
            response = requests.get(
                f"{self.base_url}/models",
                headers=headers,
                timeout=5  # çŸ­è¶…æ—¶ï¼Œç”¨äºå¿«é€ŸéªŒè¯
            )

            if response.status_code == 200:
                models_data = response.json()
                available_models = [model.get("id", "unknown") for model in models_data.get("data", [])]
                logger.info(f"âœ… LM StudioæœåŠ¡è¿æ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹: {available_models}")
            else:
                logger.warning(f"âš ï¸ LM StudioæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")

        except requests.exceptions.ConnectionError:
            logger.warning(f"âš ï¸ æ— æ³•è¿æ¥åˆ°LM StudioæœåŠ¡: {self.base_url}")
            logger.warning("   è¯·ç¡®ä¿LM Studioæ­£åœ¨è¿è¡Œå¹¶å·²å¯åŠ¨æœåŠ¡å™¨")
        except requests.exceptions.Timeout:
            logger.warning(f"âš ï¸ LM StudioæœåŠ¡è¿æ¥è¶…æ—¶: {self.base_url}")
        except Exception as e:
            logger.warning(f"âš ï¸ LM StudioæœåŠ¡éªŒè¯å¤±è´¥: {e}")

    def get_available_models(self) -> List[str]:
        """
        è·å–LM Studioä¸­å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨

        Returns:
            å¯ç”¨æ¨¡å‹IDåˆ—è¡¨
        """
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            response = requests.get(
                f"{self.base_url}/models",
                headers=headers,
                timeout=self.timeout
            )

            if response.status_code == 200:
                models_data = response.json()
                models = [model.get("id", "unknown") for model in models_data.get("data", [])]
                logger.info(f"âœ… è·å–åˆ° {len(models)} ä¸ªå¯ç”¨æ¨¡å‹: {models}")
                return models
            else:
                logger.error(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}")
                return []

        except Exception as e:
            logger.error(f"âŒ è·å–LM Studioæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def test_connection(self) -> bool:
        """
        æµ‹è¯•LM Studioè¿æ¥çŠ¶æ€

        Returns:
            è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            models = self.get_available_models()
            return len(models) > 0
        except Exception as e:
            logger.error(f"âŒ LM Studioè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å“åº”ï¼ŒåŒ…å«LM Studioç‰¹æœ‰çš„é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
        """

        start_time = time.time()
        max_retries = 2
        retry_count = 0

        while retry_count <= max_retries:
            try:
                # è°ƒç”¨çˆ¶ç±»ç”Ÿæˆæ–¹æ³•
                result = super()._generate(messages, stop, run_manager, **kwargs)

                # è®°å½•tokenä½¿ç”¨
                self._track_token_usage(result, kwargs, start_time)

                return result

            except Exception as e:
                retry_count += 1

                # åˆ†æé”™è¯¯ç±»å‹
                error_msg = str(e).lower()

                if "connection" in error_msg or "timeout" in error_msg:
                    if retry_count <= max_retries:
                        logger.warning(f"âš ï¸ LM Studioè¿æ¥é—®é¢˜ï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•: {e}")
                        time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                        continue
                    else:
                        logger.error(f"âŒ LM Studioè¿æ¥å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                        raise Exception(f"LM Studioè¿æ¥å¤±è´¥: {e}")

                elif "model" in error_msg and "not found" in error_msg:
                    logger.error(f"âŒ LM Studioæ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½: {e}")
                    raise Exception(f"æ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿åœ¨LM Studioä¸­å·²åŠ è½½æ¨¡å‹: {e}")

                else:
                    logger.error(f"âŒ LM Studioç”Ÿæˆå“åº”å¤±è´¥: {e}")
                    raise

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        raise Exception(f"LM Studioè¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")

    def _track_token_usage(self, result: ChatResult, kwargs: Dict, start_time: float):
        """è®°å½•tokenä½¿ç”¨é‡å¹¶è¾“å‡ºæ—¥å¿—ï¼ˆLM Studioä¸“ç”¨ï¼‰"""
        if not TOKEN_TRACKING_ENABLED:
            return

        try:
            # ç»Ÿè®¡tokenä¿¡æ¯
            usage = getattr(result, "usage_metadata", None)
            total_tokens = usage.get("total_tokens") if usage else None
            prompt_tokens = usage.get("input_tokens") if usage else None
            completion_tokens = usage.get("output_tokens") if usage else None

            elapsed = time.time() - start_time

            # LM Studioç‰¹æ®Šæ—¥å¿—æ ¼å¼
            logger.info(
                f"ğŸ  LM Studio Tokenä½¿ç”¨ - Model: {getattr(self, 'model_name', 'unknown')}, "
                f"æ€»tokens: {total_tokens}, æç¤º: {prompt_tokens}, è¡¥å…¨: {completion_tokens}, "
                f"ç”¨æ—¶: {elapsed:.2f}s (æœ¬åœ°æ¨ç†)"
            )

            # æœ¬åœ°æ¨¡å‹æ€§èƒ½æç¤º
            if elapsed > 30:
                logger.warning("ğŸ’¡ æœ¬åœ°æ¨¡å‹å“åº”è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜åŒ–ç¡¬ä»¶é…ç½®æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
            elif elapsed < 5:
                logger.info("âš¡ æœ¬åœ°æ¨¡å‹å“åº”å¿«é€Ÿï¼Œæ€§èƒ½è‰¯å¥½")

        except Exception as e:
            logger.warning(f"âš ï¸ LM Studio Tokenè·Ÿè¸ªè®°å½•å¤±è´¥: {e}")

    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯

        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            # è·å–æ‰€æœ‰æ¨¡å‹ä¿¡æ¯
            response = requests.get(
                f"{self.base_url}/models",
                headers=headers,
                timeout=self.timeout
            )

            if response.status_code == 200:
                models_data = response.json()
                all_models = models_data.get("data", [])

                # æŸ¥æ‰¾å½“å‰æ¨¡å‹
                current_model_name = getattr(self, 'model_name', None)
                for model in all_models:
                    if model.get("id") == current_model_name:
                        return {
                            "id": model.get("id"),
                            "object": model.get("object"),
                            "created": model.get("created"),
                            "owned_by": model.get("owned_by"),
                            "permission": model.get("permission"),
                            "root": model.get("root"),
                            "parent": model.get("parent"),
                            "is_local": True,
                            "provider": "LM Studio"
                        }

            # å¦‚æœæ²¡æ‰¾åˆ°å½“å‰æ¨¡å‹ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                "id": current_model_name or "unknown",
                "object": "model",
                "is_local": True,
                "provider": "LM Studio",
                "note": "æ¨¡å‹ä¿¡æ¯è·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¿¡æ¯"
            }

        except Exception as e:
            logger.warning(f"âš ï¸ è·å–LM Studioæ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "id": getattr(self, 'model_name', 'unknown'),
                "object": "model",
                "is_local": True,
                "provider": "LM Studio",
                "error": str(e)
            }


# LM Studioé€‚é…å™¨æ³¨å†Œä¿¡æ¯
LM_STUDIO_CONFIG = {
    "adapter_class": ChatLMStudio,
    "base_url": "http://localhost:1234/v1",
    "api_key_env": "LM_STUDIO_API_KEY",
    "supports_local_models": True,
    "default_models": [
        "llama-3.1-8b-instruct"
    ],
    "models": {
        "llama-3.1-8b-instruct": {
            "context_length": 128000,
            "supports_function_calling": True,
            "description": "Llama 3.1 8B Instruct",
            "recommended_for": ["å¤æ‚æ¨ç†", "ä¸“ä¸šåˆ†æ", "é«˜è´¨é‡è¾“å‡º"]
        }
    }
}


def create_lmstudio_llm(
    model: str = "llama-3.1-8b-instruct",
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: Optional[int] = None,
    timeout: int = 30,
    **kwargs
) -> ChatLMStudio:
    """
    åˆ›å»ºLM Studio LLMå®ä¾‹çš„å·¥å‚å‡½æ•°

    Args:
        model: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        base_url: æœåŠ¡åœ°å€
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        timeout: è¶…æ—¶æ—¶é—´
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        LM Studioé€‚é…å™¨å®ä¾‹
    """
    return ChatLMStudio(
        model=model,
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
        **kwargs
    )


def test_lmstudio_adapter():
    """æµ‹è¯•LM Studioé€‚é…å™¨æ˜¯å¦èƒ½è¢«æ­£ç¡®å®ä¾‹åŒ–"""
    try:
        adapter = ChatLMStudio(
            model="llama-3.1-8b-instruct",
            api_key="lm-studio-local",
            base_url="http://localhost:1234/v1"
        )
        logger.info("âœ… LM Studioé€‚é…å™¨å®ä¾‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        logger.info(f"â„¹ï¸ LM Studioé€‚é…å™¨å®ä¾‹åŒ–å¤±è´¥ï¼ˆé¢„æœŸï¼Œå› ä¸ºæœåŠ¡æœªè¿è¡Œï¼‰: {e}")
        return False


if __name__ == "__main__":
    test_lmstudio_adapter()