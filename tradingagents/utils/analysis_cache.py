#!/usr/bin/env python3
"""
è‚¡ç¥¨åˆ†æç»“æœç¼“å­˜ç®¡ç†ç³»ç»Ÿ
æ”¯æŒæŒ‰è‚¡ç¥¨ä»£ç å’Œåˆ†ææ—¥æœŸè¿›è¡Œæ°¸ä¹…ç¼“å­˜
"""

import json
import os
from datetime import datetime, date
from typing import Dict, Any, Optional, List
from pathlib import Path

from tradingagents.utils.logging_manager import get_logger
logger = get_logger('analysis_cache')

class AnalysisCache:
    """è‚¡ç¥¨åˆ†æç»“æœç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸º data/analysis_cache
        """
        if cache_dir is None:
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            project_root = Path(__file__).parent.parent.parent
            cache_dir = project_root / "data" / "analysis_cache"
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“ åˆ†æç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def _get_cache_key(self, symbol: str, analysis_date: str = None) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            analysis_date: åˆ†ææ—¥æœŸï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œé»˜è®¤ä¸ºä»Šå¤©
            
        Returns:
            str: ç¼“å­˜é”®
        """
        if analysis_date is None:
            analysis_date = date.today().strftime('%Y-%m-%d')
        
        # ç¡®ä¿è‚¡ç¥¨ä»£ç å¤§å†™
        symbol = symbol.upper().strip()
        
        return f"{symbol}_{analysis_date}"
    
    def _get_cache_file_path(self, cache_key: str) -> Path:
        """
        è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        Args:
            cache_key: ç¼“å­˜é”®
            
        Returns:
            Path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        return self.cache_dir / f"{cache_key}.json"
    
    def save_analysis(self, symbol: str, analysis_data: Dict[str, Any], analysis_date: str = None) -> bool:
        """
        ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            analysis_data: åˆ†ææ•°æ®
            analysis_date: åˆ†ææ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            cache_key = self._get_cache_key(symbol, analysis_date)
            cache_file = self._get_cache_file_path(cache_key)
            
            # å‡†å¤‡ç¼“å­˜æ•°æ®
            cache_data = {
                'symbol': symbol.upper().strip(),
                'analysis_date': analysis_date or date.today().strftime('%Y-%m-%d'),
                'cached_time': datetime.now().isoformat(),
                'data': analysis_data
            }
            
            # å†™å…¥æ–‡ä»¶
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"âœ… åˆ†æç»“æœå·²ç¼“å­˜: {symbol} ({analysis_date or 'ä»Šå¤©'})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆ†æç¼“å­˜å¤±è´¥: {symbol} - {e}")
            return False
    
    def load_analysis(self, symbol: str, analysis_date: str = None) -> Optional[Dict[str, Any]]:
        """
        ä»ç¼“å­˜åŠ è½½åˆ†æç»“æœ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            analysis_date: åˆ†ææ—¥æœŸï¼Œé»˜è®¤ä¸ºä»Šå¤©
            
        Returns:
            Optional[Dict]: åˆ†ææ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        try:
            cache_key = self._get_cache_key(symbol, analysis_date)
            cache_file = self._get_cache_file_path(cache_key)
            
            if not cache_file.exists():
                logger.debug(f"ğŸ“­ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_key}")
                return None
            
            # è¯»å–æ–‡ä»¶
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            logger.info(f"ğŸ“¦ å·²åŠ è½½åˆ†æç¼“å­˜: {symbol} ({analysis_date or 'ä»Šå¤©'})")
            return cache_data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½åˆ†æç¼“å­˜å¤±è´¥: {symbol} - {e}")
            return None
    
    def exists(self, symbol: str, analysis_date: str = None) -> bool:
        """
        æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            analysis_date: åˆ†ææ—¥æœŸ
            
        Returns:
            bool: ç¼“å­˜æ˜¯å¦å­˜åœ¨
        """
        cache_key = self._get_cache_key(symbol, analysis_date)
        cache_file = self._get_cache_file_path(cache_key)
        return cache_file.exists()
    
    def list_cached_analyses(self, symbol: str = None) -> List[Dict[str, str]]:
        """
        åˆ—å‡ºç¼“å­˜çš„åˆ†æç»“æœ
        
        Args:
            symbol: å¯é€‰ï¼Œç­›é€‰ç‰¹å®šè‚¡ç¥¨ä»£ç 
            
        Returns:
            List[Dict]: ç¼“å­˜åˆ—è¡¨ï¼ŒåŒ…å«symbol, date, cache_keyç­‰ä¿¡æ¯
        """
        try:
            cached_files = []
            
            for cache_file in self.cache_dir.glob("*.json"):
                cache_key = cache_file.stem
                
                # è§£æç¼“å­˜é”®
                parts = cache_key.split('_')
                if len(parts) >= 2:
                    file_symbol = '_'.join(parts[:-1])  # æ”¯æŒåŒ…å«ä¸‹åˆ’çº¿çš„è‚¡ç¥¨ä»£ç 
                    file_date = parts[-1]
                    
                    # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œåˆ™è¿‡æ»¤
                    if symbol and file_symbol.upper() != symbol.upper():
                        continue
                    
                    cached_files.append({
                        'symbol': file_symbol,
                        'date': file_date,
                        'cache_key': cache_key,
                        'file_path': str(cache_file),
                        'file_size': cache_file.stat().st_size,
                        'modified_time': datetime.fromtimestamp(cache_file.stat().st_mtime).isoformat()
                    })
            
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            cached_files.sort(key=lambda x: x['date'], reverse=True)
            
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(cached_files)} ä¸ªç¼“å­˜æ–‡ä»¶")
            return cached_files
            
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def delete_analysis(self, symbol: str, analysis_date: str = None) -> bool:
        """
        åˆ é™¤æŒ‡å®šçš„åˆ†æç¼“å­˜
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            analysis_date: åˆ†ææ—¥æœŸ
            
        Returns:
            bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            cache_key = self._get_cache_key(symbol, analysis_date)
            cache_file = self._get_cache_file_path(cache_key)
            
            if cache_file.exists():
                cache_file.unlink()
                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤åˆ†æç¼“å­˜: {symbol} ({analysis_date or 'ä»Šå¤©'})")
                return True
            else:
                logger.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_key}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤åˆ†æç¼“å­˜å¤±è´¥: {symbol} - {e}")
            return False
    
    def cleanup_old_cache(self, days: int = 30) -> int:
        """
        æ¸…ç†æ—§çš„ç¼“å­˜æ–‡ä»¶ï¼ˆå¼€å‘æ¨¡å¼ä¸‹ä¸ä½¿ç”¨ï¼Œç”Ÿäº§ç¯å¢ƒå¯ä»¥ä½¿ç”¨ï¼‰
        
        Args:
            days: ä¿ç•™å¤©æ•°
            
        Returns:
            int: åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        try:
            from datetime import timedelta
            
            cutoff_time = datetime.now() - timedelta(days=days)
            deleted_count = 0
            
            for cache_file in self.cache_dir.glob("*.json"):
                file_time = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if file_time < cutoff_time:
                    cache_file.unlink()
                    deleted_count += 1
            
            logger.info(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} ä¸ªæ—§ç¼“å­˜æ–‡ä»¶ï¼ˆè¶…è¿‡{days}å¤©ï¼‰")
            return deleted_count
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
            return 0
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            cache_files = list(self.cache_dir.glob("*.json"))
            total_files = len(cache_files)
            total_size = sum(f.stat().st_size for f in cache_files)
            
            # æŒ‰è‚¡ç¥¨åˆ†ç»„ç»Ÿè®¡
            symbol_stats = {}
            for cache_file in cache_files:
                cache_key = cache_file.stem
                parts = cache_key.split('_')
                if len(parts) >= 2:
                    symbol = '_'.join(parts[:-1])
                    if symbol not in symbol_stats:
                        symbol_stats[symbol] = 0
                    symbol_stats[symbol] += 1
            
            return {
                'total_files': total_files,
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / 1024 / 1024, 2),
                'cache_directory': str(self.cache_dir),
                'symbol_count': len(symbol_stats),
                'symbol_stats': symbol_stats
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {}


# å…¨å±€ç¼“å­˜å®ä¾‹
_global_cache = None

def get_global_cache() -> AnalysisCache:
    """è·å–å…¨å±€ç¼“å­˜å®ä¾‹"""
    global _global_cache
    if _global_cache is None:
        _global_cache = AnalysisCache()
    return _global_cache


def cache_analysis_result(symbol: str, analysis_data: Dict[str, Any], analysis_date: str = None) -> bool:
    """
    ä¾¿æ·å‡½æ•°ï¼šç¼“å­˜åˆ†æç»“æœ
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        analysis_data: åˆ†ææ•°æ®
        analysis_date: åˆ†ææ—¥æœŸ
        
    Returns:
        bool: æ˜¯å¦ç¼“å­˜æˆåŠŸ
    """
    cache = get_global_cache()
    return cache.save_analysis(symbol, analysis_data, analysis_date)


def load_cached_analysis(symbol: str, analysis_date: str = None) -> Optional[Dict[str, Any]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåŠ è½½ç¼“å­˜çš„åˆ†æç»“æœ
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        analysis_date: åˆ†ææ—¥æœŸ
        
    Returns:
        Optional[Dict]: åˆ†ææ•°æ®
    """
    cache = get_global_cache()
    return cache.load_analysis(symbol, analysis_date)


def is_analysis_cached(symbol: str, analysis_date: str = None) -> bool:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ£€æŸ¥åˆ†ææ˜¯å¦å·²ç¼“å­˜
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç 
        analysis_date: åˆ†ææ—¥æœŸ
        
    Returns:
        bool: æ˜¯å¦å·²ç¼“å­˜
    """
    cache = get_global_cache()
    return cache.exists(symbol, analysis_date)