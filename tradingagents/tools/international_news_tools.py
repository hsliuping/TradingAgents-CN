#!/usr/bin/env python3
"""
å›½é™…æ–°é—»æ•°æ®å·¥å…·

æä¾›å½­åšç¤¾ã€è·¯é€ç¤¾ã€Google Newsç­‰å›½é™…åª’ä½“æ•°æ®æº
ç”¨äºInternational News Analystè·å–çŸ­æœŸæ–°é—»å½±å“

æ•°æ®æºä¼˜å…ˆçº§ï¼š
1. NewsAPI (ä»˜è´¹ï¼Œéœ€é…ç½®NEWSAPI_KEY) - å½­åšç¤¾ã€è·¯é€ç¤¾
2. Google News (å…è´¹é™çº§æ–¹æ¡ˆ)
"""

from langchain.tools import tool
from typing import Annotated
from datetime import datetime, timedelta
import os
import requests

from tradingagents.utils.logging_manager import get_logger

logger = get_logger("tools")


@tool
def fetch_bloomberg_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯ï¼Œå¦‚'China semiconductor policy'"],
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
) -> str:
    """
    è·å–å½­åšç¤¾æ–°é—»
    
    æ•°æ®æº: NewsAPI (bloomberg.com)
    é™çº§æ–¹æ¡ˆ: Google News
    
    Args:
        keywords: æœç´¢å…³é”®è¯ï¼Œå»ºè®®ä½¿ç”¨è‹±æ–‡
        lookback_days: å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    try:
        logger.info(f"ğŸŒ [å½­åšç¤¾æ–°é—»] å¼€å§‹è·å–ï¼Œå…³é”®è¯: {keywords}, å›æº¯: {lookback_days}å¤©")
        
        # 1. æ£€æŸ¥NewsAPIé…ç½®
        api_key = os.getenv("NEWSAPI_KEY")
        if not api_key:
            logger.warning("âš ï¸ NewsAPI Keyæœªé…ç½®ï¼Œé™çº§åˆ°Google News")
            # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
            return _fetch_google_news_impl(keywords, lookback_days)
        
        # 2. è°ƒç”¨NewsAPI
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days)
        
        url = "https://newsapi.org/v2/everything"
        params = {
            "apiKey": api_key,
            "sources": "bloomberg",
            "q": keywords,
            "from": start_date.strftime("%Y-%m-%d"),
            "to": end_date.strftime("%Y-%m-%d"),
            "language": "en",
            "sortBy": "publishedAt",
            "pageSize": 10
        }
        
        logger.info(f"ğŸŒ [å½­åšç¤¾æ–°é—»] è¯·æ±‚NewsAPI: {url}")
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        articles = data.get("articles", [])
        
        if not articles:
            logger.warning(f"âš ï¸ [å½­åšç¤¾æ–°é—»] æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
            return f"## å½­åšç¤¾æ–°é—» (å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»ã€‚"
        
        # 3. æ ¼å¼åŒ–ä¸ºMarkdown
        result = _format_news_to_markdown(articles, "Bloomberg", keywords)
        
        logger.info(f"âœ… [å½­åšç¤¾æ–°é—»] è·å–æˆåŠŸ: {len(articles)} æ¡")
        return result
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ [å½­åšç¤¾æ–°é—»] APIè¯·æ±‚å¤±è´¥: {e}")
        # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
        return _fetch_google_news_impl(keywords, lookback_days)
    except Exception as e:
        logger.error(f"âŒ [å½­åšç¤¾æ–°é—»] è·å–å¤±è´¥: {e}")
        # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
        return _fetch_google_news_impl(keywords, lookback_days)


@tool
def fetch_reuters_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯"],
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
) -> str:
    """
    è·å–è·¯é€ç¤¾æ–°é—»
    
    æ•°æ®æº: NewsAPI (reuters.com)
    é™çº§æ–¹æ¡ˆ: Google News
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        lookback_days: å›æº¯å¤©æ•°
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    try:
        logger.info(f"ğŸŒ [è·¯é€ç¤¾æ–°é—»] å¼€å§‹è·å–ï¼Œå…³é”®è¯: {keywords}")
        
        # 1. æ£€æŸ¥NewsAPIé…ç½®
        api_key = os.getenv("NEWSAPI_KEY")
        if not api_key:
            logger.warning("âš ï¸ NewsAPI Keyæœªé…ç½®ï¼Œé™çº§åˆ°Google News")
            # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
            return _fetch_google_news_impl(keywords, lookback_days)
        
        # 2. è°ƒç”¨NewsAPI
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days)
        
        url = "https://newsapi.org/v2/everything"
        params = {
            "apiKey": api_key,
            "sources": "reuters",
            "q": keywords,
            "from": start_date.strftime("%Y-%m-%d"),
            "to": end_date.strftime("%Y-%m-%d"),
            "language": "en",
            "sortBy": "publishedAt",
            "pageSize": 10
        }
        
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        articles = data.get("articles", [])
        
        if not articles:
            logger.warning(f"âš ï¸ [è·¯é€ç¤¾æ–°é—»] æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
            return f"## è·¯é€ç¤¾æ–°é—» (å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»ã€‚"
        
        # 3. æ ¼å¼åŒ–ä¸ºMarkdown
        result = _format_news_to_markdown(articles, "Reuters", keywords)
        
        logger.info(f"âœ… [è·¯é€ç¤¾æ–°é—»] è·å–æˆåŠŸ: {len(articles)} æ¡")
        return result
        
    except Exception as e:
        logger.error(f"âŒ [è·¯é€ç¤¾æ–°é—»] è·å–å¤±è´¥: {e}")
        # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
        return _fetch_google_news_impl(keywords, lookback_days)


def _fetch_google_news_impl(keywords: str, lookback_days: int = 7) -> str:
    """
    å†…éƒ¨å®ç°ï¼šè·å–Google Newsæ–°é—»ï¼ˆå…è´¹é™çº§æ–¹æ¡ˆï¼‰
    
    ä½¿ç”¨Google News RSS Feed
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        lookback_days: å›æº¯å¤©æ•°ï¼ˆå‚è€ƒå€¼ï¼ŒGoogle Newså¯èƒ½ä¸ä¸¥æ ¼éµå®ˆï¼‰
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    try:
        logger.info(f"ğŸŒ [Google News] å¼€å§‹è·å–ï¼Œå…³é”®è¯: {keywords}")
        
        from GoogleNews import GoogleNews
        
        # é…ç½®GoogleNews
        googlenews = GoogleNews(lang='en', period=f'{lookback_days}d')
        googlenews.search(keywords)
        results = googlenews.results()
        
        if not results:
            logger.warning(f"âš ï¸ [Google News] æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
            return f"## Google News (å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»ã€‚"
        
        # æ ¼å¼åŒ–ä¸ºMarkdown
        output = f"## Google News æ–°é—»æ‘˜è¦ (å…³é”®è¯: {keywords})\n\n"
        
        for idx, article in enumerate(results[:10], 1):
            title = article.get('title', 'æ— æ ‡é¢˜')
            date = article.get('date', article.get('datetime', ''))
            media = article.get('media', article.get('source', ''))
            link = article.get('link', '')
            desc = article.get('desc', article.get('description', ''))
            
            output += f"### {idx}. {title}\n"
            output += f"**å‘å¸ƒæ—¶é—´**: {date}\n"
            if media:
                output += f"**æ¥æº**: {media}\n"
            if desc:
                output += f"**æ‘˜è¦**: {desc}\n"
            if link:
                output += f"**é“¾æ¥**: {link}\n"
            output += "\n"
        
        logger.info(f"âœ… [Google News] è·å–æˆåŠŸ: {len(results)} æ¡")
        return output
        
    except ImportError:
        logger.error("âŒ [Google News] GoogleNewsåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install GoogleNews")
        return "Google Newsè·å–å¤±è´¥: GoogleNewsåº“æœªå®‰è£…"
    except Exception as e:
        logger.error(f"âŒ [Google News] è·å–å¤±è´¥: {e}")
        return f"Google Newsè·å–å¤±è´¥: {str(e)}"


@tool
def fetch_google_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯"],
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
) -> str:
    """
    è·å–Google Newsæ–°é—»ï¼ˆå…è´¹é™çº§æ–¹æ¡ˆï¼‰
    
    ä½¿ç”¨Google News RSS Feed
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        lookback_days: å›æº¯å¤©æ•°ï¼ˆå‚è€ƒå€¼ï¼ŒGoogle Newså¯èƒ½ä¸ä¸¥æ ¼éµå®ˆï¼‰
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    return _fetch_google_news_impl(keywords, lookback_days)


def _format_news_to_markdown(articles: list, source: str, keywords: str) -> str:
    """
    æ ¼å¼åŒ–æ–°é—»ä¸ºMarkdown
    
    Args:
        articles: æ–°é—»åˆ—è¡¨
        source: æ–°é—»æºåç§°
        keywords: æœç´¢å…³é”®è¯
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    if not articles:
        return f"## {source} (å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»"
    
    md = f"## {source} æ–°é—»æ‘˜è¦ (å…³é”®è¯: {keywords})\n\n"
    
    for i, article in enumerate(articles[:10], 1):
        title = article.get('title', 'æ— æ ‡é¢˜')
        published = article.get('publishedAt', article.get('published', ''))
        description = article.get('description', article.get('summary', ''))
        url = article.get('url', '')
        
        md += f"### {i}. {title}\n"
        md += f"**å‘å¸ƒæ—¶é—´**: {published}\n"
        if description:
            md += f"**æ‘˜è¦**: {description}\n"
        if url:
            md += f"**é“¾æ¥**: {url}\n"
        md += "\n"
    
    return md


@tool
def fetch_cn_international_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯"] = "",
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
) -> str:
    """
    è·å–å›½é™…æ–°é—»ï¼ˆå›½å†…æºï¼‰
    
    ä½¿ç”¨AKShareè·å–ä¸œæ–¹è´¢å¯Œç¾è‚¡/å…¨çƒæ–°é—»
    ä½œä¸ºç½‘ç»œå—é™ç¯å¢ƒä¸‹çš„æ›¿ä»£æ–¹æ¡ˆ
    
    Args:
        keywords: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        lookback_days: å›æº¯å¤©æ•°
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    try:
        logger.info(f"ğŸŒ [å›½é™…æ–°é—»(å›½å†…æº)] å¼€å§‹è·å–ï¼Œå…³é”®è¯: {keywords}")
        
        from tradingagents.dataflows.index_data import IndexDataProvider
        
        provider = IndexDataProvider()
        news_list = provider.get_international_news(keywords, lookback_days)
        
        if not news_list:
            return f"## å›½é™…æ–°é—» (å›½å†…æº, å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»"
            
        md = f"## å›½é™…æ–°é—»æ‘˜è¦ (å›½å†…æº, å…³é”®è¯: {keywords})\n\n"
        
        for i, news in enumerate(news_list[:15], 1):
            title = news.get('title', 'æ— æ ‡é¢˜')
            date = news.get('date', '')
            source = news.get('source', '')
            content = news.get('content', '')
            url = news.get('url', '')
            
            md += f"### {i}. {title}\n"
            md += f"**å‘å¸ƒæ—¶é—´**: {date}\n"
            md += f"**æ¥æº**: {source}\n"
            if content and len(content) > 10:
                # æˆªå–å‰100ä¸ªå­—ç¬¦
                md += f"**æ‘˜è¦**: {content[:100]}...\n"
            if url:
                md += f"**é“¾æ¥**: {url}\n"
            md += "\n"
            
        logger.info(f"âœ… [å›½é™…æ–°é—»(å›½å†…æº)] è·å–æˆåŠŸ: {len(news_list)} æ¡")
        return md
        
    except Exception as e:
        logger.error(f"âŒ [å›½é™…æ–°é—»(å›½å†…æº)] è·å–å¤±è´¥: {e}")
        return f"å›½é™…æ–°é—»(å›½å†…æº)è·å–å¤±è´¥: {str(e)}"


# å·¥å…·åˆ—è¡¨å¯¼å‡º
INTERNATIONAL_NEWS_TOOLS = [
    fetch_bloomberg_news,
    fetch_reuters_news,
    fetch_google_news,
    fetch_cn_international_news
]
