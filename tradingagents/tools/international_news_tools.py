#!/usr/bin/env python3
"""
å›½é™…æ–°é—»æ•°æ®å·¥å…·

æä¾›å½­åšç¤¾ã€è·¯é€ç¤¾ã€Google Newsç­‰å›½é™…åª’ä½“æ•°æ®æº
ç”¨äºInternational News Analystè·å–çŸ­æœŸæ–°é—»å½±å“

æ•°æ®æºä¼˜å…ˆçº§ï¼š
1. NewsAPI (ä»˜è´¹ï¼Œéœ€é…ç½®NEWSAPI_KEY) - å½­åšç¤¾ã€è·¯é€ç¤¾
2. Google News (å…è´¹é™çº§æ–¹æ¡ˆ)
"""

from langchain.tools import tool
from typing import Annotated
from datetime import datetime, timedelta
import os
import requests

from tradingagents.utils.logging_manager import get_logger

logger = get_logger("tools")


@tool
def fetch_bloomberg_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯ï¼Œå¦‚'China semiconductor policy'"],
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
) -> str:
    """
    è·å–å½­åšç¤¾æ–°é—»
    
    æ•°æ®æº: NewsAPI (bloomberg.com)
    é™çº§æ–¹æ¡ˆ: Google News
    
    Args:
        keywords: æœç´¢å…³é”®è¯ï¼Œå»ºè®®ä½¿ç”¨è‹±æ–‡
        lookback_days: å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    try:
        logger.info(f"ğŸŒ [å½­åšç¤¾æ–°é—»] å¼€å§‹è·å–ï¼Œå…³é”®è¯: {keywords}, å›æº¯: {lookback_days}å¤©")
        
        # 1. æ£€æŸ¥NewsAPIé…ç½®
        api_key = os.getenv("NEWSAPI_KEY")
        if not api_key:
            logger.warning("âš ï¸ NewsAPI Keyæœªé…ç½®ï¼Œé™çº§åˆ°Google News")
            # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
            return _fetch_google_news_impl(keywords, lookback_days)
        
        # 2. è°ƒç”¨NewsAPI
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days)
        
        url = "https://newsapi.org/v2/everything"
        params = {
            "apiKey": api_key,
            "sources": "bloomberg",
            "q": keywords,
            "from": start_date.strftime("%Y-%m-%d"),
            "to": end_date.strftime("%Y-%m-%d"),
            "language": "en",
            "sortBy": "publishedAt",
            "pageSize": 10
        }
        
        logger.info(f"ğŸŒ [å½­åšç¤¾æ–°é—»] è¯·æ±‚NewsAPI: {url}")
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        articles = data.get("articles", [])
        
        if not articles:
            logger.warning(f"âš ï¸ [å½­åšç¤¾æ–°é—»] æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
            return f"## å½­åšç¤¾æ–°é—» (å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»ã€‚"
        
        # 3. æ ¼å¼åŒ–ä¸ºMarkdown
        result = _format_news_to_markdown(articles, "Bloomberg", keywords)
        
        logger.info(f"âœ… [å½­åšç¤¾æ–°é—»] è·å–æˆåŠŸ: {len(articles)} æ¡")
        return result
        
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ [å½­åšç¤¾æ–°é—»] APIè¯·æ±‚å¤±è´¥: {e}")
        # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
        return _fetch_google_news_impl(keywords, lookback_days)
    except Exception as e:
        logger.error(f"âŒ [å½­åšç¤¾æ–°é—»] è·å–å¤±è´¥: {e}")
        # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
        return _fetch_google_news_impl(keywords, lookback_days)


@tool
def fetch_reuters_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯"],
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
) -> str:
    """
    è·å–è·¯é€ç¤¾æ–°é—»
    
    æ•°æ®æº: NewsAPI (reuters.com)
    é™çº§æ–¹æ¡ˆ: Google News
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        lookback_days: å›æº¯å¤©æ•°
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    try:
        logger.info(f"ğŸŒ [è·¯é€ç¤¾æ–°é—»] å¼€å§‹è·å–ï¼Œå…³é”®è¯: {keywords}")
        
        # 1. æ£€æŸ¥NewsAPIé…ç½®
        api_key = os.getenv("NEWSAPI_KEY")
        if not api_key:
            logger.warning("âš ï¸ NewsAPI Keyæœªé…ç½®ï¼Œé™çº§åˆ°Google News")
            # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
            return _fetch_google_news_impl(keywords, lookback_days)
        
        # 2. è°ƒç”¨NewsAPI
        end_date = datetime.now()
        start_date = end_date - timedelta(days=lookback_days)
        
        url = "https://newsapi.org/v2/everything"
        params = {
            "apiKey": api_key,
            "sources": "reuters",
            "q": keywords,
            "from": start_date.strftime("%Y-%m-%d"),
            "to": end_date.strftime("%Y-%m-%d"),
            "language": "en",
            "sortBy": "publishedAt",
            "pageSize": 10
        }
        
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        articles = data.get("articles", [])
        
        if not articles:
            logger.warning(f"âš ï¸ [è·¯é€ç¤¾æ–°é—»] æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
            return f"## è·¯é€ç¤¾æ–°é—» (å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»ã€‚"
        
        # 3. æ ¼å¼åŒ–ä¸ºMarkdown
        result = _format_news_to_markdown(articles, "Reuters", keywords)
        
        logger.info(f"âœ… [è·¯é€ç¤¾æ–°é—»] è·å–æˆåŠŸ: {len(articles)} æ¡")
        return result
        
    except Exception as e:
        logger.error(f"âŒ [è·¯é€ç¤¾æ–°é—»] è·å–å¤±è´¥: {e}")
        # ç›´æ¥è°ƒç”¨å®ç°å‡½æ•°è€Œä¸æ˜¯å·¥å…·å¯¹è±¡
        return _fetch_google_news_impl(keywords, lookback_days)


def _fetch_google_news_impl(keywords: str, lookback_days: int = 7) -> str:
    """
    å†…éƒ¨å®ç°ï¼šè·å–Google Newsæ–°é—»ï¼ˆå…è´¹é™çº§æ–¹æ¡ˆï¼‰
    
    ä½¿ç”¨Google News RSS Feed
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        lookback_days: å›æº¯å¤©æ•°ï¼ˆå‚è€ƒå€¼ï¼ŒGoogle Newså¯èƒ½ä¸ä¸¥æ ¼éµå®ˆï¼‰
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    try:
        logger.info(f"ğŸŒ [Google News] å¼€å§‹è·å–ï¼Œå…³é”®è¯: {keywords}")
        
        from GoogleNews import GoogleNews
        
        # é…ç½®GoogleNews
        googlenews = GoogleNews(lang='en', period=f'{lookback_days}d')
        googlenews.search(keywords)
        results = googlenews.results()
        
        if not results:
            logger.warning(f"âš ï¸ [Google News] æœªæ‰¾åˆ°ç›¸å…³æ–°é—»")
            return _fetch_cn_news_fallback(keywords, lookback_days)
        
        # æ ¼å¼åŒ–ä¸ºMarkdown
        output = f"## Google News æ–°é—»æ‘˜è¦ (å…³é”®è¯: {keywords})\n\n"
        
        for idx, article in enumerate(results[:10], 1):
            title = article.get('title', 'æ— æ ‡é¢˜')
            date = article.get('date', article.get('datetime', ''))
            media = article.get('media', article.get('source', ''))
            link = article.get('link', '')
            desc = article.get('desc', article.get('description', ''))
            
            output += f"### {idx}. {title}\n"
            output += f"**å‘å¸ƒæ—¶é—´**: {date}\n"
            if media:
                output += f"**æ¥æº**: {media}\n"
            if desc:
                output += f"**æ‘˜è¦**: {desc}\n"
            if link:
                output += f"**é“¾æ¥**: {link}\n"
            output += "\n"
        
        logger.info(f"âœ… [Google News] è·å–æˆåŠŸ: {len(results)} æ¡")
        return output
        
    except ImportError:
        logger.error("âŒ [Google News] GoogleNewsåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install GoogleNews")
        return _fetch_cn_news_fallback(keywords, lookback_days)
    except Exception as e:
        logger.error(f"âŒ [Google News] è·å–å¤±è´¥: {e}")
        return _fetch_cn_news_fallback(keywords, lookback_days)


def _fetch_cn_news_fallback(keywords: str, lookback_days: int) -> str:
    """
    é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨å›½å†…æºè·å–å›½é™…æ–°é—»
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        lookback_days: å›æº¯å¤©æ•°
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    logger.info(f"âš ï¸ [å›½é™…æ–°é—»] Google Newsä¸å¯ç”¨ï¼Œé™çº§ä½¿ç”¨å›½å†…æº (AKShare/é‡‘åæ•°æ®)...")
    try:
        from tradingagents.dataflows.index_data import get_index_data_provider
        
        # è·å–å…¨å±€Providerå®ä¾‹
        provider = get_index_data_provider()
        
        # å°è¯•è°ƒç”¨åŒæ­¥æ–¹æ³• (HybridIndexDataProviderå·²å®ç°åŒæ­¥åŒ…è£…å™¨)
        if hasattr(provider, 'get_international_news'):
            # è‡ªåŠ¨æ£€æµ‹è‹±æ–‡å…³é”®è¯å¹¶å¤„ç†
            search_keywords = keywords
            if keywords and all(ord(c) < 128 for c in keywords):
                logger.info(f"âš ï¸ [å›½é™…æ–°é—»] æ£€æµ‹åˆ°è‹±æ–‡å…³é”®è¯ '{keywords}'ï¼Œé™çº§åˆ°å›½å†…æºæ—¶å°†è·å–å…¨é‡æ–°é—»ä»¥æé«˜å‘½ä¸­ç‡")
                search_keywords = ""
                
            news_list = provider.get_international_news(search_keywords, lookback_days)
            
            if not news_list:
                return f"## å›½é™…æ–°é—» (å›½å†…æº, å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—» (æ•°æ®æºå¯èƒ½ä¸ºç©ºæˆ–å…³é”®è¯æœªåŒ¹é…)ã€‚"
                
            md = f"## å›½é™…æ–°é—»æ‘˜è¦ (å›½å†…æº, å…³é”®è¯: {keywords})\n\n"
            
            for i, news in enumerate(news_list[:10], 1):
                title = news.get('title', 'æ— æ ‡é¢˜')
                date = news.get('date', '')
                source = news.get('source', '')
                content = news.get('content', '')
                
                md += f"### {i}. {title}\n"
                md += f"**å‘å¸ƒæ—¶é—´**: {date}\n"
                md += f"**æ¥æº**: {source}\n"
                if content and len(content) > 10:
                    md += f"**æ‘˜è¦**: {content[:100]}...\n"
                md += "\n"
                
            logger.info(f"âœ… [å›½é™…æ–°é—»] å›½å†…æºè·å–æˆåŠŸ: {len(news_list)} æ¡")
            return md
            
        else:
            return f"å›½é™…æ–°é—»è·å–å¤±è´¥: Google Newsä¸å¯ç”¨ä¸”å›½å†…æºæœªå®ç°get_international_news"
            
    except Exception as e:
        logger.error(f"âŒ [å›½é™…æ–°é—»] å›½å†…æºé™çº§å¤±è´¥: {e}")
        return f"å›½é™…æ–°é—»è·å–å¤±è´¥: Google Newsä¸å¯ç”¨ä¸”å›½å†…æºæŠ¥é”™ ({str(e)})"


@tool
def fetch_google_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯"],
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
) -> str:
    """
    è·å–Google Newsæ–°é—»ï¼ˆå…è´¹é™çº§æ–¹æ¡ˆï¼‰
    
    ä½¿ç”¨Google News RSS Feed
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        lookback_days: å›æº¯å¤©æ•°ï¼ˆå‚è€ƒå€¼ï¼ŒGoogle Newså¯èƒ½ä¸ä¸¥æ ¼éµå®ˆï¼‰
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    return _fetch_google_news_impl(keywords, lookback_days)


def _format_news_to_markdown(articles: list, source: str, keywords: str) -> str:
    """
    æ ¼å¼åŒ–æ–°é—»ä¸ºMarkdown
    
    Args:
        articles: æ–°é—»åˆ—è¡¨
        source: æ–°é—»æºåç§°
        keywords: æœç´¢å…³é”®è¯
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    if not articles:
        return f"## {source} (å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»"
    
    md = f"## {source} æ–°é—»æ‘˜è¦ (å…³é”®è¯: {keywords})\n\n"
    
    for i, article in enumerate(articles[:10], 1):
        title = article.get('title', 'æ— æ ‡é¢˜')
        published = article.get('publishedAt', article.get('published', ''))
        description = article.get('description', article.get('summary', ''))
        url = article.get('url', '')
        
        md += f"### {i}. {title}\n"
        md += f"**å‘å¸ƒæ—¶é—´**: {published}\n"
        if description:
            md += f"**æ‘˜è¦**: {description}\n"
        if url:
            md += f"**é“¾æ¥**: {url}\n"
        md += "\n"
    
    return md


@tool
async def fetch_cn_international_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯"] = "",
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
) -> str:
    """
    è·å–å›½é™…æ–°é—»ï¼ˆå›½å†…æºï¼‰
    
    ä½¿ç”¨AKShareè·å–ä¸œæ–¹è´¢å¯Œç¾è‚¡/å…¨çƒæ–°é—»
    ä½œä¸ºç½‘ç»œå—é™ç¯å¢ƒä¸‹çš„æ›¿ä»£æ–¹æ¡ˆ
    
    Args:
        keywords: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
        lookback_days: å›æº¯å¤©æ•°
        
    Returns:
        Markdownæ ¼å¼çš„æ–°é—»æ‘˜è¦
    """
    try:
        logger.info(f"ğŸŒ [å›½é™…æ–°é—»(å›½å†…æº)] å¼€å§‹è·å–ï¼Œå…³é”®è¯: {keywords}")
        
        from tradingagents.dataflows.hybrid_provider import HybridIndexDataProvider
        
        provider = HybridIndexDataProvider()
        
        if hasattr(provider, 'get_international_news_async'):
            news_list = await provider.get_international_news_async(keywords, lookback_days)
        else:
            # Fallback (should not happen if hybrid_provider updated)
            loop = asyncio.get_running_loop()
            news_list = await loop.run_in_executor(None, getattr(provider, 'get_international_news', lambda x,y: []), keywords, lookback_days)
        
        if not news_list:
            return f"## å›½é™…æ–°é—» (å›½å†…æº, å…³é”®è¯: {keywords})\n\næš‚æ— ç›¸å…³æ–°é—»"
            
        md = f"## å›½é™…æ–°é—»æ‘˜è¦ (å›½å†…æº, å…³é”®è¯: {keywords})\n\n"
        
        for i, news in enumerate(news_list[:15], 1):
            title = news.get('title', 'æ— æ ‡é¢˜')
            date = news.get('date', '')
            source = news.get('source', '')
            content = news.get('content', '')
            url = news.get('url', '')
            
            md += f"### {i}. {title}\n"
            md += f"**å‘å¸ƒæ—¶é—´**: {date}\n"
            md += f"**æ¥æº**: {source}\n"
            if content and len(content) > 10:
                # æˆªå–å‰100ä¸ªå­—ç¬¦
                md += f"**æ‘˜è¦**: {content[:100]}...\n"
            if url:
                md += f"**é“¾æ¥**: {url}\n"
            md += "\n"
            
        logger.info(f"âœ… [å›½é™…æ–°é—»(å›½å†…æº)] è·å–æˆåŠŸ: {len(news_list)} æ¡")
        return md
        
    except Exception as e:
        logger.error(f"âŒ [å›½é™…æ–°é—»(å›½å†…æº)] è·å–å¤±è´¥: {e}")
        return f"å›½é™…æ–°é—»(å›½å†…æº)è·å–å¤±è´¥: {str(e)}"


import asyncio
from concurrent.futures import ThreadPoolExecutor

@tool
async def fetch_aggregated_news(
    keywords: Annotated[str, "æœç´¢å…³é”®è¯ï¼Œå¦‚'China economic stimulus'"],
    lookback_days: Annotated[int, "å›æº¯å¤©æ•°ï¼Œé»˜è®¤3å¤©"] = 3
) -> str:
    """
    èšåˆè·å–å¤šæºæ–°é—»ï¼ˆå›½é™…+å›½å†…ï¼‰
    
    å¹¶è¡Œè°ƒç”¨ä»¥ä¸‹æ•°æ®æºï¼Œç¡®ä¿è‡³å°‘æœ‰æ•°æ®è¿”å›ï¼š
    1. å½­åšç¤¾ (Bloomberg via NewsAPI)
    2. è·¯é€ç¤¾ (Reuters via NewsAPI)
    3. Google News
    4. å›½å†…å›½é™…æ–°é—»æº (AKShare/é‡‘åæ•°æ®)
    5. å›½å†…æ”¿ç­–æ–°é—» (Policy News)
    
    Args:
        keywords: æœç´¢å…³é”®è¯
        lookback_days: å›æº¯å¤©æ•°
        
    Returns:
        Markdownæ ¼å¼çš„èšåˆæ–°é—»æŠ¥å‘Š
    """
    logger.info(f"ğŸŒ [èšåˆæ–°é—»å·¥å…·] å¼€å§‹å¹¶è¡Œè·å–å¤šæºæ–°é—», å…³é”®è¯: {keywords}")
    
    # å®šä¹‰å„ä¸ªå­ä»»åŠ¡
    
    # 1. Bloomberg (Sync -> Thread)
    async def task_bloomberg():
        try:
            # æ£€æŸ¥API Keyï¼Œå¦‚æœæ²¡æœ‰åˆ™ç›´æ¥è¿”å›ç©ºï¼Œé¿å…é™çº§é€»è¾‘é‡å¤æ‰§è¡ŒGoogle News
            if not os.getenv("NEWSAPI_KEY"):
                return ""
            
            # ä½¿ç”¨ run_in_executor æ‰§è¡ŒåŒæ­¥çš„ fetch_bloomberg_news
            # æ³¨æ„ï¼šfetch_bloomberg_news æ˜¯ Tool å¯¹è±¡ï¼Œæˆ‘ä»¬éœ€è¦è°ƒç”¨å…¶ run æˆ– invokeï¼Œæˆ–è€…æå–é€»è¾‘
            # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨ tool.invokeï¼Œä½†è¦å°å¿ƒå®ƒçš„é™çº§é€»è¾‘
            # æ›´å¥½çš„æ–¹å¼æ˜¯ï¼šåœ¨ task ä¸­ç›´æ¥ä½¿ç”¨ requests è°ƒç”¨ï¼Œæˆ–è€…å¤ç”¨ _fetch_newsapi_impl (å¦‚æœæå–çš„è¯)
            # ç”±äºæœªæå–ï¼Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨ toolï¼Œä½†å¦‚æœä¸é…ç½® Keyï¼Œtool ä¼šé™çº§è°ƒç”¨ Googleã€‚
            # è¿™ä¼šå¯¼è‡´ Google è¢«è°ƒç”¨ä¸¤æ¬¡ã€‚
            # å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ä¸Šé¢åŠ äº† Key æ£€æŸ¥ã€‚
            
            # è°ƒç”¨ Tool å¯¹è±¡éœ€è¦ä¼ å…¥å­—å…¸
            return await asyncio.to_thread(fetch_bloomberg_news.invoke, {"keywords": keywords, "lookback_days": lookback_days})
        except Exception as e:
            logger.warning(f"âš ï¸ [èšåˆæ–°é—»] Bloombergä»»åŠ¡å¤±è´¥: {e}")
            return ""

    # 2. Reuters (Sync -> Thread)
    async def task_reuters():
        try:
            if not os.getenv("NEWSAPI_KEY"):
                return ""
            return await asyncio.to_thread(fetch_reuters_news.invoke, {"keywords": keywords, "lookback_days": lookback_days})
        except Exception as e:
            logger.warning(f"âš ï¸ [èšåˆæ–°é—»] Reutersä»»åŠ¡å¤±è´¥: {e}")
            return ""

    # 3. Google News (Sync -> Thread)
    async def task_google():
        try:
            # ç›´æ¥è°ƒç”¨å†…éƒ¨å®ç°ï¼Œé¿å… Tool åŒ…è£…å¸¦æ¥çš„é¢å¤–å¼€é”€
            return await asyncio.to_thread(_fetch_google_news_impl, keywords, lookback_days)
        except Exception as e:
            logger.warning(f"âš ï¸ [èšåˆæ–°é—»] Google Newsä»»åŠ¡å¤±è´¥: {e}")
            return ""

    # 4. å›½å†…å›½é™…æ–°é—»æº (Async)
    async def task_cn_intl():
        try:
            return await fetch_cn_international_news.ainvoke({"keywords": keywords, "lookback_days": lookback_days})
        except Exception as e:
            logger.warning(f"âš ï¸ [èšåˆæ–°é—»] å›½å†…å›½é™…æ–°é—»ä»»åŠ¡å¤±è´¥: {e}")
            return ""
            
    # 5. å›½å†…æ”¿ç­–æ–°é—» (Sync -> Thread)
    async def task_policy():
        try:
            from tradingagents.tools.index_tools import fetch_policy_news
            return await asyncio.to_thread(fetch_policy_news.invoke, {"lookback_days": lookback_days})
        except Exception as e:
            logger.warning(f"âš ï¸ [èšåˆæ–°é—»] æ”¿ç­–æ–°é—»ä»»åŠ¡å¤±è´¥: {e}")
            return ""

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œè®¾ç½®æ€»ä½“è¶…æ—¶
    # æ³¨æ„ï¼šfetch_cn_international_news å†…éƒ¨å¯èƒ½æœ‰é™çº§é€»è¾‘
    
    tasks = [
        task_bloomberg(),
        task_reuters(),
        task_google(),
        task_cn_intl(),
        task_policy()
    ]
    
    # ä½¿ç”¨ gather å¹¶å‘æ‰§è¡Œ
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # å¤„ç†ç»“æœ
    final_output = []
    sources_found = 0
    
    source_names = ["Bloomberg", "Reuters", "Google News", "Domestic Intl News", "Policy News"]
    
    for i, res in enumerate(results):
        source_name = source_names[i]
        if isinstance(res, Exception):
            logger.error(f"âŒ [èšåˆæ–°é—»] {source_name} å¼‚å¸¸: {res}")
        elif res and isinstance(res, str) and "æš‚æ— ç›¸å…³æ–°é—»" not in res and "è·å–å¤±è´¥" not in res:
            final_output.append(res)
            sources_found += 1
        else:
            logger.debug(f"â„¹ï¸ [èšåˆæ–°é—»] {source_name} æ— æœ‰æ•ˆæ•°æ®")
            
    if not final_output:
        logger.warning("âš ï¸ [èšåˆæ–°é—»] æ‰€æœ‰æºå‡æœªè¿”å›æœ‰æ•ˆæ•°æ®")
        # å°è¯•æœ€åçš„å…œåº•ï¼šå¼ºåˆ¶è°ƒç”¨å›½å†…å…¨é‡æ–°é—»
        try:
            fallback = await fetch_cn_international_news.ainvoke({"keywords": "", "lookback_days": lookback_days})
            if fallback and "æš‚æ— ç›¸å…³æ–°é—»" not in fallback:
                 return f"## èšåˆæ–°é—»æŠ¥å‘Š (å…¨æºå¤±è´¥ï¼Œä½¿ç”¨å…œåº•)\n\n{fallback}"
        except:
            pass
        return f"## èšåˆæ–°é—»æŠ¥å‘Š\n\næœªæ‰¾åˆ°å…³äº '{keywords}' çš„ç›¸å…³æ–°é—» (å°è¯•äº†Bloomberg, Reuters, Google, å›½å†…æº, æ”¿ç­–æº)ã€‚"
        
    logger.info(f"âœ… [èšåˆæ–°é—»å·¥å…·] æˆåŠŸè·å– {sources_found} ä¸ªæºçš„æ•°æ®")
    return "## èšåˆæ–°é—»æŠ¥å‘Š\n\n" + "\n\n---\n\n".join(final_output)

# å·¥å…·åˆ—è¡¨å¯¼å‡º
INTERNATIONAL_NEWS_TOOLS = [
    fetch_aggregated_news, # é¦–é€‰èšåˆå·¥å…·
    fetch_bloomberg_news,
    fetch_reuters_news,
    fetch_google_news,
    fetch_cn_international_news
]
