#!/usr/bin/env python3
"""
æŒ‡æ•°æ•°æ®æä¾›è€…
æä¾›å®è§‚ç»æµã€æ”¿ç­–æ–°é—»ã€æ¿å—èµ„é‡‘æµå‘ç­‰æŒ‡æ•°åˆ†ææ‰€éœ€æ•°æ®

æ•°æ®æ¥æº:
- AKShare: å®è§‚ç»æµæ•°æ®ã€æ¿å—èµ„é‡‘æµã€æ–°é—»æ•°æ®
- MongoDB: æ•°æ®ç¼“å­˜
"""

from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import pandas as pd
import json

from tradingagents.utils.logging_manager import get_logger

logger = get_logger('agents')


class IndexDataProvider:
    """
    æŒ‡æ•°æ•°æ®æä¾›è€…
    
    æä¾›æŒ‡æ•°åˆ†ææ‰€éœ€çš„å„ç±»æ•°æ®:
    - å®è§‚ç»æµæ•°æ® (GDP, CPI, PMI, M2, LPRç­‰)
    - æ”¿ç­–æ–°é—»æ•°æ®
    - æ¿å—èµ„é‡‘æµå‘æ•°æ®
    
    æ‰€æœ‰æ•°æ®å‡æ”¯æŒMongoDBç¼“å­˜æœºåˆ¶
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æŒ‡æ•°æ•°æ®æä¾›è€…"""
        self.cache = self._get_cache()
        self.cache_ttl = {
            'macro': 86400,  # å®è§‚æ•°æ®24å°æ—¶
            'news': 21600,   # æ–°é—»6å°æ—¶
            'sector': 3600   # æ¿å—æ•°æ®1å°æ—¶
        }
        
        # åˆå§‹åŒ–AKShare
        self.ak = None
        self._init_akshare()
        
        logger.info("âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] åˆå§‹åŒ–å®Œæˆ")
    
    def _get_cache(self):
        """è·å–ç¼“å­˜å®ä¾‹"""
        try:
            from tradingagents.config.database_manager import get_mongodb_client
            client = get_mongodb_client()
            if client:
                db = client.get_database('tradingagents')
                logger.info("âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] MongoDBç¼“å­˜å·²è¿æ¥")
                return db
        except Exception as e:
            logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] MongoDBç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ç›´æ¥ä»APIè·å–æ•°æ®")
        return None
    
    def _init_akshare(self):
        """åˆå§‹åŒ–AKShare"""
        try:
            import akshare as ak
            self.ak = ak
            logger.info("âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] AKShareåˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] AKShareæœªå®‰è£…: {e}")
            self.ak = None
    
    def get_macro_economics_data(self, end_date: str = None) -> Dict[str, Any]:
        """
        è·å–å®è§‚ç»æµæ•°æ®
        
        Args:
            end_date: æŸ¥è¯¢æˆªæ­¢æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¥æœŸ
            
        Returns:
            Dict[str, Any]: åŒ…å«GDPã€CPIã€PMIã€M2ã€LPRç­‰æŒ‡æ ‡çš„å­—å…¸
        """
        logger.info(f"ğŸ“Š [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–å®è§‚ç»æµæ•°æ®, end_date={end_date}")
        
        if end_date is None:
            end_date = datetime.now().strftime("%Y-%m-%d")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"macro_data_{end_date}"
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.cache_ttl['macro']:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–å®è§‚æ•°æ®")
                        return cached_data.get("data", {})
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # 2. ä»AKShareè·å–æ•°æ®ï¼ˆå¢åŠ é‡è¯•æœºåˆ¶ï¼‰
        macro_data = {}
        
        # å¢åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        retry_delay = 1  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                # 2.1 è·å–GDPæ•°æ®ï¼ˆå­£åº¦ï¼‰
                try:
                    gdp_df = self.ak.macro_china_gdp()
                    if not gdp_df.empty:
                        # GDPæ•°æ®æŒ‰æ—¶é—´é™åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨æœ€å‰ï¼‰
                        latest_gdp = gdp_df.iloc[0]
                        macro_data['gdp'] = {
                            'quarter': str(latest_gdp.get('å­£åº¦', 'N/A')),
                            'value': float(latest_gdp.get('å›½å†…ç”Ÿäº§æ€»å€¼-ç»å¯¹å€¼', 0)),
                            'growth_rate': float(latest_gdp.get('å›½å†…ç”Ÿäº§æ€»å€¼-åŒæ¯”å¢é•¿', 0))
                        }
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] GDPæ•°æ®è·å–æˆåŠŸ: {macro_data['gdp']['quarter']}")
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] GDPæ•°æ®ä¸ºç©º")
                        macro_data['gdp'] = {'quarter': 'N/A', 'value': 0, 'growth_rate': 0}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] GDPæ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['gdp'] = {'quarter': 'N/A', 'value': 0, 'growth_rate': 0}
                
                # 2.2 è·å–CPIæ•°æ®ï¼ˆæœˆåº¦ï¼‰
                try:
                    # ä½¿ç”¨ macro_china_cpi_monthly è·å–æœˆåº¦CPIå¹´ç‡
                    cpi_df = self.ak.macro_china_cpi_monthly()
                    if not cpi_df.empty:
                        # è¿‡æ»¤æ‰ NaN å€¼
                        cpi_df = cpi_df.dropna(subset=['ä»Šå€¼'])
                        if not cpi_df.empty:
                            latest_cpi = cpi_df.iloc[-1] # æŒ‰æ—¶é—´å‡åº
                            macro_data['cpi'] = {
                                'month': str(latest_cpi.get('æ—¥æœŸ', 'N/A')),
                                'value': float(latest_cpi.get('ä»Šå€¼', 0)), # CPIå¹´ç‡
                                'year_on_year': float(latest_cpi.get('ä»Šå€¼', 0)) # è¿™é‡Œä»Šå€¼å°±æ˜¯åŒæ¯”
                            }
                            logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] CPIæ•°æ®è·å–æˆåŠŸ: {macro_data['cpi']['month']}")
                        else:
                            macro_data['cpi'] = {'month': 'N/A', 'value': 0, 'year_on_year': 0}
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] CPIæ•°æ®ä¸ºç©º")
                        macro_data['cpi'] = {'month': 'N/A', 'value': 0, 'year_on_year': 0}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] CPIæ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['cpi'] = {'month': 'N/A', 'value': 0, 'year_on_year': 0}
                
                # 2.3 è·å–PMIæ•°æ®ï¼ˆæœˆåº¦ï¼‰
                try:
                    pmi_df = self.ak.macro_china_pmi_yearly()
                    if not pmi_df.empty:
                        pmi_df = pmi_df.dropna(subset=['ä»Šå€¼'])
                        if not pmi_df.empty:
                            latest_pmi = pmi_df.iloc[-1]
                            macro_data['pmi'] = {
                                'month': str(latest_pmi.get('æ—¥æœŸ', 'N/A')),
                                'manufacturing': float(latest_pmi.get('ä»Šå€¼', 50)),
                                'non_manufacturing': 50.0 # æš‚æ—¶æ— æ³•è·å–éåˆ¶é€ ä¸šPMIï¼Œè®¾ä¸ºä¸­æ€§
                            }
                            logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] PMIæ•°æ®è·å–æˆåŠŸ: {macro_data['pmi']['month']}")
                        else:
                             macro_data['pmi'] = {'month': 'N/A', 'manufacturing': 50, 'non_manufacturing': 50}
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] PMIæ•°æ®ä¸ºç©º")
                        macro_data['pmi'] = {'month': 'N/A', 'manufacturing': 50, 'non_manufacturing': 50}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] PMIæ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['pmi'] = {'month': 'N/A', 'manufacturing': 50, 'non_manufacturing': 50}
                
                # 2.4 è·å–M2è´§å¸ä¾›åº”é‡ï¼ˆæœˆåº¦ï¼‰
                try:
                    m2_df = self.ak.macro_china_m2_yearly()
                    if not m2_df.empty:
                        m2_df = m2_df.dropna(subset=['ä»Šå€¼'])
                        if not m2_df.empty:
                            latest_m2 = m2_df.iloc[-1]
                            macro_data['m2'] = {
                                'month': str(latest_m2.get('æ—¥æœŸ', 'N/A')),
                                'value': 0, # M2ä½™é¢æš‚æ— æ³•ç›´æ¥è·å–
                                'growth_rate': float(latest_m2.get('ä»Šå€¼', 0)) # M2å¹´ç‡
                            }
                            logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] M2æ•°æ®è·å–æˆåŠŸ: {macro_data['m2']['month']}")
                        else:
                             macro_data['m2'] = {'month': 'N/A', 'value': 0, 'growth_rate': 0}
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] M2æ•°æ®ä¸ºç©º")
                        macro_data['m2'] = {'month': 'N/A', 'value': 0, 'growth_rate': 0}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] M2æ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['m2'] = {'month': 'N/A', 'value': 0, 'growth_rate': 0}
                
                # 2.5 è·å–LPRåˆ©ç‡ï¼ˆæœˆåº¦ï¼‰
                try:
                    # ä¿®å¤LPRæ•°æ®è·å–æ–¹æ³•ï¼Œä½¿ç”¨æ­£ç¡®çš„AKShareæ¥å£
                    lpr_df = self.ak.macro_china_lpr()
                    if not lpr_df.empty:
                        latest_lpr = lpr_df.iloc[-1]
                        macro_data['lpr'] = {
                            'date': str(latest_lpr.get('TRADE_DATE', 'N/A')),
                            'lpr_1y': float(latest_lpr.get('LPR1Y', 0)),
                            'lpr_5y': float(latest_lpr.get('LPR5Y', 0))
                        }
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] LPRæ•°æ®è·å–æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] LPRæ•°æ®ä¸ºç©º")
                        macro_data['lpr'] = {'date': 'N/A', 'lpr_1y': 0, 'lpr_5y': 0}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] LPRæ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['lpr'] = {'date': 'N/A', 'lpr_1y': 0, 'lpr_5y': 0}
                
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] å®è§‚æ•°æ®è·å–å®Œæˆï¼Œå…±{len(macro_data)}ä¸ªæŒ‡æ ‡")
                break  # æˆåŠŸè·å–æ•°æ®ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] å®è§‚æ•°æ®è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    import time
                    time.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] å®è§‚æ•°æ®è·å–å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
        
        # 3. ç¼“å­˜æ•°æ®
        if self.cache is not None and macro_data:
            try:
                cache_doc = {
                    "cache_key": cache_key,
                    "data": macro_data,
                    "created_at": datetime.utcnow(),
                    "end_date": end_date
                }
                self.cache.index_cache.update_one(
                    {"cache_key": cache_key},
                    {"$set": cache_doc},
                    upsert=True
                )
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] å®è§‚æ•°æ®å·²ç¼“å­˜")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        return macro_data

    def get_market_funds_flow(self) -> Dict[str, Any]:
        """è·å–å¸‚åœºèµ„é‡‘æµå‘ï¼ˆåŒ—å‘èµ„é‡‘ã€ä¸¤èä½™é¢ç­‰ï¼‰"""
        # æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…åº”è°ƒç”¨ ak.stock_hsgt_hist_em ç­‰æ¥å£
        return {
            "northbound": {"net_inflow": 15.2, "trend": "inflow"},
            "southbound": {"net_inflow": 5.8, "trend": "inflow"},
            "margin_balance": {"value": 16500, "change": 120},
            "turnover_rate": 2.5
        }

    def get_index_valuation(self, index_code: str) -> Dict[str, Any]:
        """
        è·å–æŒ‡æ•°ä¼°å€¼æ•°æ®
        
        Args:
            index_code: æŒ‡æ•°ä»£ç ï¼Œå¦‚ "sh000300"
            
        Returns:
            Dict: åŒ…å«PE, PB, è‚¡æ¯ç‡ç­‰
        """
        import re
        # æå–æ•°å­—ä»£ç 
        code_match = re.search(r'\d{6}', index_code)
        if not code_match:
            logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ— æ³•ä» {index_code} æå–æ•°å­—ä»£ç ")
            return {}
            
        pure_code = code_match.group(0)
        
        try:
            # ä½¿ç”¨ ak.stock_zh_index_value_csindex è·å–ä¸­è¯æŒ‡æ•°ä¼°å€¼
            # æ³¨æ„ï¼šæ­¤æ¥å£ä¸»è¦æ”¯æŒä¸­è¯ç³»åˆ—æŒ‡æ•°
            df = self.ak.stock_zh_index_value_csindex(symbol=pure_code)
            
            if not df.empty:
                latest = df.iloc[0] # æŒ‰æ—¥æœŸé™åºï¼Œå–æœ€æ–°çš„
                
                valuation_data = {
                    "pe": float(latest.get('å¸‚ç›ˆç‡1', 0)), # é™æ€PE
                    "pe_ttm": float(latest.get('å¸‚ç›ˆç‡2', 0)), # æ»šåŠ¨PE
                    "dividend_yield": float(latest.get('è‚¡æ¯ç‡1', 0)),
                    "date": str(latest.get('æ—¥æœŸ', '')),
                    "evaluation": "N/A" # æš‚æ— æ³•è‡ªåŠ¨åˆ¤æ–­é«˜ä¼°ä½ä¼°
                }
                
                # å°è¯•è®¡ç®—PBï¼ˆå¦‚æœæ¥å£ä¸æä¾›ï¼Œè¿™é‡Œå¯èƒ½ç¼ºå¤±ï¼‰
                valuation_data["pb"] = 0
                
                return valuation_data
                
            return {}
        except Exception as e:
            logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–æŒ‡æ•°ä¼°å€¼å¤±è´¥: {e}")
            # é™çº§è¿”å›ç©ºå­—å…¸
            return {}

    def get_index_technicals(self, index_code: str) -> Dict[str, Any]:
        """è·å–æŒ‡æ•°æŠ€æœ¯æŒ‡æ ‡"""
        # æ¨¡æ‹Ÿæ•°æ®
        return {
            "ma": {"ma5": 3050, "ma20": 3020, "ma60": 2980, "trend": "bullish"},
            "macd": {"dif": 15.5, "dea": 10.2, "macd": 5.3, "signal": "golden_cross"},
            "rsi": {"rsi6": 65, "rsi12": 58, "rsi24": 52},
            "kdj": {"k": 75, "d": 70, "j": 80},
            "volume": {"trend": "increasing"}
        }

    def get_index_constituents(self, index_code: str) -> List[Dict[str, Any]]:
        """
        è·å–æŒ‡æ•°æˆåˆ†è‚¡
        
        Args:
            index_code: æŒ‡æ•°ä»£ç ï¼Œå¦‚ "sh000300"
        """
        import re
        code_match = re.search(r'\d{6}', index_code)
        if not code_match:
            return []
            
        pure_code = code_match.group(0)
        
        try:
            df = self.ak.index_stock_cons(symbol=pure_code)
            if not df.empty:
                constituents = []
                for _, row in df.head(10).iterrows(): # åªå–å‰10å¤§æƒé‡è‚¡
                    # index_stock_cons è¿”å›: å“ç§ä»£ç , å“ç§åç§°, çº³å…¥æ—¥æœŸ
                    constituents.append({
                        "code": row.get('å“ç§ä»£ç '),
                        "name": row.get('å“ç§åç§°'),
                        "weight": 0, # æš‚æ— æƒé‡æ•°æ®
                        "price": 0, # æš‚æ— ä»·æ ¼æ•°æ®
                        "change_pct": 0
                    })
                return constituents
            return []
        except Exception as e:
            logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–æˆåˆ†è‚¡å¤±è´¥: {e}")
            return []
    
    def get_policy_news(self, lookback_days: int = 7) -> List[Dict[str, Any]]:
        """
        è·å–æ”¿ç­–æ–°é—»
        
        Args:
            lookback_days: å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©
            
        Returns:
            List[Dict[str, Any]]: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«æ ‡é¢˜ã€å†…å®¹ã€æ—¶é—´ç­‰
        """
        logger.info(f"ğŸ“° [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–æ”¿ç­–æ–°é—», lookback_days={lookback_days}")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"policy_news_{lookback_days}"
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.cache_ttl['news']:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–æ”¿ç­–æ–°é—»")
                        return cached_data.get("data", [])
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # 2. ä»AKShareè·å–æ–°é—»
        news_list = []
        
        try:
            # 2.1 å°è¯•è·å–æ–°é—»è”æ’­æ–‡å­—ç¨¿ï¼ˆä¸»è¦æ•°æ®æºï¼‰
            try:
                news_df = self.ak.news_cctv()
                if not news_df.empty:
                    # åªå–æœ€è¿‘lookback_dayså¤©çš„æ–°é—»
                    news_df['date'] = pd.to_datetime(news_df['date'])
                    cutoff_date = datetime.now() - timedelta(days=lookback_days)
                    recent_news = news_df[news_df['date'] >= cutoff_date]
                    
                    for _, row in recent_news.iterrows():
                        news_list.append({
                            'title': row.get('title', ''),
                            'content': row.get('content', ''),
                            'date': row.get('date').strftime('%Y-%m-%d') if pd.notna(row.get('date')) else '',
                            'source': 'æ–°é—»è”æ’­'
                        })
                    
                    logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] æ–°é—»è”æ’­æ•°æ®è·å–æˆåŠŸï¼Œå…±{len(news_list)}æ¡")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ–°é—»è”æ’­æ•°æ®è·å–å¤±è´¥: {e}")
            
            # 2.2 é™çº§æ–¹æ¡ˆï¼šè·å–ç™¾åº¦è´¢ç»æ–°é—»
            if len(news_list) == 0:
                try:
                    logger.info(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šç™¾åº¦è´¢ç»æ–°é—»")
                    news_df = self.ak.stock_news_em(symbol="å®è§‚ç»æµ")
                    if not news_df.empty:
                        for _, row in news_df.head(10).iterrows():
                            news_list.append({
                                'title': row.get('æ–°é—»æ ‡é¢˜', ''),
                                'content': row.get('æ–°é—»å†…å®¹', ''),
                                'date': row.get('å‘å¸ƒæ—¶é—´', ''),
                                'source': 'ä¸œæ–¹è´¢å¯Œ'
                            })
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ç™¾åº¦è´¢ç»æ–°é—»è·å–æˆåŠŸï¼Œå…±{len(news_list)}æ¡")
                except Exception as e2:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç™¾åº¦è´¢ç»æ–°é—»è·å–å¤±è´¥: {e2}")
            
        except Exception as e:
            logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ”¿ç­–æ–°é—»è·å–å¤±è´¥: {e}")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if len(news_list) == 0:
            logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æœªè·å–åˆ°æ”¿ç­–æ–°é—»ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            news_list = [{
                'title': 'æš‚æ— æ”¿ç­–æ–°é—»',
                'content': 'å½“å‰æ— æ³•è·å–æ”¿ç­–æ–°é—»æ•°æ®ï¼Œè¯·ç¨åé‡è¯•',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'ç³»ç»Ÿæç¤º'
            }]
        
        # 3. ç¼“å­˜æ•°æ®
        if self.cache is not None and news_list:
            try:
                cache_doc = {
                    "cache_key": cache_key,
                    "data": news_list,
                    "created_at": datetime.utcnow(),
                    "lookback_days": lookback_days
                }
                self.cache.index_cache.update_one(
                    {"cache_key": cache_key},
                    {"$set": cache_doc},
                    upsert=True
                )
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] æ”¿ç­–æ–°é—»å·²ç¼“å­˜")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        return news_list

    def get_multi_source_news(self, keywords: str = None, lookback_days: int = 1) -> List[Dict[str, Any]]:
        """
        è·å–å¤šæºèšåˆæ–°é—» (7x24å°æ—¶å¿«è®¯)
        
        æ•´åˆæ¥æº:
        1. è´¢è”ç¤¾ (CLS)
        2. æ–°æµªè´¢ç» (Sina)
        3. åŒèŠ±é¡º (THS)
        4. å¯Œé€”ç‰›ç‰› (Futu)
        
        Args:
            keywords: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼‰
            lookback_days: å›æº¯å¤©æ•° (é»˜è®¤1å¤©ï¼Œå› ä¸ºæ˜¯å¿«è®¯)
            
        Returns:
            List[Dict[str, Any]]: èšåˆåçš„æ–°é—»åˆ—è¡¨
        """
        logger.info(f"ğŸŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–å¤šæºèšåˆæ–°é—», keywords={keywords}")
        
        # 1. æ£€æŸ¥ç¼“å­˜ (ç¼“å­˜é”®åŒ…å«å…³é”®è¯ï¼Œå› ä¸ºè¿‡æ»¤æ˜¯åœ¨è·å–åè¿›è¡Œçš„ï¼Œä½†ä¸ºäº†æ•ˆç‡æˆ‘ä»¬ç¼“å­˜åŸå§‹èšåˆæ•°æ®)
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬åªç¼“å­˜å…¨é‡æ•°æ®ï¼Œç„¶åè¿‡æ»¤
        cache_key = f"multi_source_news_raw_{datetime.now().strftime('%Y%m%d_%H')}" # æŒ‰å°æ—¶ç¼“å­˜
        
        all_news = []
        cached = False
        
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ (30åˆ†é’Ÿè¿‡æœŸï¼Œå› ä¸ºæ˜¯å¿«è®¯)
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < 1800:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–å¤šæºæ–°é—»")
                        all_news = cached_data.get("data", [])
                        cached = True
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")

        if not cached:
            # 2. ä»å„ä¸ªæºè·å–æ•°æ®
            
            # 2.1 è´¢è”ç¤¾
            try:
                if hasattr(self.ak, 'stock_info_global_cls'):
                    df = self.ak.stock_info_global_cls()
                    if not df.empty:
                        for _, row in df.iterrows():
                            # æ ¼å¼: æ ‡é¢˜, å†…å®¹, å‘å¸ƒæ—¥æœŸ, å‘å¸ƒæ—¶é—´
                            pub_time = f"{row.get('å‘å¸ƒæ—¥æœŸ', '')} {row.get('å‘å¸ƒæ—¶é—´', '')}".strip()
                            all_news.append({
                                'title': row.get('æ ‡é¢˜', '')[:50] + '...' if row.get('æ ‡é¢˜') else 'æ— æ ‡é¢˜', # CLSæ ‡é¢˜å¾€å¾€å°±æ˜¯å†…å®¹
                                'content': row.get('æ ‡é¢˜', ''), # CLSå†…å®¹åœ¨æ ‡é¢˜é‡Œï¼Œæˆ–è€…æœ‰contentåˆ—
                                'date': pub_time,
                                'source': 'è´¢è”ç¤¾',
                                'url': ''
                            })
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–è´¢è”ç¤¾æ–°é—»: {len(df)}æ¡")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] è´¢è”ç¤¾æ–°é—»è·å–å¤±è´¥: {e}")

            # 2.2 æ–°æµªè´¢ç»
            try:
                if hasattr(self.ak, 'stock_info_global_sina'):
                    df = self.ak.stock_info_global_sina()
                    if not df.empty:
                        for _, row in df.iterrows():
                            # æ ¼å¼: æ—¶é—´, å†…å®¹
                            # æ–°æµªè¿”å›çš„æ—¶é—´é€šå¸¸æ˜¯ HH:MM:SSï¼Œéœ€è¦åŠ ä¸Šæ—¥æœŸ
                            time_str = row.get('æ—¶é—´', '')
                            if len(time_str) < 12: # åªæœ‰æ—¶é—´
                                time_str = f"{datetime.now().strftime('%Y-%m-%d')} {time_str}"
                                
                            all_news.append({
                                'title': row.get('å†…å®¹', '')[:30] + '...',
                                'content': row.get('å†…å®¹', ''),
                                'date': time_str,
                                'source': 'æ–°æµªè´¢ç»',
                                'url': ''
                            })
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–æ–°æµªè´¢ç»æ–°é—»: {len(df)}æ¡")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ–°æµªè´¢ç»æ–°é—»è·å–å¤±è´¥: {e}")

            # 2.3 åŒèŠ±é¡º
            try:
                if hasattr(self.ak, 'stock_info_global_ths'):
                    df = self.ak.stock_info_global_ths()
                    if not df.empty:
                        for _, row in df.iterrows():
                            title = row.get('æ ‡é¢˜', '')
                            content = row.get('å†…å®¹', '')
                            if not title and content:
                                title = content[:30] + '...'
                            
                            all_news.append({
                                'title': title,
                                'content': content,
                                'date': row.get('å‘å¸ƒæ—¶é—´', '') if 'å‘å¸ƒæ—¶é—´' in row else datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'source': 'åŒèŠ±é¡º',
                                'url': row.get('é“¾æ¥', '')
                            })
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–åŒèŠ±é¡ºæ–°é—»: {len(df)}æ¡")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] åŒèŠ±é¡ºæ–°é—»è·å–å¤±è´¥: {e}")
                
            # 2.4 å¯Œé€”ç‰›ç‰›
            try:
                if hasattr(self.ak, 'stock_info_global_futu'):
                    df = self.ak.stock_info_global_futu()
                    if not df.empty:
                        for _, row in df.iterrows():
                            title = row.get('æ ‡é¢˜', '')
                            content = row.get('å†…å®¹', '')
                            if not title and content:
                                title = content[:30] + '...'
                                
                            all_news.append({
                                'title': title,
                                'content': content,
                                'date': row.get('å‘å¸ƒæ—¶é—´', '') if 'å‘å¸ƒæ—¶é—´' in row else datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'source': 'å¯Œé€”ç‰›ç‰›',
                                'url': row.get('é“¾æ¥', '')
                            })
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–å¯Œé€”ç‰›ç‰›æ–°é—»: {len(df)}æ¡")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] å¯Œé€”ç‰›ç‰›æ–°é—»è·å–å¤±è´¥: {e}")

            # 3. ç¼“å­˜æ•°æ®
            if self.cache is not None and all_news:
                try:
                    cache_doc = {
                        "cache_key": cache_key,
                        "data": all_news,
                        "created_at": datetime.utcnow()
                    }
                    self.cache.index_cache.update_one(
                        {"cache_key": cache_key},
                        {"$set": cache_doc},
                        upsert=True
                    )
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

        # 4. è¿‡æ»¤å’Œæ’åº
        filtered_news = []
        
        # æ—¥æœŸè¿‡æ»¤
        cutoff_date = datetime.now() - timedelta(days=lookback_days)
        
        for news in all_news:
            # ç®€å•çš„å…³é”®è¯è¿‡æ»¤
            if keywords:
                if keywords.lower() not in news['title'].lower() and keywords.lower() not in news['content'].lower():
                    continue
            
            filtered_news.append(news)
            
        # æŒ‰æ—¶é—´å€’åºæ’åº (å°è¯•è§£ææ—¥æœŸï¼Œè§£æå¤±è´¥åˆ™æ”¾åˆ°æœ€å)
        def parse_date(x):
            try:
                return pd.to_datetime(x['date'])
            except:
                return datetime.min
                
        filtered_news.sort(key=parse_date, reverse=True)
        
        return filtered_news

    def get_sector_news(self, sector_name: str, lookback_days: int = 7) -> List[Dict[str, Any]]:
        """
        è·å–ç‰¹å®šæ¿å—/æ¦‚å¿µçš„æ–°é—»
        
        Args:
            sector_name: æ¿å—æˆ–æ¦‚å¿µåç§° (å¦‚: "åŠå¯¼ä½“", "åŒ»è¯", "æ–°èƒ½æº")
            lookback_days: å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©
            
        Returns:
            List[Dict[str, Any]]: æ–°é—»åˆ—è¡¨
        """
        logger.info(f"ğŸ­ [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–æ¿å—æ–°é—», sector={sector_name}, lookback_days={lookback_days}")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"sector_news_{sector_name}_{lookback_days}"
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.cache_ttl['sector']:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–{sector_name}æ¿å—æ–°é—»")
                        return cached_data.get("data", [])
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # 2. ä»AKShareè·å–æ•°æ®
        news_list = []
        try:
            # ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æ¥å£ï¼Œç›´æ¥ä¼ å…¥æ¿å—åç§°
            news_df = self.ak.stock_news_em(symbol=sector_name)
            
            if not news_df.empty:
                count = 0
                cutoff_date = datetime.now() - timedelta(days=lookback_days)
                
                for _, row in news_df.iterrows():
                    # ç®€å•å¤„ç†æ—¥æœŸï¼Œå‡è®¾æ˜¯æœ€è¿‘çš„
                    # ä¸œæ–¹è´¢å¯Œè¿”å›æ ¼å¼é€šå¸¸ä¸º 'YYYY-MM-DD HH:MM:SS'
                    pub_time_str = row.get('å‘å¸ƒæ—¶é—´', '')
                    try:
                        pub_time = pd.to_datetime(pub_time_str)
                        if pub_time < cutoff_date:
                            continue
                    except:
                        pass # è§£æå¤±è´¥åˆ™ä¿ç•™ï¼Œå‡è®¾æ˜¯æœ€è¿‘çš„
                        
                    news_item = {
                        'title': row.get('æ–°é—»æ ‡é¢˜', ''),
                        'content': row.get('æ–°é—»å†…å®¹', ''),
                        'date': pub_time_str,
                        'source': f'ä¸œæ–¹è´¢å¯Œ-{sector_name}',
                        'url': row.get('æ–°é—»é“¾æ¥', '')
                    }
                    news_list.append(news_item)
                    count += 1
                    if count >= 20: # é™åˆ¶æ¡æ•°
                        break
                
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–{sector_name}æ¿å—æ–°é—»æˆåŠŸ: {count}æ¡")
            else:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æœªæ‰¾åˆ°{sector_name}æ¿å—ç›¸å…³æ–°é—»")
                
        except Exception as e:
            logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ–°é—»è·å–å¤±è´¥: {e}")
            
        # 3. ç¼“å­˜æ•°æ®
        if self.cache is not None and news_list:
            try:
                cache_doc = {
                    "cache_key": cache_key,
                    "data": news_list,
                    "created_at": datetime.utcnow()
                }
                self.cache.index_cache.update_one(
                    {"cache_key": cache_key},
                    {"$set": cache_doc},
                    upsert=True
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
                
        return news_list

    def get_international_news(self, keywords: str = None, lookback_days: int = 7) -> List[Dict[str, Any]]:
        """
        è·å–å›½é™…æ–°é—»ï¼ˆå›½å†…æºï¼‰
        
        Args:
            keywords: æœç´¢å…³é”®è¯ï¼ˆå¯é€‰ï¼Œç”¨äºè¿‡æ»¤ï¼‰
            lookback_days: å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©
            
        Returns:
            List[Dict[str, Any]]: æ–°é—»åˆ—è¡¨
        """
        logger.info(f"ğŸŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–å›½é™…æ–°é—»(å›½å†…æº), keywords={keywords}, lookback_days={lookback_days}")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"intl_news_cn_{lookback_days}"
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.cache_ttl['news']:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–å›½é™…æ–°é—»")
                        # å¦‚æœæœ‰å…³é”®è¯ï¼Œè¿›è¡Œè¿‡æ»¤
                        news_data = cached_data.get("data", [])
                        if keywords:
                            filtered_news = [
                                n for n in news_data 
                                if keywords.lower() in n['title'].lower() or keywords.lower() in n['content'].lower()
                            ]
                            return filtered_news
                        return news_data
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # 2. ä»AKShareè·å–æ–°é—»
        news_list = []
        
        try:
            # å¦‚æœæä¾›äº†å…³é”®è¯ï¼Œä¼˜å…ˆå°è¯•ç›´æ¥ç”¨å…³é”®è¯æœç´¢
            if keywords:
                try:
                    logger.info(f"ğŸ” [æŒ‡æ•°æ•°æ®æä¾›è€…] å°è¯•ä½¿ç”¨å…³é”®è¯ç›´æ¥æœç´¢: {keywords}")
                    # å¤„ç†å¤šå…³é”®è¯æƒ…å†µï¼Œå–ç¬¬ä¸€ä¸ªæˆ–ä¸»è¦å…³é”®è¯
                    # å‡è®¾keywordså¯èƒ½åŒ…å«å¤šä¸ªè¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”
                    search_key = keywords.split()[0] if ' ' in keywords else keywords
                    
                    news_df = self.ak.stock_news_em(symbol=search_key)
                    if not news_df.empty:
                        count = 0
                        for _, row in news_df.iterrows():
                            pub_time = row.get('å‘å¸ƒæ—¶é—´', '')
                            news_item = {
                                'title': row.get('æ–°é—»æ ‡é¢˜', ''),
                                'content': row.get('æ–°é—»å†…å®¹', ''),
                                'date': pub_time,
                                'source': f'ä¸œæ–¹è´¢å¯Œ-{search_key}',
                                'url': row.get('æ–°é—»é“¾æ¥', '')
                            }
                            news_list.append(news_item)
                            count += 1
                            if count >= 20: break
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] å…³é”®è¯æœç´¢æˆåŠŸ: {count}æ¡")
                        # å¦‚æœç›´æ¥æœç´¢æˆåŠŸï¼Œç›´æ¥è¿”å›ï¼ˆéœ€æ’åºï¼‰
                        return news_list
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] å…³é”®è¯æœç´¢å¤±è´¥: {e}ï¼Œå°†å°è¯•é€šç”¨æº")

            sources = ["ç¾è‚¡", "å…¨çƒ"]
            for source in sources:
                try:
                    news_df = self.ak.stock_news_em(symbol=source)
                    if not news_df.empty:
                        # è½¬æ¢æ—¥æœŸæ ¼å¼å¹¶è¿‡æ»¤
                        # æ³¨æ„ï¼šä¸œæ–¹è´¢å¯Œè¿”å›çš„å‘å¸ƒæ—¶é—´æ ¼å¼å¯èƒ½ä¸ç»Ÿä¸€ï¼Œè¿™é‡Œåšç®€å•å¤„ç†
                        # é€šå¸¸æ˜¯ 'YYYY-MM-DD HH:MM:SS'
                        
                        count = 0
                        for _, row in news_df.iterrows():
                            pub_time = row.get('å‘å¸ƒæ—¶é—´', '')
                            # ç®€å•æ—¥æœŸè¿‡æ»¤é€»è¾‘ï¼šå‡è®¾è¿”å›çš„æ˜¯æœ€è¿‘çš„æ–°é—»
                            # å¦‚æœéœ€è¦ä¸¥æ ¼è¿‡æ»¤ï¼Œéœ€è¦è§£ææ—¥æœŸå­—ç¬¦ä¸²
                            
                            news_item = {
                                'title': row.get('æ–°é—»æ ‡é¢˜', ''),
                                'content': row.get('æ–°é—»å†…å®¹', ''),
                                'date': pub_time,
                                'source': f'ä¸œæ–¹è´¢å¯Œ-{source}',
                                'url': row.get('æ–°é—»é“¾æ¥', '')
                            }
                            
                            news_list.append(news_item)
                            count += 1
                            if count >= 20: # æ¯ä¸ªæºé™åˆ¶20æ¡
                                break
                                
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–{source}æ–°é—»æˆåŠŸ: {count}æ¡")
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–{source}æ–°é—»å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] å›½é™…æ–°é—»è·å–å¤±è´¥: {e}")
            
        # 3. ç¼“å­˜æ•°æ® (ç¼“å­˜å…¨é‡æ•°æ®ï¼Œä¸è¿‡æ»¤å…³é”®è¯)
        if self.cache is not None and news_list:
            try:
                cache_doc = {
                    "cache_key": cache_key,
                    "data": news_list,
                    "created_at": datetime.utcnow()
                }
                self.cache.index_cache.update_one(
                    {"cache_key": cache_key},
                    {"$set": cache_doc},
                    upsert=True
                )
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        # 4. å…³é”®è¯è¿‡æ»¤
        if keywords:
            filtered_news = [
                n for n in news_list 
                if keywords.lower() in n['title'].lower() or keywords.lower() in n['content'].lower()
            ]
            return filtered_news
            
        return news_list
    
    def get_sector_flows(self, trade_date: str = None) -> Dict[str, Any]:
        """
        è·å–æ¿å—èµ„é‡‘æµå‘
        
        Args:
            trade_date: äº¤æ˜“æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºæœ€æ–°äº¤æ˜“æ—¥
            
        Returns:
            Dict[str, Any]: æ¿å—èµ„é‡‘æµå‘æ•°æ®ï¼ŒåŒ…å«topæ¶¨å¹…æ¿å—å’Œèµ„é‡‘æµå…¥æ•°æ®
        """
        logger.info(f"ğŸ’° [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–æ¿å—èµ„é‡‘æµå‘, trade_date={trade_date}")
        
        if trade_date is None:
            trade_date = datetime.now().strftime("%Y-%m-%d")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"sector_flows_{trade_date}"
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.cache_ttl['sector']:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–æ¿å—æ•°æ®")
                        return cached_data.get("data", {})
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # 2. ä»AKShareè·å–æ¿å—æ•°æ®ï¼ˆå¢åŠ é‡è¯•æœºåˆ¶ï¼‰
        sector_data = {
            'top_sectors': [],
            'bottom_sectors': [],
            'all_sectors': []
        }
        
        max_retries = 3
        retry_delay = 1  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                # è·å–ä¸œæ–¹è´¢å¯Œæ¿å—èµ„é‡‘æµæ•°æ®
                sector_df = self.ak.stock_board_industry_name_em()
                
                if not sector_df.empty:
                    # è·å–æ¶¨è·Œå¹…æ•°æ®
                    sector_flow_df = self.ak.stock_board_industry_summary_ths()
                    
                    if not sector_flow_df.empty:
                        # æŒ‰æ¶¨è·Œå¹…æ’åº
                        sector_flow_df = sector_flow_df.sort_values('æ¶¨è·Œå¹…', ascending=False)
                        
                        # Top 5 é¢†æ¶¨æ¿å—
                        for _, row in sector_flow_df.head(5).iterrows():
                            sector_data['top_sectors'].append({
                                'name': row.get('æ¿å—', ''),
                                'change_pct': float(row.get('æ¶¨è·Œå¹…', 0)),
                                'net_inflow': float(row.get('æµå…¥èµ„é‡‘', 0)) if 'æµå…¥èµ„é‡‘' in row else 0,
                                'turnover_rate': float(row.get('æ¢æ‰‹ç‡', 0)) if 'æ¢æ‰‹ç‡' in row else 0
                            })
                        
                        # Bottom 5 é¢†è·Œæ¿å—
                        for _, row in sector_flow_df.tail(5).iterrows():
                            sector_data['bottom_sectors'].append({
                                'name': row.get('æ¿å—', ''),
                                'change_pct': float(row.get('æ¶¨è·Œå¹…', 0)),
                                'net_inflow': float(row.get('æµå…¥èµ„é‡‘', 0)) if 'æµå…¥èµ„é‡‘' in row else 0,
                                'turnover_rate': float(row.get('æ¢æ‰‹ç‡', 0)) if 'æ¢æ‰‹ç‡' in row else 0
                            })
                        
                        # æ‰€æœ‰æ¿å—æ¦‚å†µ
                        for _, row in sector_flow_df.iterrows():
                            sector_data['all_sectors'].append({
                                'name': row.get('æ¿å—', ''),
                                'change_pct': float(row.get('æ¶¨è·Œå¹…', 0))
                            })
                        
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ•°æ®è·å–æˆåŠŸï¼Œå…±{len(sector_flow_df)}ä¸ªæ¿å—")
                        break  # æˆåŠŸè·å–æ•°æ®ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æµå‘æ•°æ®ä¸ºç©º")
                else:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—åç§°æ•°æ®ä¸ºç©º")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ•°æ®è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    import time
                    time.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ•°æ®è·å–å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
        
        # 3. ç¼“å­˜æ•°æ®
        if self.cache is not None and sector_data:
            try:
                cache_doc = {
                    "cache_key": cache_key,
                    "data": sector_data,
                    "created_at": datetime.utcnow(),
                    "trade_date": trade_date
                }
                self.cache.index_cache.update_one(
                    {"cache_key": cache_key},
                    {"$set": cache_doc},
                    upsert=True
                )
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ•°æ®å·²ç¼“å­˜")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        return sector_data
    
    def get_index_daily(self, ts_code: str, start_date: str = None, end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡æ•°æ—¥çº¿æ•°æ® (åŸºç±»é»˜è®¤å®ç°è¿”å›Noneï¼Œç”±å­ç±»å®ç°)
        """
        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] get_index_daily æœªå®ç°ï¼Œè¯·ä½¿ç”¨ HybridIndexDataProvider")
        return None


# å…¨å±€å®ä¾‹
_index_data_provider = None


def get_index_data_provider() -> IndexDataProvider:
    """è·å–å…¨å±€æŒ‡æ•°æ•°æ®æä¾›è€…å®ä¾‹ (ä¼˜å…ˆè¿”å› HybridIndexDataProvider)"""
    global _index_data_provider
    if _index_data_provider is None:
        try:
            from tradingagents.dataflows.hybrid_provider import HybridIndexDataProvider
            _index_data_provider = HybridIndexDataProvider()
            logger.info("âœ… å·²åˆå§‹åŒ– HybridIndexDataProvider")
        except ImportError:
            logger.warning("âš ï¸ HybridIndexDataProvider å¯¼å…¥å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€ IndexDataProvider")
            _index_data_provider = IndexDataProvider()
    return _index_data_provider
