#!/usr/bin/env python3
"""
æŒ‡æ•°æ•°æ®æä¾›è€…
æä¾›å®è§‚ç»æµã€æ”¿ç­–æ–°é—»ã€æ¿å—èµ„é‡‘æµå‘ç­‰æŒ‡æ•°åˆ†ææ‰€éœ€æ•°æ®

æ•°æ®æ¥æº:
- AKShare: å®è§‚ç»æµæ•°æ®ã€æ¿å—èµ„é‡‘æµã€æ–°é—»æ•°æ®
- MongoDB: æ•°æ®ç¼“å­˜
"""

from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import pandas as pd
import json

from tradingagents.utils.logging_manager import get_logger

logger = get_logger('agents')


class IndexDataProvider:
    """
    æŒ‡æ•°æ•°æ®æä¾›è€…
    
    æä¾›æŒ‡æ•°åˆ†ææ‰€éœ€çš„å„ç±»æ•°æ®:
    - å®è§‚ç»æµæ•°æ® (GDP, CPI, PMI, M2, LPRç­‰)
    - æ”¿ç­–æ–°é—»æ•°æ®
    - æ¿å—èµ„é‡‘æµå‘æ•°æ®
    
    æ‰€æœ‰æ•°æ®å‡æ”¯æŒMongoDBç¼“å­˜æœºåˆ¶
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æŒ‡æ•°æ•°æ®æä¾›è€…"""
        self.cache = self._get_cache()
        self.cache_ttl = {
            'macro': 86400,  # å®è§‚æ•°æ®24å°æ—¶
            'news': 21600,   # æ–°é—»6å°æ—¶
            'sector': 3600   # æ¿å—æ•°æ®1å°æ—¶
        }
        
        # åˆå§‹åŒ–AKShare
        self.ak = None
        self._init_akshare()
        
        logger.info("âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] åˆå§‹åŒ–å®Œæˆ")
    
    def _get_cache(self):
        """è·å–ç¼“å­˜å®ä¾‹"""
        try:
            from tradingagents.config.database_manager import get_mongodb_client
            client = get_mongodb_client()
            if client:
                db = client.get_database('tradingagents')
                logger.info("âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] MongoDBç¼“å­˜å·²è¿æ¥")
                return db
        except Exception as e:
            logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] MongoDBç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ç›´æ¥ä»APIè·å–æ•°æ®")
        return None
    
    def _init_akshare(self):
        """åˆå§‹åŒ–AKShare"""
        try:
            import akshare as ak
            self.ak = ak
            logger.info("âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] AKShareåˆå§‹åŒ–æˆåŠŸ")
        except ImportError as e:
            logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] AKShareæœªå®‰è£…: {e}")
            self.ak = None
    
    def get_macro_economics_data(self, end_date: str = None) -> Dict[str, Any]:
        """
        è·å–å®è§‚ç»æµæ•°æ®
        
        Args:
            end_date: æŸ¥è¯¢æˆªæ­¢æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¥æœŸ
            
        Returns:
            Dict[str, Any]: åŒ…å«GDPã€CPIã€PMIã€M2ã€LPRç­‰æŒ‡æ ‡çš„å­—å…¸
        """
        logger.info(f"ğŸ“Š [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–å®è§‚ç»æµæ•°æ®, end_date={end_date}")
        
        if end_date is None:
            end_date = datetime.now().strftime("%Y-%m-%d")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"macro_data_{end_date}"
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.cache_ttl['macro']:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–å®è§‚æ•°æ®")
                        return cached_data.get("data", {})
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # 2. ä»AKShareè·å–æ•°æ®ï¼ˆå¢åŠ é‡è¯•æœºåˆ¶ï¼‰
        macro_data = {}
        
        # å¢åŠ é‡è¯•æœºåˆ¶
        max_retries = 3
        retry_delay = 1  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                # 2.1 è·å–GDPæ•°æ®ï¼ˆå­£åº¦ï¼‰
                try:
                    gdp_df = self.ak.macro_china_gdp()
                    if not gdp_df.empty:
                        latest_gdp = gdp_df.iloc[-1]
                        macro_data['gdp'] = {
                            'quarter': str(latest_gdp.get('å­£åº¦', 'N/A')),
                            'value': float(latest_gdp.get('å›½å†…ç”Ÿäº§æ€»å€¼-ç»å¯¹å€¼', 0)),
                            'growth_rate': float(latest_gdp.get('å›½å†…ç”Ÿäº§æ€»å€¼-åŒæ¯”å¢é•¿', 0))
                        }
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] GDPæ•°æ®è·å–æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] GDPæ•°æ®ä¸ºç©º")
                        macro_data['gdp'] = {'quarter': 'N/A', 'value': 0, 'growth_rate': 0}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] GDPæ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['gdp'] = {'quarter': 'N/A', 'value': 0, 'growth_rate': 0}
                
                # 2.2 è·å–CPIæ•°æ®ï¼ˆæœˆåº¦ï¼‰
                try:
                    cpi_df = self.ak.macro_china_cpi_yearly()
                    if not cpi_df.empty:
                        latest_cpi = cpi_df.iloc[-1]
                        macro_data['cpi'] = {
                            'month': str(latest_cpi.get('æœˆä»½', 'N/A')),
                            'value': float(latest_cpi.get('å…¨å›½-å½“æœˆ', 100)),
                            'year_on_year': float(latest_cpi.get('å…¨å›½-åŒæ¯”', 0))
                        }
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] CPIæ•°æ®è·å–æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] CPIæ•°æ®ä¸ºç©º")
                        macro_data['cpi'] = {'month': 'N/A', 'value': 100, 'year_on_year': 0}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] CPIæ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['cpi'] = {'month': 'N/A', 'value': 100, 'year_on_year': 0}
                
                # 2.3 è·å–PMIæ•°æ®ï¼ˆæœˆåº¦ï¼‰
                try:
                    pmi_df = self.ak.macro_china_pmi_yearly()
                    if not pmi_df.empty:
                        latest_pmi = pmi_df.iloc[-1]
                        macro_data['pmi'] = {
                            'month': str(latest_pmi.get('æœˆä»½', 'N/A')),
                            'manufacturing': float(latest_pmi.get('åˆ¶é€ ä¸š-æŒ‡æ•°', 50)),
                            'non_manufacturing': float(latest_pmi.get('éåˆ¶é€ ä¸š-æŒ‡æ•°', 50))
                        }
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] PMIæ•°æ®è·å–æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] PMIæ•°æ®ä¸ºç©º")
                        macro_data['pmi'] = {'month': 'N/A', 'manufacturing': 50, 'non_manufacturing': 50}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] PMIæ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['pmi'] = {'month': 'N/A', 'manufacturing': 50, 'non_manufacturing': 50}
                
                # 2.4 è·å–M2è´§å¸ä¾›åº”é‡ï¼ˆæœˆåº¦ï¼‰
                try:
                    m2_df = self.ak.macro_china_m2_yearly()
                    if not m2_df.empty:
                        latest_m2 = m2_df.iloc[-1]
                        macro_data['m2'] = {
                            'month': str(latest_m2.get('æœˆä»½', 'N/A')),
                            'value': float(latest_m2.get('è´§å¸å’Œå‡†è´§å¸(M2)-æ•°é‡(äº¿å…ƒ)', 0)),
                            'growth_rate': float(latest_m2.get('è´§å¸å’Œå‡†è´§å¸(M2)-åŒæ¯”å¢é•¿', 0))
                        }
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] M2æ•°æ®è·å–æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] M2æ•°æ®ä¸ºç©º")
                        macro_data['m2'] = {'month': 'N/A', 'value': 0, 'growth_rate': 0}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] M2æ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['m2'] = {'month': 'N/A', 'value': 0, 'growth_rate': 0}
                
                # 2.5 è·å–LPRåˆ©ç‡ï¼ˆæœˆåº¦ï¼‰
                try:
                    # ä¿®å¤LPRæ•°æ®è·å–æ–¹æ³•ï¼Œä½¿ç”¨æ­£ç¡®çš„AKShareæ¥å£
                    lpr_df = self.ak.macro_china_lpr()
                    if not lpr_df.empty:
                        latest_lpr = lpr_df.iloc[-1]
                        macro_data['lpr'] = {
                            'date': str(latest_lpr.get('TRADE_DATE', 'N/A')),
                            'lpr_1y': float(latest_lpr.get('LPR1Y', 0)),
                            'lpr_5y': float(latest_lpr.get('LPR5Y', 0))
                        }
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] LPRæ•°æ®è·å–æˆåŠŸ")
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] LPRæ•°æ®ä¸ºç©º")
                        macro_data['lpr'] = {'date': 'N/A', 'lpr_1y': 0, 'lpr_5y': 0}
                except Exception as e:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] LPRæ•°æ®è·å–å¤±è´¥: {e}")
                    macro_data['lpr'] = {'date': 'N/A', 'lpr_1y': 0, 'lpr_5y': 0}
                
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] å®è§‚æ•°æ®è·å–å®Œæˆï¼Œå…±{len(macro_data)}ä¸ªæŒ‡æ ‡")
                break  # æˆåŠŸè·å–æ•°æ®ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] å®è§‚æ•°æ®è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    import time
                    time.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] å®è§‚æ•°æ®è·å–å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
        
        # 3. ç¼“å­˜æ•°æ®
        if self.cache is not None and macro_data:
            try:
                cache_doc = {
                    "cache_key": cache_key,
                    "data": macro_data,
                    "created_at": datetime.utcnow(),
                    "end_date": end_date
                }
                self.cache.index_cache.update_one(
                    {"cache_key": cache_key},
                    {"$set": cache_doc},
                    upsert=True
                )
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] å®è§‚æ•°æ®å·²ç¼“å­˜")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        return macro_data
    
    def get_policy_news(self, lookback_days: int = 7) -> List[Dict[str, Any]]:
        """
        è·å–æ”¿ç­–æ–°é—»
        
        Args:
            lookback_days: å›æº¯å¤©æ•°ï¼Œé»˜è®¤7å¤©
            
        Returns:
            List[Dict[str, Any]]: æ–°é—»åˆ—è¡¨ï¼Œæ¯æ¡æ–°é—»åŒ…å«æ ‡é¢˜ã€å†…å®¹ã€æ—¶é—´ç­‰
        """
        logger.info(f"ğŸ“° [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–æ”¿ç­–æ–°é—», lookback_days={lookback_days}")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"policy_news_{lookback_days}"
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.cache_ttl['news']:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–æ”¿ç­–æ–°é—»")
                        return cached_data.get("data", [])
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # 2. ä»AKShareè·å–æ–°é—»
        news_list = []
        
        try:
            # 2.1 å°è¯•è·å–æ–°é—»è”æ’­æ–‡å­—ç¨¿ï¼ˆä¸»è¦æ•°æ®æºï¼‰
            try:
                news_df = self.ak.news_cctv()
                if not news_df.empty:
                    # åªå–æœ€è¿‘lookback_dayså¤©çš„æ–°é—»
                    news_df['date'] = pd.to_datetime(news_df['date'])
                    cutoff_date = datetime.now() - timedelta(days=lookback_days)
                    recent_news = news_df[news_df['date'] >= cutoff_date]
                    
                    for _, row in recent_news.iterrows():
                        news_list.append({
                            'title': row.get('title', ''),
                            'content': row.get('content', ''),
                            'date': row.get('date').strftime('%Y-%m-%d') if pd.notna(row.get('date')) else '',
                            'source': 'æ–°é—»è”æ’­'
                        })
                    
                    logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] æ–°é—»è”æ’­æ•°æ®è·å–æˆåŠŸï¼Œå…±{len(news_list)}æ¡")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ–°é—»è”æ’­æ•°æ®è·å–å¤±è´¥: {e}")
            
            # 2.2 é™çº§æ–¹æ¡ˆï¼šè·å–ç™¾åº¦è´¢ç»æ–°é—»
            if len(news_list) == 0:
                try:
                    logger.info(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šç™¾åº¦è´¢ç»æ–°é—»")
                    news_df = self.ak.stock_news_em(symbol="å®è§‚ç»æµ")
                    if not news_df.empty:
                        for _, row in news_df.head(10).iterrows():
                            news_list.append({
                                'title': row.get('æ–°é—»æ ‡é¢˜', ''),
                                'content': row.get('æ–°é—»å†…å®¹', ''),
                                'date': row.get('å‘å¸ƒæ—¶é—´', ''),
                                'source': 'ä¸œæ–¹è´¢å¯Œ'
                            })
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ç™¾åº¦è´¢ç»æ–°é—»è·å–æˆåŠŸï¼Œå…±{len(news_list)}æ¡")
                except Exception as e2:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç™¾åº¦è´¢ç»æ–°é—»è·å–å¤±è´¥: {e2}")
            
        except Exception as e:
            logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ”¿ç­–æ–°é—»è·å–å¤±è´¥: {e}")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if len(news_list) == 0:
            logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æœªè·å–åˆ°æ”¿ç­–æ–°é—»ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            news_list = [{
                'title': 'æš‚æ— æ”¿ç­–æ–°é—»',
                'content': 'å½“å‰æ— æ³•è·å–æ”¿ç­–æ–°é—»æ•°æ®ï¼Œè¯·ç¨åé‡è¯•',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'source': 'ç³»ç»Ÿæç¤º'
            }]
        
        # 3. ç¼“å­˜æ•°æ®
        if self.cache is not None and news_list:
            try:
                cache_doc = {
                    "cache_key": cache_key,
                    "data": news_list,
                    "created_at": datetime.utcnow(),
                    "lookback_days": lookback_days
                }
                self.cache.index_cache.update_one(
                    {"cache_key": cache_key},
                    {"$set": cache_doc},
                    upsert=True
                )
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] æ”¿ç­–æ–°é—»å·²ç¼“å­˜")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        return news_list
    
    def get_sector_flows(self, trade_date: str = None) -> Dict[str, Any]:
        """
        è·å–æ¿å—èµ„é‡‘æµå‘
        
        Args:
            trade_date: äº¤æ˜“æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºæœ€æ–°äº¤æ˜“æ—¥
            
        Returns:
            Dict[str, Any]: æ¿å—èµ„é‡‘æµå‘æ•°æ®ï¼ŒåŒ…å«topæ¶¨å¹…æ¿å—å’Œèµ„é‡‘æµå…¥æ•°æ®
        """
        logger.info(f"ğŸ’° [æŒ‡æ•°æ•°æ®æä¾›è€…] è·å–æ¿å—èµ„é‡‘æµå‘, trade_date={trade_date}")
        
        if trade_date is None:
            trade_date = datetime.now().strftime("%Y-%m-%d")
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"sector_flows_{trade_date}"
        if self.cache is not None:
            try:
                cached_data = self.cache.index_cache.find_one({"cache_key": cache_key})
                if cached_data:
                    cache_time = cached_data.get("created_at")
                    if cache_time and (datetime.utcnow() - cache_time).total_seconds() < self.cache_ttl['sector']:
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] ä»ç¼“å­˜è·å–æ¿å—æ•°æ®")
                        return cached_data.get("data", {})
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        # 2. ä»AKShareè·å–æ¿å—æ•°æ®ï¼ˆå¢åŠ é‡è¯•æœºåˆ¶ï¼‰
        sector_data = {
            'top_sectors': [],
            'bottom_sectors': [],
            'all_sectors': []
        }
        
        max_retries = 3
        retry_delay = 1  # é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        
        for attempt in range(max_retries):
            try:
                # è·å–ä¸œæ–¹è´¢å¯Œæ¿å—èµ„é‡‘æµæ•°æ®
                sector_df = self.ak.stock_board_industry_name_em()
                
                if not sector_df.empty:
                    # è·å–æ¶¨è·Œå¹…æ•°æ®
                    sector_flow_df = self.ak.stock_board_industry_summary_ths()
                    
                    if not sector_flow_df.empty:
                        # æŒ‰æ¶¨è·Œå¹…æ’åº
                        sector_flow_df = sector_flow_df.sort_values('æ¶¨è·Œå¹…', ascending=False)
                        
                        # Top 5 é¢†æ¶¨æ¿å—
                        for _, row in sector_flow_df.head(5).iterrows():
                            sector_data['top_sectors'].append({
                                'name': row.get('æ¿å—', ''),
                                'change_pct': float(row.get('æ¶¨è·Œå¹…', 0)),
                                'net_inflow': float(row.get('æµå…¥èµ„é‡‘', 0)) if 'æµå…¥èµ„é‡‘' in row else 0,
                                'turnover_rate': float(row.get('æ¢æ‰‹ç‡', 0)) if 'æ¢æ‰‹ç‡' in row else 0
                            })
                        
                        # Bottom 5 é¢†è·Œæ¿å—
                        for _, row in sector_flow_df.tail(5).iterrows():
                            sector_data['bottom_sectors'].append({
                                'name': row.get('æ¿å—', ''),
                                'change_pct': float(row.get('æ¶¨è·Œå¹…', 0)),
                                'net_inflow': float(row.get('æµå…¥èµ„é‡‘', 0)) if 'æµå…¥èµ„é‡‘' in row else 0,
                                'turnover_rate': float(row.get('æ¢æ‰‹ç‡', 0)) if 'æ¢æ‰‹ç‡' in row else 0
                            })
                        
                        # æ‰€æœ‰æ¿å—æ¦‚å†µ
                        for _, row in sector_flow_df.iterrows():
                            sector_data['all_sectors'].append({
                                'name': row.get('æ¿å—', ''),
                                'change_pct': float(row.get('æ¶¨è·Œå¹…', 0))
                            })
                        
                        logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ•°æ®è·å–æˆåŠŸï¼Œå…±{len(sector_flow_df)}ä¸ªæ¿å—")
                        break  # æˆåŠŸè·å–æ•°æ®ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                    else:
                        logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æµå‘æ•°æ®ä¸ºç©º")
                else:
                    logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—åç§°æ•°æ®ä¸ºç©º")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ•°æ®è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    import time
                    time.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"âŒ [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ•°æ®è·å–å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
        
        # 3. ç¼“å­˜æ•°æ®
        if self.cache is not None and sector_data:
            try:
                cache_doc = {
                    "cache_key": cache_key,
                    "data": sector_data,
                    "created_at": datetime.utcnow(),
                    "trade_date": trade_date
                }
                self.cache.index_cache.update_one(
                    {"cache_key": cache_key},
                    {"$set": cache_doc},
                    upsert=True
                )
                logger.info(f"âœ… [æŒ‡æ•°æ•°æ®æä¾›è€…] æ¿å—æ•°æ®å·²ç¼“å­˜")
            except Exception as e:
                logger.warning(f"âš ï¸ [æŒ‡æ•°æ•°æ®æä¾›è€…] ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
        
        return sector_data


# å…¨å±€å®ä¾‹
_index_data_provider = None


def get_index_data_provider() -> IndexDataProvider:
    """è·å–å…¨å±€æŒ‡æ•°æ•°æ®æä¾›è€…å®ä¾‹"""
    global _index_data_provider
    if _index_data_provider is None:
        _index_data_provider = IndexDataProvider()
    return _index_data_provider
