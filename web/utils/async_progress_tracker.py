#!/usr/bin/env python3
"""
Asynchronous progress tracker
Supports both Redis and file storage. The frontend polls for progress updates periodically.
"""

import json
import time
import os
from typing import Dict, Any, Optional, List
from datetime import datetime
import threading
from pathlib import Path

# Import logging module
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('async_progress')

"""Safely serialize objects, handling non-serializable types"""
def safe_serialize(obj):
    if hasattr(obj, 'dict'):
        # Pydantic objects
        return obj.dict()
    elif hasattr(obj, '__dict__'):
        # Regular objects, convert to dictionary
        result = {}
        for key, value in obj.__dict__.items():
            if not key.startswith('_'):  # Skip private attributes
                try:
                    json.dumps(value)  # Test if serializable
                    result[key] = value
                except (TypeError, ValueError):
                    result[key] = str(value)  # Convert to string
        return result
    elif isinstance(obj, (list, tuple)):
        return [safe_serialize(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: safe_serialize(value) for key, value in obj.items()}
    else:
        try:
            json.dumps(obj)  # Test if serializable
            return obj
        except (TypeError, ValueError):
            return str(obj)  # Convert to string

class AsyncProgressTracker:
    """Asynchronous progress tracker"""
    
    def __init__(self, analysis_id: str, analysts: List[str], research_depth: int, llm_provider: str):
        self.analysis_id = analysis_id
        self.analysts = analysts
        self.research_depth = research_depth
        self.llm_provider = llm_provider
        self.start_time = time.time()
        
        # Generate analysis steps
        self.analysis_steps = self._generate_dynamic_steps()
        self.estimated_duration = self._estimate_total_duration()
        
        # Initialize status
        self.current_step = 0
        self.progress_data = {
            'analysis_id': analysis_id,
            'status': 'running',
            'current_step': 0,
            'total_steps': len(self.analysis_steps),
            'progress_percentage': 0.0,
            'current_step_name': self.analysis_steps[0]['name'],
            'current_step_description': self.analysis_steps[0]['description'],
            'elapsed_time': 0.0,
            'estimated_total_time': self.estimated_duration,
            'remaining_time': self.estimated_duration,
            'last_message': 'å‡†å¤‡å¼€å§‹åˆ†æ...',
            'last_update': time.time(),
            'start_time': self.start_time,
            'steps': self.analysis_steps
        }
        
        # Try to initialize Redis, fallback to file
        self.redis_client = None
        self.use_redis = self._init_redis()
        
        if not self.use_redis:
            # Use file storage
            self.progress_file = f"./data/progress_{analysis_id}.json"
            os.makedirs(os.path.dirname(self.progress_file), exist_ok=True)
        
        # Save initial status
        self._save_progress()
        
        logger.info(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] åˆå§‹åŒ–å®Œæˆ: {analysis_id}, å­˜å‚¨æ–¹å¼: {'Redis' if self.use_redis else 'æ–‡ä»¶'}")

        # Register with logging system for automatic progress updates
        try:
            from .progress_log_handler import register_analysis_tracker
            import threading

            # Use timeout mechanism to avoid deadlock
            def register_with_timeout():
                try:
                    register_analysis_tracker(self.analysis_id, self)
                    print(f"âœ… [è¿›åº¦é›†æˆ] è·Ÿè¸ªå™¨æ³¨å†ŒæˆåŠŸ: {self.analysis_id}")
                except Exception as e:
                    print(f"âŒ [è¿›åº¦é›†æˆ] è·Ÿè¸ªå™¨æ³¨å†Œå¤±è´¥: {e}")

            # Register in a separate thread to avoid blocking the main thread
            register_thread = threading.Thread(target=register_with_timeout, daemon=True)
            register_thread.start()
            register_thread.join(timeout=2.0)  # 2 seconds timeout

            if register_thread.is_alive():
                print(f"âš ï¸ [è¿›åº¦é›†æˆ] è·Ÿè¸ªå™¨æ³¨å†Œè¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œ: {self.analysis_id}")

        except ImportError:
            logger.debug("ğŸ“Š [å¼‚æ­¥è¿›åº¦] æ—¥å¿—é›†æˆä¸å¯ç”¨")
        except Exception as e:
            print(f"âŒ [è¿›åº¦é›†æˆ] è·Ÿè¸ªå™¨æ³¨å†Œå¼‚å¸¸: {e}")
    
    def _init_redis(self) -> bool:
        """Initialize Redis connection"""
        try:
            # First check REDIS_ENABLED environment variable
            redis_enabled_raw = os.getenv('REDIS_ENABLED', 'false')
            redis_enabled = redis_enabled_raw.lower()
            logger.info(f"ğŸ” [Redisæ£€æŸ¥] REDIS_ENABLEDåŸå€¼='{redis_enabled_raw}' -> å¤„ç†å='{redis_enabled}'")

            if redis_enabled != 'true':
                logger.info(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] Rediså·²ç¦ç”¨ï¼Œä½¿ç”¨æ–‡ä»¶å­˜å‚¨")
                return False

            import redis

            # Get Redis configuration from environment variables
            redis_host = os.getenv('REDIS_HOST', 'localhost')
            redis_port = int(os.getenv('REDIS_PORT', 6379))
            redis_password = os.getenv('REDIS_PASSWORD', None)
            redis_db = int(os.getenv('REDIS_DB', 0))

            # Create Redis connection
            if redis_password:
                self.redis_client = redis.Redis(
                    host=redis_host,
                    port=redis_port,
                    password=redis_password,
                    db=redis_db,
                    decode_responses=True
                )
            else:
                self.redis_client = redis.Redis(
                    host=redis_host,
                    port=redis_port,
                    db=redis_db,
                    decode_responses=True
                )

            # Test connection
            self.redis_client.ping()
            logger.info(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] Redisè¿æ¥æˆåŠŸ: {redis_host}:{redis_port}")
            return True
        except Exception as e:
            logger.warning(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶å­˜å‚¨: {e}")
            return False
    
    def _generate_dynamic_steps(self) -> List[Dict]:
        """Dynamically generate analysis steps based on analyst count and research depth"""
        steps = [
            {"name": "ğŸ“‹ å‡†å¤‡é˜¶æ®µ", "description": "éªŒè¯è‚¡ç¥¨ä»£ç ï¼Œæ£€æŸ¥æ•°æ®æºå¯ç”¨æ€§", "weight": 0.05},
            {"name": "ğŸ”§ ç¯å¢ƒæ£€æŸ¥", "description": "æ£€æŸ¥APIå¯†é’¥é…ç½®ï¼Œç¡®ä¿æ•°æ®è·å–æ­£å¸¸", "weight": 0.02},
            {"name": "ğŸ’° æˆæœ¬ä¼°ç®—", "description": "æ ¹æ®åˆ†ææ·±åº¦é¢„ä¼°APIè°ƒç”¨æˆæœ¬", "weight": 0.01},
            {"name": "âš™ï¸ å‚æ•°è®¾ç½®", "description": "é…ç½®åˆ†æå‚æ•°å’ŒAIæ¨¡å‹é€‰æ‹©", "weight": 0.02},
            {"name": "ğŸš€ å¯åŠ¨å¼•æ“", "description": "åˆå§‹åŒ–AIåˆ†æå¼•æ“ï¼Œå‡†å¤‡å¼€å§‹åˆ†æ", "weight": 0.05},
        ]

        # Add specific steps for each analyst
        analyst_base_weight = 0.6 / len(self.analysts)  # 60% of time for analysts
        for analyst in self.analysts:
            analyst_info = self._get_analyst_step_info(analyst)
            steps.append({
                "name": analyst_info["name"],
                "description": analyst_info["description"],
                "weight": analyst_base_weight
            })

        # Add subsequent steps based on research depth
        if self.research_depth >= 2:
            # Standard and deep analysis include researcher debates
            steps.extend([
                {"name": "ğŸ“ˆ å¤šå¤´è§‚ç‚¹", "description": "ä»ä¹è§‚è§’åº¦åˆ†ææŠ•èµ„æœºä¼šå’Œä¸Šæ¶¨æ½œåŠ›", "weight": 0.06},
                {"name": "ğŸ“‰ ç©ºå¤´è§‚ç‚¹", "description": "ä»è°¨æ…è§’åº¦åˆ†ææŠ•èµ„é£é™©å’Œä¸‹è·Œå¯èƒ½", "weight": 0.06},
                {"name": "ğŸ¤ è§‚ç‚¹æ•´åˆ", "description": "ç»¼åˆå¤šç©ºè§‚ç‚¹ï¼Œå½¢æˆå¹³è¡¡çš„æŠ•èµ„å»ºè®®", "weight": 0.05},
            ])

        # All depths include trading decisions
        steps.append({"name": "ğŸ’¡ æŠ•èµ„å»ºè®®", "description": "åŸºäºåˆ†æç»“æœåˆ¶å®šå…·ä½“çš„ä¹°å–å»ºè®®", "weight": 0.06})

        if self.research_depth >= 3:
            # Deep analysis includes detailed risk assessment
            steps.extend([
                {"name": "ğŸ”¥ æ¿€è¿›ç­–ç•¥", "description": "è¯„ä¼°é«˜é£é™©é«˜æ”¶ç›Šçš„æŠ•èµ„ç­–ç•¥", "weight": 0.03},
                {"name": "ğŸ›¡ï¸ ä¿å®ˆç­–ç•¥", "description": "è¯„ä¼°ä½é£é™©ç¨³å¥çš„æŠ•èµ„ç­–ç•¥", "weight": 0.03},
                {"name": "âš–ï¸ å¹³è¡¡ç­–ç•¥", "description": "è¯„ä¼°é£é™©æ”¶ç›Šå¹³è¡¡çš„æŠ•èµ„ç­–ç•¥", "weight": 0.03},
                {"name": "ğŸ¯ é£é™©æ§åˆ¶", "description": "åˆ¶å®šé£é™©æ§åˆ¶æªæ–½å’Œæ­¢æŸç­–ç•¥", "weight": 0.04},
            ])
        else:
            # Simplified risk assessment for fast and standard analysis
            steps.append({"name": "âš ï¸ é£é™©æç¤º", "description": "è¯†åˆ«ä¸»è¦æŠ•èµ„é£é™©å¹¶æä¾›é£é™©æç¤º", "weight": 0.05})

        # Final organization steps
        steps.append({"name": "ğŸ“Š ç”ŸæˆæŠ¥å‘Š", "description": "æ•´ç†æ‰€æœ‰åˆ†æç»“æœï¼Œç”Ÿæˆæœ€ç»ˆæŠ•èµ„æŠ¥å‘Š", "weight": 0.04})

        # Rebalance weights to ensure total sum is 1.0
        total_weight = sum(step["weight"] for step in steps)
        for step in steps:
            step["weight"] = step["weight"] / total_weight

        return steps
    
    def _get_analyst_display_name(self, analyst: str) -> str:
        """Get analyst display name (for compatibility)"""
        name_map = {
            'market': 'å¸‚åœºåˆ†æå¸ˆ',
            'fundamentals': 'åŸºæœ¬é¢åˆ†æå¸ˆ',
            'technical': 'æŠ€æœ¯åˆ†æå¸ˆ',
            'sentiment': 'æƒ…ç»ªåˆ†æå¸ˆ',
            'risk': 'é£é™©åˆ†æå¸ˆ'
        }
        return name_map.get(analyst, f'{analyst}åˆ†æå¸ˆ')

    def _get_analyst_step_info(self, analyst: str) -> Dict[str, str]:
        """Get analyst step information (name and description)"""
        analyst_info = {
            'market': {
                "name": "ğŸ“Š å¸‚åœºåˆ†æ",
                "description": "åˆ†æè‚¡ä»·èµ°åŠ¿ã€æˆäº¤é‡ã€å¸‚åœºçƒ­åº¦ç­‰å¸‚åœºè¡¨ç°"
            },
            'fundamentals': {
                "name": "ğŸ’¼ åŸºæœ¬é¢åˆ†æ",
                "description": "åˆ†æå…¬å¸è´¢åŠ¡çŠ¶å†µã€ç›ˆåˆ©èƒ½åŠ›ã€æˆé•¿æ€§ç­‰åŸºæœ¬é¢"
            },
            'technical': {
                "name": "ğŸ“ˆ æŠ€æœ¯åˆ†æ",
                "description": "åˆ†æKçº¿å›¾å½¢ã€æŠ€æœ¯æŒ‡æ ‡ã€æ”¯æ’‘é˜»åŠ›ç­‰æŠ€æœ¯é¢"
            },
            'sentiment': {
                "name": "ğŸ’­ æƒ…ç»ªåˆ†æ",
                "description": "åˆ†æå¸‚åœºæƒ…ç»ªã€æŠ•èµ„è€…å¿ƒç†ã€èˆ†è®ºå€¾å‘ç­‰"
            },
            'news': {
                "name": "ğŸ“° æ–°é—»åˆ†æ",
                "description": "åˆ†æç›¸å…³æ–°é—»ã€å…¬å‘Šã€è¡Œä¸šåŠ¨æ€å¯¹è‚¡ä»·çš„å½±å“"
            },
            'social_media': {
                "name": "ğŸŒ ç¤¾äº¤åª’ä½“",
                "description": "åˆ†æç¤¾äº¤åª’ä½“è®¨è®ºã€ç½‘ç»œçƒ­åº¦ã€æ•£æˆ·æƒ…ç»ªç­‰"
            },
            'risk': {
                "name": "âš ï¸ é£é™©åˆ†æ",
                "description": "è¯†åˆ«æŠ•èµ„é£é™©ã€è¯„ä¼°é£é™©ç­‰çº§ã€åˆ¶å®šé£æ§æªæ–½"
            }
        }

        return analyst_info.get(analyst, {
            "name": f"ğŸ” {analyst}åˆ†æ",
            "description": f"è¿›è¡Œ{analyst}ç›¸å…³çš„ä¸“ä¸šåˆ†æ"
        })
    
    def _estimate_total_duration(self) -> float:
        """Estimate total duration based on analyst count, research depth, and model type (seconds)"""
        # Base time (seconds) - environment preparation, configuration, etc.
        base_time = 60
        
        # Actual time for each analyst (based on real test data)
        analyst_base_time = {
            1: 120,  # Fast analysis: approximately 2 minutes per analyst
            2: 180,  # Basic analysis: approximately 3 minutes per analyst  
            3: 240   # Standard analysis: approximately 4 minutes per analyst
        }.get(self.research_depth, 180)
        
        analyst_time = len(self.analysts) * analyst_base_time
        
        # Model speed impact (based on actual tests)
        model_multiplier = {
            'dashscope': 1.0,  # Ali Baiyan speed is moderate
            'deepseek': 0.7,   # DeepSeek is faster
            'google': 1.3      # Google is slower
        }.get(self.llm_provider, 1.0)
        
        # Research depth additional impact (tool call complexity)
        depth_multiplier = {
            1: 0.8,  # Fast analysis, fewer tool calls
            2: 1.0,  # Basic analysis, standard tool calls
            3: 1.3   # Standard analysis, more tool calls and reasoning
        }.get(self.research_depth, 1.0)
        
        total_time = (base_time + analyst_time) * model_multiplier * depth_multiplier
        return total_time
    
    def update_progress(self, message: str, step: Optional[int] = None):
        """Update progress status"""
        current_time = time.time()
        elapsed_time = current_time - self.start_time

        # Automatically detect step
        if step is None:
            step = self._detect_step_from_message(message)

        # Update step (prevent regression)
        if step is not None and step >= self.current_step:
            self.current_step = step
            logger.debug(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] æ­¥éª¤æ¨è¿›åˆ° {self.current_step + 1}/{len(self.analysis_steps)}")

        # If it's a completion message, ensure progress is 100%
        if "åˆ†æå®Œæˆ" in message or "åˆ†ææˆåŠŸ" in message or "âœ… åˆ†æå®Œæˆ" in message:
            self.current_step = len(self.analysis_steps) - 1
            logger.info(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] åˆ†æå®Œæˆï¼Œè®¾ç½®ä¸ºæœ€ç»ˆæ­¥éª¤")

        # Calculate progress
        progress_percentage = self._calculate_weighted_progress() * 100
        remaining_time = self._estimate_remaining_time(progress_percentage / 100, elapsed_time)

        # Update progress data
        current_step_info = self.analysis_steps[self.current_step] if self.current_step < len(self.analysis_steps) else self.analysis_steps[-1]

        # Special handling for tool call messages, update description but not step
        step_description = current_step_info['description']
        if "å·¥å…·è°ƒç”¨" in message:
            # Extract tool name and update description
            if "get_stock_market_data_unified" in message:
                step_description = "æ­£åœ¨è·å–å¸‚åœºæ•°æ®å’ŒæŠ€æœ¯æŒ‡æ ‡..."
            elif "get_stock_fundamentals_unified" in message:
                step_description = "æ­£åœ¨è·å–åŸºæœ¬é¢æ•°æ®å’Œè´¢åŠ¡æŒ‡æ ‡..."
            elif "get_china_stock_data" in message:
                step_description = "æ­£åœ¨è·å–Aè‚¡å¸‚åœºæ•°æ®..."
            elif "get_china_fundamentals" in message:
                step_description = "æ­£åœ¨è·å–Aè‚¡åŸºæœ¬é¢æ•°æ®..."
            else:
                step_description = "æ­£åœ¨è°ƒç”¨åˆ†æå·¥å…·..."
        elif "æ¨¡å—å¼€å§‹" in message:
            step_description = f"å¼€å§‹{current_step_info['name']}..."
        elif "æ¨¡å—å®Œæˆ" in message:
            step_description = f"{current_step_info['name']}å·²å®Œæˆ"

        self.progress_data.update({
            'current_step': self.current_step,
            'progress_percentage': progress_percentage,
            'current_step_name': current_step_info['name'],
            'current_step_description': step_description,
            'elapsed_time': elapsed_time,
            'remaining_time': remaining_time,
            'last_message': message,
            'last_update': current_time,
            'status': 'completed' if progress_percentage >= 100 else 'running'
        })

        # Save to storage
        self._save_progress()

        # Detailed update log
        step_name = current_step_info.get('name', 'Unknown')
        logger.info(f"ğŸ“Š [è¿›åº¦æ›´æ–°] {self.analysis_id}: {message[:50]}...")
        logger.debug(f"ğŸ“Š [è¿›åº¦è¯¦æƒ…] æ­¥éª¤{self.current_step + 1}/{len(self.analysis_steps)} ({step_name}), è¿›åº¦{progress_percentage:.1f}%, è€—æ—¶{elapsed_time:.1f}s")
    
    def _detect_step_from_message(self, message: str) -> Optional[int]:
        """Intelligently detect current step based on message content"""
        message_lower = message.lower()

        # Start analysis phase - only match initial start message
        if "ğŸš€ å¼€å§‹è‚¡ç¥¨åˆ†æ" in message:
            return 0
        # Data validation phase
        elif "éªŒè¯" in message or "é¢„è·å–" in message or "æ•°æ®å‡†å¤‡" in message:
            return 0
        # Environment preparation phase
        elif "ç¯å¢ƒ" in message or "api" in message_lower or "å¯†é’¥" in message:
            return 1
        # Cost estimation phase
        elif "æˆæœ¬" in message or "é¢„ä¼°" in message:
            return 2
        # Parameter configuration phase
        elif "é…ç½®" in message or "å‚æ•°" in message:
            return 3
        # Engine initialization phase
        elif "åˆå§‹åŒ–" in message or "å¼•æ“" in message:
            return 4
        # Module start log - only advance step when first started
        elif "æ¨¡å—å¼€å§‹" in message:
            # Extract analyst type from log, match new step names
            if "market_analyst" in message or "market" in message:
                return self._find_step_by_keyword(["å¸‚åœºåˆ†æ", "å¸‚åœº"])
            elif "fundamentals_analyst" in message or "fundamentals" in message:
                return self._find_step_by_keyword(["åŸºæœ¬é¢åˆ†æ", "åŸºæœ¬é¢"])
            elif "technical_analyst" in message or "technical" in message:
                return self._find_step_by_keyword(["æŠ€æœ¯åˆ†æ", "æŠ€æœ¯"])
            elif "sentiment_analyst" in message or "sentiment" in message:
                return self._find_step_by_keyword(["æƒ…ç»ªåˆ†æ", "æƒ…ç»ª"])
            elif "news_analyst" in message or "news" in message:
                return self._find_step_by_keyword(["æ–°é—»åˆ†æ", "æ–°é—»"])
            elif "social_media_analyst" in message or "social" in message:
                return self._find_step_by_keyword(["ç¤¾äº¤åª’ä½“", "ç¤¾äº¤"])
            elif "risk_analyst" in message or "risk" in message:
                return self._find_step_by_keyword(["é£é™©åˆ†æ", "é£é™©"])
            elif "bull_researcher" in message or "bull" in message:
                return self._find_step_by_keyword(["å¤šå¤´è§‚ç‚¹", "å¤šå¤´", "çœ‹æ¶¨"])
            elif "bear_researcher" in message or "bear" in message:
                return self._find_step_by_keyword(["ç©ºå¤´è§‚ç‚¹", "ç©ºå¤´", "çœ‹è·Œ"])
            elif "research_manager" in message:
                return self._find_step_by_keyword(["è§‚ç‚¹æ•´åˆ", "æ•´åˆ"])
            elif "trader" in message:
                return self._find_step_by_keyword(["æŠ•èµ„å»ºè®®", "å»ºè®®"])
            elif "risk_manager" in message:
                return self._find_step_by_keyword(["é£é™©æ§åˆ¶", "æ§åˆ¶"])
            elif "graph_signal_processing" in message or "signal" in message:
                return self._find_step_by_keyword(["ç”ŸæˆæŠ¥å‘Š", "æŠ¥å‘Š"])
        # Tool call log - do not advance step, only update description
        elif "å·¥å…·è°ƒç”¨" in message:
            # Stay on current step, do not advance
            return None
        # Module completion log - advance to next step
        elif "æ¨¡å—å®Œæˆ" in message:
            # When module completes, advance from current step to next step
            # No longer rely on module name, but advance based on current progress
            next_step = min(self.current_step + 1, len(self.analysis_steps) - 1)
            logger.debug(f"ğŸ“Š [æ­¥éª¤æ¨è¿›] æ¨¡å—å®Œæˆï¼Œä»æ­¥éª¤{self.current_step}æ¨è¿›åˆ°æ­¥éª¤{next_step}")
            return next_step

        return None

    def _find_step_by_keyword(self, keywords) -> Optional[int]:
        """Find step index by keyword"""
        if isinstance(keywords, str):
            keywords = [keywords]

        for i, step in enumerate(self.analysis_steps):
            for keyword in keywords:
                if keyword in step["name"]:
                    return i
        return None

    def _get_next_step(self, keyword: str) -> Optional[int]:
        """Get the next step for a specified keyword"""
        current_step_index = self._find_step_by_keyword(keyword)
        if current_step_index is not None:
            return min(current_step_index + 1, len(self.analysis_steps) - 1)
        return None

    def _calculate_weighted_progress(self) -> float:
        """Calculate progress based on step weights"""
        if self.current_step >= len(self.analysis_steps):
            return 1.0

        # If it's the last step, return 100%
        if self.current_step == len(self.analysis_steps) - 1:
            return 1.0

        completed_weight = sum(step["weight"] for step in self.analysis_steps[:self.current_step])
        total_weight = sum(step["weight"] for step in self.analysis_steps)

        return min(completed_weight / total_weight, 1.0)
    
    def _estimate_remaining_time(self, progress: float, elapsed_time: float) -> float:
        """Estimate remaining time based on total estimated time"""
        # If progress is completed, remaining time is 0
        if progress >= 1.0:
            return 0.0

        # Use a simple and accurate method: total estimated time - elapsed time
        remaining = max(self.estimated_duration - elapsed_time, 0)

        # If already exceeded estimated time, dynamically adjust based on current progress
        if remaining <= 0 and progress > 0:
            # Re-estimate total time based on current progress, then calculate remaining
            estimated_total = elapsed_time / progress
            remaining = max(estimated_total - elapsed_time, 0)

        return remaining
    
    def _save_progress(self):
        """Save progress to storage"""
        try:
            current_step_name = self.progress_data.get('current_step_name', 'Unknown')
            progress_pct = self.progress_data.get('progress_percentage', 0)
            status = self.progress_data.get('status', 'running')

            if self.use_redis:
                # Save to Redis (safely serialize)
                key = f"progress:{self.analysis_id}"
                safe_data = safe_serialize(self.progress_data)
                data_json = json.dumps(safe_data, ensure_ascii=False)
                self.redis_client.setex(key, 3600, data_json)  # 1 hour expiration

                logger.info(f"ğŸ“Š [Rediså†™å…¥] {self.analysis_id} -> {status} | {current_step_name} | {progress_pct:.1f}%")
                logger.debug(f"ğŸ“Š [Redisè¯¦æƒ…] é”®: {key}, æ•°æ®å¤§å°: {len(data_json)} å­—èŠ‚")
            else:
                # Save to file (safely serialize)
                safe_data = safe_serialize(self.progress_data)
                with open(self.progress_file, 'w', encoding='utf-8') as f:
                    json.dump(safe_data, f, ensure_ascii=False, indent=2)

                logger.info(f"ğŸ“Š [æ–‡ä»¶å†™å…¥] {self.analysis_id} -> {status} | {current_step_name} | {progress_pct:.1f}%")
                logger.debug(f"ğŸ“Š [æ–‡ä»¶è¯¦æƒ…] è·¯å¾„: {self.progress_file}")

        except Exception as e:
            logger.error(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] ä¿å­˜å¤±è´¥: {e}")
            # Try fallback storage method
            try:
                if self.use_redis:
                    # Redis failed, try file storage
                    logger.warning(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] Redisä¿å­˜å¤±è´¥ï¼Œå°è¯•æ–‡ä»¶å­˜å‚¨")
                    backup_file = f"./data/progress_{self.analysis_id}.json"
                    os.makedirs(os.path.dirname(backup_file), exist_ok=True)
                    safe_data = safe_serialize(self.progress_data)
                    with open(backup_file, 'w', encoding='utf-8') as f:
                        json.dump(safe_data, f, ensure_ascii=False, indent=2)
                    logger.info(f"ğŸ“Š [å¤‡ç”¨å­˜å‚¨] æ–‡ä»¶ä¿å­˜æˆåŠŸ: {backup_file}")
                else:
                    # File storage failed, try simplified data
                    logger.warning(f"ï¿½ï¿½ [å¼‚æ­¥è¿›åº¦] æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œå°è¯•ç®€åŒ–æ•°æ®")
                    simplified_data = {
                        'analysis_id': self.analysis_id,
                        'status': self.progress_data.get('status', 'unknown'),
                        'progress_percentage': self.progress_data.get('progress_percentage', 0),
                        'last_message': str(self.progress_data.get('last_message', '')),
                        'last_update': self.progress_data.get('last_update', time.time())
                    }
                    backup_file = f"./data/progress_{self.analysis_id}.json"
                    with open(backup_file, 'w', encoding='utf-8') as f:
                        json.dump(simplified_data, f, ensure_ascii=False, indent=2)
                    logger.info(f"ğŸ“Š [å¤‡ç”¨å­˜å‚¨] ç®€åŒ–æ•°æ®ä¿å­˜æˆåŠŸ: {backup_file}")
            except Exception as backup_e:
                logger.error(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] å¤‡ç”¨å­˜å‚¨ä¹Ÿå¤±è´¥: {backup_e}")
    
    def get_progress(self) -> Dict[str, Any]:
        """Get current progress"""
        return self.progress_data.copy()
    
    def mark_completed(self, message: str = "åˆ†æå®Œæˆ", results: Any = None):
        """Mark analysis as completed"""
        self.update_progress(message)
        self.progress_data['status'] = 'completed'
        self.progress_data['progress_percentage'] = 100.0
        self.progress_data['remaining_time'] = 0.0

        # Save analysis results (safely serialize)
        if results is not None:
            try:
                self.progress_data['raw_results'] = safe_serialize(results)
                logger.info(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] ä¿å­˜åˆ†æç»“æœ: {self.analysis_id}")
            except Exception as e:
                logger.warning(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] ç»“æœåºåˆ—åŒ–å¤±è´¥: {e}")
                self.progress_data['raw_results'] = str(results)  # Final fallback

        self._save_progress()
        logger.info(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] åˆ†æå®Œæˆ: {self.analysis_id}")

        # Unregister from logging system
        try:
            from .progress_log_handler import unregister_analysis_tracker
            unregister_analysis_tracker(self.analysis_id)
        except ImportError:
            pass
    
    def mark_failed(self, error_message: str):
        """Mark analysis as failed"""
        self.progress_data['status'] = 'failed'
        self.progress_data['last_message'] = f"åˆ†æå¤±è´¥: {error_message}"
        self.progress_data['last_update'] = time.time()
        self._save_progress()
        logger.error(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] åˆ†æå¤±è´¥: {self.analysis_id}, é”™è¯¯: {error_message}")

        # Unregister from logging system
        try:
            from .progress_log_handler import unregister_analysis_tracker
            unregister_analysis_tracker(self.analysis_id)
        except ImportError:
            pass

def get_progress_by_id(analysis_id: str) -> Optional[Dict[str, Any]]:
    """Get progress by analysis ID"""
    try:
        # Check REDIS_ENABLED environment variable
        redis_enabled = os.getenv('REDIS_ENABLED', 'false').lower() == 'true'

        # If Redis is enabled, try Redis first
        if redis_enabled:
            try:
                import redis

                # Get Redis configuration from environment variables
                redis_host = os.getenv('REDIS_HOST', 'localhost')
                redis_port = int(os.getenv('REDIS_PORT', 6379))
                redis_password = os.getenv('REDIS_PASSWORD', None)
                redis_db = int(os.getenv('REDIS_DB', 0))

                # Create Redis connection
                if redis_password:
                    redis_client = redis.Redis(
                        host=redis_host,
                        port=redis_port,
                        password=redis_password,
                        db=redis_db,
                        decode_responses=True
                    )
                else:
                    redis_client = redis.Redis(
                        host=redis_host,
                        port=redis_port,
                        db=redis_db,
                        decode_responses=True
                    )

                key = f"progress:{analysis_id}"
                data = redis_client.get(key)
                if data:
                    return json.loads(data)
            except Exception as e:
                logger.debug(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] Redisè¯»å–å¤±è´¥: {e}")

        # Try file
        progress_file = f"./data/progress_{analysis_id}.json"
        if os.path.exists(progress_file):
            with open(progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)

        return None
    except Exception as e:
        logger.error(f"ğŸ“Š [å¼‚æ­¥è¿›åº¦] è·å–è¿›åº¦å¤±è´¥: {analysis_id}, é”™è¯¯: {e}")
        return None

def format_time(seconds: float) -> str:
    """Format time for display"""
    if seconds < 60:
        return f"{seconds:.1f}ç§’"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f}åˆ†é’Ÿ"
    else:
        hours = seconds / 3600
        return f"{hours:.1f}å°æ—¶"


def get_latest_analysis_id() -> Optional[str]:
    """Get the latest analysis ID"""
    try:
        # Check REDIS_ENABLED environment variable
        redis_enabled = os.getenv('REDIS_ENABLED', 'false').lower() == 'true'

        # If Redis is enabled, try to get from Redis first
        if redis_enabled:
            try:
                import redis

                # Get Redis configuration from environment variables
                redis_host = os.getenv('REDIS_HOST', 'localhost')
                redis_port = int(os.getenv('REDIS_PORT', 6379))
                redis_password = os.getenv('REDIS_PASSWORD', None)
                redis_db = int(os.getenv('REDIS_DB', 0))

                # Create Redis connection
                if redis_password:
                    redis_client = redis.Redis(
                        host=redis_host,
                        port=redis_port,
                        password=redis_password,
                        db=redis_db,
                        decode_responses=True
                    )
                else:
                    redis_client = redis.Redis(
                        host=redis_host,
                        port=redis_port,
                        db=redis_db,
                        decode_responses=True
                    )

                # Get all progress keys
                keys = redis_client.keys("progress:*")
                if not keys:
                    return None

                # Get data for each key, find the latest
                latest_time = 0
                latest_id = None

                for key in keys:
                    try:
                        data = redis_client.get(key)
                        if data:
                            progress_data = json.loads(data)
                            last_update = progress_data.get('last_update', 0)
                            if last_update > latest_time:
                                latest_time = last_update
                                # Extract analysis_id from key (remove "progress:" prefix)
                                latest_id = key.replace('progress:', '')
                    except Exception:
                        continue

                if latest_id:
                    logger.info(f"ğŸ“Š [æ¢å¤åˆ†æ] æ‰¾åˆ°æœ€æ–°åˆ†æID: {latest_id}")
                    return latest_id

            except Exception as e:
                logger.debug(f"ğŸ“Š [æ¢å¤åˆ†æ] RedisæŸ¥æ‰¾å¤±è´¥: {e}")

        # If Redis fails or is not enabled, try to find from file
        data_dir = Path("data")
        if data_dir.exists():
            progress_files = list(data_dir.glob("progress_*.json"))
            if progress_files:
                # Sort by modification time, get the latest
                latest_file = max(progress_files, key=lambda f: f.stat().st_mtime)
                # Extract analysis_id from filename
                filename = latest_file.name
                if filename.startswith("progress_") and filename.endswith(".json"):
                    analysis_id = filename[9:-5]  # Remove prefix and suffix
                    logger.debug(f"ğŸ“Š [æ¢å¤åˆ†æ] ä»æ–‡ä»¶æ‰¾åˆ°æœ€æ–°åˆ†æID: {analysis_id}")
                    return analysis_id

        return None
    except Exception as e:
        logger.error(f"ï¿½ï¿½ [æ¢å¤åˆ†æ] è·å–æœ€æ–°åˆ†æIDå¤±è´¥: {e}")
        return None
