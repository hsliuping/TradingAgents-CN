#!/usr/bin/env python3
"""
ç¼“å­˜ç°æœ‰åˆ†æç»“æœçš„è„šæœ¬
å°†å·²æœ‰çš„è‚¡ç¥¨åˆ†ææ•°æ®ç¼“å­˜åˆ°æ–°çš„ç¼“å­˜ç³»ç»Ÿä¸­
"""

import os
import sys
import json
from datetime import date, datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tradingagents.utils.analysis_cache import get_global_cache, cache_analysis_result
from tradingagents.utils.logging_manager import get_logger

logger = get_logger('cache_script')

def cache_existing_progress_files():
    """ç¼“å­˜dataç›®å½•ä¸­çš„ç°æœ‰åˆ†æè¿›åº¦æ–‡ä»¶"""
    data_dir = project_root / "data"
    cache = get_global_cache()
    
    logger.info("ğŸ” å¼€å§‹æ‰«æç°æœ‰åˆ†ææ–‡ä»¶...")
    
    # å¤„ç†progress_analysisæ–‡ä»¶
    progress_files = list(data_dir.glob("progress_analysis_*.json"))
    
    for progress_file in progress_files:
        try:
            logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {progress_file.name}")
            
            with open(progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            
            # ä»æ–‡ä»¶åæå–æ—¥æœŸ
            filename_parts = progress_file.stem.split('_')
            if len(filename_parts) >= 3:
                date_str = filename_parts[2]  # 20250730æ ¼å¼
                # è½¬æ¢ä¸ºæ ‡å‡†æ—¥æœŸæ ¼å¼
                try:
                    analysis_date = datetime.strptime(date_str, '%Y%m%d').strftime('%Y-%m-%d')
                except ValueError:
                    analysis_date = date.today().strftime('%Y-%m-%d')
            else:
                analysis_date = date.today().strftime('%Y-%m-%d')
            
            # å°è¯•ä»åˆ†ææ•°æ®ä¸­æå–è‚¡ç¥¨ä»£ç 
            stock_symbol = None
            
            # æ–¹æ³•1: æ£€æŸ¥analysis_idä¸­æ˜¯å¦åŒ…å«è‚¡ç¥¨ä»£ç 
            analysis_id = progress_data.get('analysis_id', '')
            
            # æ–¹æ³•2: æ£€æŸ¥stepsä¸­çš„ç»“æœæ•°æ®
            if 'results' in progress_data:
                for step_key, step_result in progress_data['results'].items():
                    if isinstance(step_result, dict) and 'stock_symbol' in step_result:
                        stock_symbol = step_result['stock_symbol']
                        break
            
            # å¦‚æœæ‰¾ä¸åˆ°è‚¡ç¥¨ä»£ç ï¼Œä½¿ç”¨æ–‡ä»¶åˆ†æIDä½œä¸ºæ ‡è¯†
            if not stock_symbol:
                stock_symbol = f"UNKNOWN_{filename_parts[1]}"  # ä½¿ç”¨IDéƒ¨åˆ†
            
            logger.info(f"ğŸ“Š è‚¡ç¥¨ä»£ç : {stock_symbol}, åˆ†ææ—¥æœŸ: {analysis_date}")
            
            # ä¿å­˜åˆ°ç¼“å­˜
            success = cache.save_analysis(stock_symbol, progress_data, analysis_date)
            if success:
                logger.info(f"âœ… å·²ç¼“å­˜: {stock_symbol} ({analysis_date})")
            else:
                logger.error(f"âŒ ç¼“å­˜å¤±è´¥: {stock_symbol}")
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {progress_file.name}: {e}")
    
    # å¤„ç†reportsç›®å½•ä¸­çš„åˆ†ææŠ¥å‘Š
    reports_dir = data_dir / "reports"
    if reports_dir.exists():
        report_files = list(reports_dir.glob("*.md"))
        
        for report_file in report_files:
            try:
                # ä»æ–‡ä»¶åè§£æè‚¡ç¥¨ä»£ç å’Œæ—¥æœŸ
                filename = report_file.stem
                parts = filename.split('_')
                
                if len(parts) >= 3:
                    stock_symbol = parts[0]
                    date_part = parts[-2]  # å€’æ•°ç¬¬äºŒä¸ªéƒ¨åˆ†é€šå¸¸æ˜¯æ—¥æœŸ
                    
                    try:
                        analysis_date = datetime.strptime(date_part, '%Y%m%d').strftime('%Y-%m-%d')
                    except ValueError:
                        analysis_date = date.today().strftime('%Y-%m-%d')
                    
                    # è¯»å–æŠ¥å‘Šå†…å®¹
                    with open(report_file, 'r', encoding='utf-8') as f:
                        report_content = f.read()
                    
                    # æ„é€ åˆ†ææ•°æ®
                    analysis_data = {
                        'type': 'markdown_report',
                        'source_file': str(report_file),
                        'content': report_content,
                        'generated_time': datetime.fromtimestamp(report_file.stat().st_mtime).isoformat()
                    }
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    success = cache.save_analysis(stock_symbol, analysis_data, analysis_date)
                    if success:
                        logger.info(f"âœ… å·²ç¼“å­˜æŠ¥å‘Š: {stock_symbol} ({analysis_date})")
                        
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æŠ¥å‘Šå¤±è´¥ {report_file.name}: {e}")

def create_920005_sample_data():
    """ä¸º920005åˆ›å»ºç¤ºä¾‹åˆ†ææ•°æ®"""
    from datetime import timedelta
    
    cache = get_global_cache()
    
    logger.info("ğŸš¢ åˆ›å»º920005æ±Ÿé¾™èˆ¹è‰‡çš„ç¤ºä¾‹åˆ†ææ•°æ®...")
    
    # åˆ›å»ºä»Šå¤©çš„åˆ†ææ•°æ®
    today = date.today().strftime('%Y-%m-%d')
    
    sample_analysis = {
        'stock_symbol': '920005',
        'stock_name': 'æ±Ÿé¾™èˆ¹è‰‡',
        'market_type': 'Aè‚¡',
        'analysis_type': 'comprehensive',
        'analysis_date': today,
        'created_time': datetime.now().isoformat(),
        'market_analysis': {
            'current_price': 64.63,
            'price_change': '+2.35%',
            'volume': 1256789,
            'market_cap': 'çº¦20äº¿å…ƒ',
            'trend': 'ä¸Šå‡è¶‹åŠ¿',
            'technical_indicators': {
                'ma5': 62.45,
                'ma10': 60.12,
                'rsi': 68.5,
                'macd': 'DIFFé‡‘å‰DEA'
            }
        },
        'fundamentals_analysis': {
            'industry': 'èˆ¹èˆ¶åˆ¶é€ ',
            'main_business': 'ç‰¹ç§èˆ¹èˆ¶åˆ¶é€ ä¸æµ·æ´‹å·¥ç¨‹è£…å¤‡',
            'financial_highlights': {
                'revenue_growth': '+15.2%',
                'profit_margin': '12.3%',
                'roe': '15.8%',
                'debt_ratio': '35.2%'
            },
            'competitive_advantages': [
                'åœ¨ç‰¹ç§èˆ¹èˆ¶åˆ¶é€ é¢†åŸŸå…·æœ‰æŠ€æœ¯ä¼˜åŠ¿',
                'å†›æ°‘èåˆæˆ˜ç•¥å¸ƒå±€',
                'æµ·æ´‹å·¥ç¨‹è£…å¤‡å¸‚åœºéœ€æ±‚å¢é•¿'
            ]
        },
        'news_analysis': {
            'recent_news': [
                {
                    'title': 'æ±Ÿé¾™èˆ¹è‰‡è·å¾—å†›ç”¨ç‰¹ç§èˆ¹èˆ¶è®¢å•',
                    'impact': 'positive',
                    'summary': 'å…¬å¸è¿‘æœŸè·å¾—å¤§é¢å†›ç”¨ç‰¹ç§èˆ¹èˆ¶åˆ¶é€ è®¢å•ï¼Œé¢„è®¡å¯¹ä¸šç»©äº§ç”Ÿç§¯æå½±å“'
                },
                {
                    'title': 'æµ·æ´‹ç»æµæ”¿ç­–åˆ©å¥½èˆ¹èˆ¶åˆ¶é€ è¡Œä¸š',
                    'impact': 'positive',
                    'summary': 'å›½å®¶æµ·æ´‹ç»æµå‘å±•æ”¿ç­–ä¸ºèˆ¹èˆ¶åˆ¶é€ ä¼ä¸šå¸¦æ¥æ–°æœºé‡'
                }
            ],
            'sentiment_score': 0.75,
            'market_sentiment': 'ç§¯æ'
        },
        'risk_analysis': {
            'risk_level': 'ä¸­ç­‰',
            'main_risks': [
                'èˆ¹èˆ¶åˆ¶é€ å‘¨æœŸè¾ƒé•¿ï¼Œèµ„é‡‘å ç”¨é£é™©',
                'å†›å·¥è®¢å•ä¾èµ–åº¦è¾ƒé«˜',
                'åŸææ–™ä»·æ ¼æ³¢åŠ¨å½±å“'
            ],
            'risk_score': 3.2
        },
        'investment_advice': {
            'recommendation': 'è°¨æ…ä¹°å…¥',
            'target_price': 68.00,
            'confidence_level': 0.72,
            'investment_reasoning': [
                'å…¬å¸åœ¨ç‰¹ç§èˆ¹èˆ¶åˆ¶é€ é¢†åŸŸå…·æœ‰ç«äº‰ä¼˜åŠ¿',
                'å—ç›Šäºæµ·æ´‹ç»æµå’Œå†›æ°‘èåˆæ”¿ç­–',
                'è¿‘æœŸè·å¾—é‡è¦è®¢å•ï¼Œä¸šç»©é¢„æœŸå‘å¥½',
                'ä½†éœ€å…³æ³¨è¡Œä¸šå‘¨æœŸæ€§å’Œèµ„é‡‘é£é™©'
            ],
            'position_suggestion': 'å»ºè®®åˆ†æ‰¹å»ºä»“ï¼Œæ§åˆ¶ä»“ä½åœ¨5-8%'
        },
        'technical_analysis': {
            'chart_pattern': 'ä¸Šå‡ä¸‰è§’å½¢',
            'support_levels': [60.0, 62.5],
            'resistance_levels': [66.0, 68.5],
            'trend_direction': 'çŸ­æœŸçœ‹æ¶¨',
            'volume_analysis': 'æ”¾é‡ä¸Šæ¶¨ï¼Œèµ„é‡‘æµå…¥ç§¯æ'
        },
        'summary': {
            'overall_score': 7.2,
            'strengths': [
                'æŠ€æœ¯ä¼˜åŠ¿æ˜æ˜¾',
                'æ”¿ç­–ç¯å¢ƒåˆ©å¥½',
                'è®¢å•å¢é•¿ç¨³å®š'
            ],
            'weaknesses': [
                'è¡Œä¸šå‘¨æœŸæ€§å¼º',
                'èµ„é‡‘éœ€æ±‚å¤§',
                'å¸‚åœºç«äº‰åŠ å‰§'
            ],
            'conclusion': 'æ±Ÿé¾™èˆ¹è‰‡ä½œä¸ºç‰¹ç§èˆ¹èˆ¶åˆ¶é€ ä¼ä¸šï¼Œåœ¨å½“å‰æ”¿ç­–ç¯å¢ƒä¸‹å…·å¤‡è‰¯å¥½å‘å±•å‰æ™¯ã€‚å»ºè®®æŠ•èµ„è€…å…³æ³¨å…¶è®¢å•æ‰§è¡Œæƒ…å†µå’Œç°é‡‘æµçŠ¶å†µï¼Œé€‚åº¦å‚ä¸ã€‚'
        }
    }
    
    # ä¿å­˜åˆ†ææ•°æ®
    success = cache.save_analysis('920005', sample_analysis, today)
    
    if success:
        logger.info("âœ… 920005æ±Ÿé¾™èˆ¹è‰‡åˆ†ææ•°æ®å·²ç¼“å­˜")
        
        # ä¹Ÿåˆ›å»ºä¸€ä¸ªæ˜¨å¤©çš„æ•°æ®ä½œä¸ºå†å²è®°å½•
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        historical_analysis = sample_analysis.copy()
        historical_analysis['analysis_date'] = yesterday
        historical_analysis['market_analysis']['current_price'] = 63.15
        historical_analysis['market_analysis']['price_change'] = '+1.85%'
        
        cache.save_analysis('920005', historical_analysis, yesterday)
        logger.info("âœ… 920005å†å²åˆ†ææ•°æ®å·²ç¼“å­˜")
    else:
        logger.error("âŒ 920005åˆ†ææ•°æ®ç¼“å­˜å¤±è´¥")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ç¼“å­˜ç°æœ‰åˆ†æç»“æœ...")
    
    # ç¼“å­˜ç°æœ‰æ–‡ä»¶
    cache_existing_progress_files()
    
    # åˆ›å»º920005ç¤ºä¾‹æ•°æ®
    create_920005_sample_data()
    
    # æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
    cache = get_global_cache()
    stats = cache.get_cache_stats()
    
    logger.info("ğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    logger.info(f"   æ€»æ–‡ä»¶æ•°: {stats.get('total_files', 0)}")
    logger.info(f"   æ€»å¤§å°: {stats.get('total_size_mb', 0)} MB")
    logger.info(f"   è‚¡ç¥¨æ•°é‡: {stats.get('symbol_count', 0)}")
    logger.info(f"   ç¼“å­˜ç›®å½•: {stats.get('cache_directory', 'unknown')}")
    
    # åˆ—å‡ºæ‰€æœ‰ç¼“å­˜
    cached_list = cache.list_cached_analyses()
    logger.info(f"ğŸ“‹ ç¼“å­˜åˆ—è¡¨ ({len(cached_list)}ä¸ª):")
    for item in cached_list[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        logger.info(f"   {item['symbol']} - {item['date']}")
    
    logger.info("âœ… ç¼“å­˜ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()